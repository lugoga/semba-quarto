[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Information Management Strategy for Coastal and Marine Data\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\nStreamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\nThe Skill Gap in Data Science\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\nCompute Normalized Difference Vegetation Index in R\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\nInteractive Web Application for Cage Aquaculture in Lake Victoria\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nHow to handle irregular cell size error when creating Raster in R\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\nWill Your Data Science Programming Skills Be Replaced with Artificail Intelligence?\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\nMachine learning with tidymodels: Binary Classification Model\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nMachine learning with tidymodels: Linear and Bayesian Regression Models\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\n\n\n\n\n\n\nMachine learning with tidymodels: Classification Models\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\n\n\n\n\n\n\nGetting GEBCO Bathymetry Data and glean the power of terra and tidyterra packages for raster and vector objects\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: box plot\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: bar plot\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Line plot\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Joint plot\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Distribution plot\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\nMulti-lingual: R and Python for Data Science\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\nMain and Inset maps with R\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n\n\n\n\n\n\nInferential statistics in R:ttest\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\nCombining plots in R\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n\n\n\n\n\n\nManipulating Simple features: point to polyline\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nForecasting Rising Temperature with prophet package in R\n\n\n\n\n\n\n\n\n\nAug 13, 2022\n\n\n\n\n\n\n\n\nTeacher’s Employment Allocations by LGA\n\n\n\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\nSpatial Data is Maturing in R\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\nGetting and Processing Satellite Data Made Easier in R\n\n\n\n\n\n\n\n\n\nSep 18, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "ng'ara",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nInformation Management Strategy for Coastal and Marine Data\n\n\n\n\n\n\n\nSpatial\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nStreamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region\n\n\n\n\n\n\n\nSpatial\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nMasumbuko Semba\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Skill Gap in Data Science\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nData\n\n\nProgramming\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nMasumbuko Semba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCompute Normalized Difference Vegetation Index in R\n\n\n\n\n\n\n\nSpatial\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nApril 28, 2023\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Web Application for Cage Aquaculture in Lake Victoria\n\n\n\n\n\n\n\nFisheries\n\n\nAquaculture\n\n\n\n\n\n\n\n\n\n\n\nApril 17, 2023\n\n\nMasumbuko Semba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to handle irregular cell size error when creating Raster in R\n\n\n\n\n\n\n\nR\n\n\nModelling\n\n\n\n\n\n\n\n\n\n\n\nApril 16, 2023\n\n\nMasumbuko Semba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nWill Your Data Science Programming Skills Be Replaced with Artificail Intelligence?\n\n\n\n\n\n\n\nR\n\n\nModelling\n\n\n\n\n\n\n\n\n\n\n\nApril 15, 2023\n\n\nMasumbuko Semba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning with tidymodels: Binary Classification Model\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\nModelling\n\n\n\n\n\n\n\n\n\n\n\nApril 13, 2023\n\n\nMasumbuko Semba\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning with tidymodels: Linear and Bayesian Regression Models\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\nModelling\n\n\n\n\n\n\n\n\n\n\n\nApril 11, 2023\n\n\nMasumbuko Semba\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning with tidymodels: Classification Models\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\nModelling\n\n\n\n\n\n\n\n\n\n\n\nApril 3, 2023\n\n\nMasumbuko Semba\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nGetting GEBCO Bathymetry Data and glean the power of terra and tidyterra packages for raster and vector objects\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMarch 1, 2023\n\n\nMasumbuko Semba\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: box plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 24, 2023\n\n\nMasumbuko Semba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: bar plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 23, 2023\n\n\nMasumbuko Semba\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Joint plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 22, 2023\n\n\nMasumbuko Semba\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Line plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 22, 2023\n\n\nMasumbuko Semba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Distribution plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 21, 2023\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nMulti-lingual: R and Python for Data Science\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 20, 2023\n\n\nMasumbuko Semba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMain and Inset maps with R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 19, 2023\n\n\nMasumbuko Semba\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nInferential statistics in R:ttest\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 15, 2023\n\n\nMasumbuko Semba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCombining plots in R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 4, 2023\n\n\nMasumbuko Semba, Nyamisi Peter\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nManipulating Simple features: point to polyline\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 1, 2022\n\n\nMasumbuko Semba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Rising Temperature with prophet package in R\n\n\n\n\n\n\n\ncode\n\n\nAnalysis\n\n\nSpatial\n\n\n\n\n\n\n\n\n\n\n\nAugust 13, 2022\n\n\nMasumbuko Semba\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTeacher’s Employment Allocations by LGA\n\n\n\n\n\n\n\nVisualization\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nAugust 11, 2022\n\n\nMasjumbuko Semba\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Data is Maturing in R\n\n\n\n\n\n\n\nAnalysis\n\n\nSpatial\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nAugust 8, 2022\n\n\nMasjumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nGetting and Processing Satellite Data Made Easier in R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18, 2018\n\n\nMasumbuko Semba\n\n\n13 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ng'ara",
    "section": "",
    "text": "Greetings, I am Masumbuko Semba, a skilled data scientist specializing in spatial analysis and oceanography. Leveraging my programming expertise, I have developed a diverse range of tools, algorithms, and analytical workflows that streamline the organization, management, analysis, and modeling of data. Furthermore, I possess proficiency in various programming languages such as – MATLAB, Python, Observable JavaScript, and R. By harnessing these languages, I am able to automate data processing and analysis tasks, as well as generate comprehensive reports in formats such as PDF or HTML documents. Notably, the scripts and tools I create ensure the reproducibility of data analysis workflows, bolstering informed decision-making processes. I take pride in openly sharing my code and work on platforms such as my blog, website and github accounts, allowing the public to freely access and utilize these resources."
  },
  {
    "objectID": "index.html#peer-review-articles",
    "href": "index.html#peer-review-articles",
    "title": "ng'ara",
    "section": "Peer Review Articles",
    "text": "Peer Review Articles\n\nSilas, M. O., Kishe, M. A., Semba, M. R., Kuboja, B. N., Ngatunga, B., Mgeleka, S. S., … & Gullström, M. (2023). Seascape configuration influences big blue octopus (Octopus cyanea) catches: Implications for a sustainable fishery. Fisheries Research, 264, 106716.\nSilas, M. O., Semba, M. R., Mgeleka, S. S., Van Well, L., Linderholm, H. W., & Gullström, M. (2023). Using fishers’ local ecological knowledge for management of small-scale fisheries in data-poor regions: Comparing seasonal interview and field observation records in East Africa. Fisheries Research, 264, 106721\nLugendo, B. R., Igulu, M. M., Semba, M. L., & Kimirei, I. A. (2022). Caudal fin as a proxy for dorsal muscle for nutrient enrichment monitoring using stable isotope analysis: the case of Gerres filamentosus and G. oyena from mangrove creeks of Tanzania. Western Indian Ocean Journal of Marine Science, 21(2), 71-81.\nSilas, M. O., Semba, M. L., Mgeleka, S., Linderholm, H. W., & Gullström, M. (2022). Fishers’ ecological knowledge for management in data-poor fisheries: a case of East African small-scale fisheries in a changing climate.\nSilas, M. O., Kishe, M. A., Mshana, J. G., Semba, M. L., Mgeleka, S. S., Kuboja, B. N., … & Matiku, P. (2021). Growth, mortality, exploitation rate and recruitment pattern of Octopus cyanea (Mollusca: Cephalopoda) in the WIO region: A case study from the Mafia Archipelago, Tanzania. Western Indian Ocean Journal of Marine Science, 20(1), 71-79.\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. (2021). Seasonal variability of vertical patterns in chlorophyll-a fluorescence in the coastal waters off Kimbiji, Tanzania. Western Indian Ocean Journal of Marine Science, 20(1), 21-33.\nKyewalyanga, M. S., Peter, N., Semba, M., & Mahongo, S. B. (2020). Coastal upwelling and seasonal variation in phytoplankton biomass in the Pemba Channel. Western Indian Ocean Journal of Marine Science, (1/2020), 19-32.\nSemba, M., Lumpkin, R., Kimirei, I., Shaghude, Y., & Nyandwi, N. (2019). Seasonal and spatial variation of surface current in the Pemba Channel, Tanzania. PloS one, 14(1), e0210303\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. S. (2018). The influence of physical-chemical variables on the spatial and seasonal variation of Chlorophyll-a in coastal waters of Unguja, Zanzibar, Tanzania. Western Indian Ocean Journal of Marine Science, 17(2), 25-34\nSemba, M., Kimirei, I., Kyewalyanga, M., Peter, N., Brendonck, L., & Somers, B. (2016). The decline in phytoplankton biomass and prawn catches in the Rufiji-Mafia Channel, Tanzania. Western Indian Ocean Journal of Marine Science, 15(1), 15-29.\nCrawford, B., Herrera, M. D., Hernandez, N., Leclair, C. R., Jiddawi, N., Masumbuko, S., & Haws, M. (2010). Small scale fisheries management: lessons from cockle harvesters in Nicaragua and Tanzania. Coastal Management, 38(3), 195-215."
  },
  {
    "objectID": "index.html#book-chapters",
    "href": "index.html#book-chapters",
    "title": "ng'ara",
    "section": "Book chapters",
    "text": "Book chapters\n\nKimirei, I. A., Igulu, M. M., Semba, M., & Lugendo, B. R. (2016). Small estuarine and non-estuarine mangrove ecosystems of Tanzania: overlooked coastal habitats?. Estuaries: A Lifeline of Ecosystem Services in the Western Indian Ocean, 209-226.\nKimirei, I. A., Semba, M., Mwakosya, C., Mgaya, Y. D., & Mahongo, S. B. (2017). Environmental Changes in the Tanzanian Part of Lake Victoria. Lake Victoria fisheries resources: Research and management in Tanzania, 37-59.\nMahatane, A., Mgaya, Y. D., Hoza, R. B., Onyango, P. O., & Semba, M. (2017). Co-management of Lake Victoria Fisheries. Lake Victoria Fisheries Resources: Research and Management in Tanzania, 219-239.\nSobo, F., Mgaya, Y. D., Kayanda, R. J., & Semba, M. (2017). Fisheries Statistics for Lake Victoria, Tanzania. Lake Victoria Fisheries Resources: Research and Management in Tanzania, 241-253.\nSemba, M., Ndebele-Murisa, M., Mubaya, C. P., Kimirei, I. A., Chavula, G., Mwedzi, T., … & Zenda, S. (2021). Historical and Future Climate Scenarios of the Zambezi River Basin. Ecological Changes in the Zambezi River Basin, 115.\nNdebele-Murisa, M., Kimirei, I. A., Mubaya, C. P., Chavula, G., Mwedzi, T., Mutimukuru-Maravanyika, T., & Semba, M. (2021). A Review of the Comparative Research Method. Ecological Changes in the Zambezi River Basin, 15.\nNdebele-Murisa, M. (Ed.). (2021). Ecological Changes in the Zambezi River Basin. African Books Collective.\nMgaya, Y. D., & Mahongo, S. B. (2017). Lake victoria fisheries resources. Springer."
  },
  {
    "objectID": "index.html#digital-books",
    "href": "index.html#digital-books",
    "title": "ng'ara",
    "section": "Digital books",
    "text": "Digital books\n\nSemba M (2021). Practical Spatial Data Coastal and Marine Environment in R\nSemba M, Peter N. & I. Kimirei (2020). Developing packages in R\nSemba M. (2018). A gentle Introduction of Coding with R\nSemba M., Silas M., Matiku P., & Chande M. (2019) Spatial distribution of octopus fishery in Kilwa District\nSemba M. (2018). Putting fishing landing sites and their instranstructure on spatial aspect\nSemba M. (2023). Fisheries Data Management in Modern R and Excel. Manual with Applications in the Western Indian Ocean Region]. Food and Agriculture Organization and the Western Indian Ocean Marine Science Association"
  },
  {
    "objectID": "index.html#interactive-apps",
    "href": "index.html#interactive-apps",
    "title": "ng'ara",
    "section": "Interactive Apps",
    "text": "Interactive Apps\n\nKwala Commercial and Investment City\nThe Exclusive Economic Zone Tool\nThe Pemba Channel Hydrographic Hub\nThe coastal and Marine Data Visualization tool\nThe National Environmenal Master Plan\nThe Dashboard for Marine Spatial planning"
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "ng'ara",
    "section": "Packages",
    "text": "Packages\nA wior packaged built by Masumbuko Semba and Nyamisi Peter focusing on Easy Tidy and Process Oceanographic Data. The packages is basically developed to help marine and freshwater scientist access a large and varied format of in-situ and satellite data in easy way. In fact the package has made data access in much easy way. The authors are trying to remove the barrier of data access and leave a space for scientists to focus in much deeper thinking of their field rather than spending several days to understand codes for a specific data download. The package contains several tools that allows scientist to get a wide array of datasets. And the funny things is that you get a tidy format result of the download, which is easy to handle in R and also to share it out of R environment. The tidy format is in form that many scientist familiar with Excel spreadsheet will find it handy. You can access this package through this link: https://github.com/lugoga/wior"
  },
  {
    "objectID": "posts/chatgpt/index.html",
    "href": "posts/chatgpt/index.html",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "",
    "text": "In recent years, the rise of artificial intelligence (AI) and machine learning (ML) has led to the development of chatbots like chatGPT (Rodriguez, 2023), which can simulate human conversation. These chatbots are now being used in various industries to improve customer service, automate tasks, and even provide medical advice. However, as chatbots become more advanced, some people are asking whether they could replace human data scientists. In this blog post, we will explore this question and examine the potential benefits and drawbacks of using chatbots in data science.\nData science is the field of study that involves extracting insights and knowledge from data. Data scientists use various techniques, such as statistical analysis, machine learning, and data visualization, to analyze large datasets and identify patterns and trends. Data science is used in many industries, including healthcare, finance, and marketing, to make data-driven decisions and improve business outcomes.\nWikipedia define chatbot as a computer program that uses natural language processing (NLP) to simulate human conversation (Wikipedia contributors, 2023). Chatbots can be designed to perform various tasks, such as answering customer queries, providing recommendations, or even assisting with medical diagnoses. Some chatbots are rule-based, which means they follow a predefined set of rules to respond to user input. Others are powered by AI and ML algorithms, which enable them to learn from user interactions and improve their responses over time.\nReturning to the fundamental question, Can Artificial Intelligence Replace Data Science Programming Skills? This crucial question has a clear answer–Not Always! While chatbots can be useful tools for data analysis and decision-making, they cannot replace the expertise and experience of a human data scientist.\nThe GPT-3 AI language-generating system for example is the offspring of GPT-2, the world most harzadous AI. GPT-3 touts the ability to code in a variety of languages such as Cascading Style sheets (CSS), hypertext Markup language (HTML), Python, R, etc, it still has several issues that need to be fixed. Among them is that the code that GPT-3 generates might not be of much use. Also it makes mistakes that are challenging for inexperienced data scientists and beginners. For example, I was chatting with chatGPT and asked it to generate R codes to download chlorophyll-a acquired with MODIS sensor for the Pemba Channel.\nIt did wonderful job. As plainly seen in Figure 1, it first loaded the rerddap package and defined the ERDDAP server URL and the dataset ID for the MODIS Aqua Chlorophyll-a Level 3 dataset. It then defined the study area and time range by specifying longitude and latitude ranges, and start and end dates. It built the ERDDAP query string by specifying the variable name, standard name, and the desired ranges for longitude, latitude, and time.Then used the erddap function from the rerddap package to query the ERDDAP server and retrieve the data. The data is returned as a data frame in CSV format.\nI copied the code chunk from ChatGPT snippet (Figure 1) and paste it in the chunk below:\nDespite the detailed explanation of chatGPT provided, when the chunk is exuted, it resulted into an error message stating that Error: ‘erddap’ is not an exported object from ‘namespace:rerddap’. It failed miserable because the erddap function used in line 24 of the chunk code above is not part of the erddap package. Therefore, as it has been emphasized, inaccurate information and lines of codes is a common mistake we expect from these kind of tools, and the essence of expert to support the error is paramount. Therefore, the code chunk below provide a correct functions and lines of code that answer the question posted in the chatGPT.\nI then used the ggplot2 package (Campitelli, 2019; Wickham, 2016) to visualize the distribution of chlorophyll-a in the Pemba Channel. I specified the longitude and latitude values from the data frame, and used the z parameter to specify the chlorophyll-a values for each pixel in the plot. I also specified a color scale for the plot using the scale_fill_gradientn function and separated plots for each date of the year with The facet_wrap() function. The labs() function was used to add a title and axis labels to faceted plots.\nIn summary, although there existing chatbots like ChatGPT that can generate simple code, they cannot decide which feature should be prioritized or what issues that algorithm or piece of code should address. While chatbots like ChatGPT can be useful tools for data analysis and decision-making, they cannot replace the expertise and experience of a human data scientist. Chatbots are limited by their programming and lack the domain knowledge, creativity, and human interaction required for complex data science tasks. However, chatbots can provide some benefits, such as improved efficiency, faster response times, scalability, and a user-friendly interface. For the time being, only a smart data scientist can create code based on an understanding of precise requirements and specification of the data related issue that need to bring business solution."
  },
  {
    "objectID": "posts/chatgpt/index.html#what-is-a-chatbot",
    "href": "posts/chatgpt/index.html#what-is-a-chatbot",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "What is a Chatbot?",
    "text": "What is a Chatbot?\nWikipedia define chatbot as a computer program that uses natural language processing (NLP) to simulate human conversation (Wikipedia contributors, 2023). Chatbots can be designed to perform various tasks, such as answering customer queries, providing recommendations, or even assisting with medical diagnoses. Some chatbots are rule-based, which means they follow a predefined set of rules to respond to user input. Others are powered by AI and ML algorithms, which enable them to learn from user interactions and improve their responses over time."
  },
  {
    "objectID": "posts/chatgpt/index.html#what-is-data-science",
    "href": "posts/chatgpt/index.html#what-is-data-science",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nData science is the field of study that involves extracting insights and knowledge from data. Data scientists use various techniques, such as statistical analysis, machine learning, and data visualization, to analyze large datasets and identify patterns and trends. Data science is used in many industries, including healthcare, finance, and marketing, to make data-driven decisions and improve business outcomes."
  },
  {
    "objectID": "posts/chatgpt/index.html#can-chatbots-replace-data-scientists",
    "href": "posts/chatgpt/index.html#can-chatbots-replace-data-scientists",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "Can Chatbots Replace Data Scientists?",
    "text": "Can Chatbots Replace Data Scientists?\nThe short answer is no. While chatbots can be useful tools for data analysis and decision-making, they cannot replace the expertise and experience of a human data scientist. Here are some reasons why:\n\nLimited Domain Knowledge: Chatbots are designed to perform specific tasks within a limited domain. They may be able to provide answers to simple questions or perform basic analyses, but they lack the domain knowledge and expertise required for complex data science tasks.\nLack of Creativity: Data science involves creative problem-solving and the ability to think outside the box. Chatbots are limited by their programming and cannot come up with innovative solutions or new approaches to data analysis.\nLimited Data Access: Chatbots can only access data that has been pre-defined and made available to them. They cannot access or analyze data that has not been specifically programmed into their system.\nLack of Human Interaction: Data science involves collaboration and communication between team members. Chatbots cannot replace the value of human interaction and the ability to discuss ideas and insights with other experts in the field."
  },
  {
    "objectID": "posts/chatgpt/index.html#benefits-of-using-chatbots-in-data-science",
    "href": "posts/chatgpt/index.html#benefits-of-using-chatbots-in-data-science",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "Benefits of Using Chatbots in Data Science",
    "text": "Benefits of Using Chatbots in Data Science\nWhile chatbots cannot replace human data scientists, they can be useful tools for data analysis and decision-making. Here are some potential benefits of using chatbots in data science:\n\nImproved Efficiency: Chatbots can automate routine tasks, such as data cleaning and preprocessing, which can save time and improve efficiency.\nFaster Response Times: Chatbots can provide instant responses to user queries, which can speed up the decision-making process.\nScalability: Chatbots can handle large volumes of data and perform analyses at scale, which can be useful for businesses with large datasets.\nUser-Friendly Interface: Chatbots provide a user-friendly interface for data analysis, which can make it easier for non-technical users to access and understand data insights.\n\nThe GPT-3 AI language-generating system for example is the offspring of GPT-2, the world most harzadous AI. GPT-3 touts the ability to code in a variety of languages such as Cascading Style sheets (CSS), hypertext Markup language (HTML), Python, R, etc, it still has several issues that need to be fixed. Among them is that the code that GPT-3 generates might not be of much use. Also it makes mistakes that are challenging for inexperienced data scientists and beginners. For example, I was chatting with chatGPT and asked it to generate R codes to download chlorophyll-a acquired with MODIS sensor for the Pemba Channel.\nIt did wonderful job. As plainly seen in Figure 1, it first loaded the rerddap package and defined the ERDDAP server URL and the dataset ID for the MODIS Aqua Chlorophyll-a Level 3 dataset. It then defined the study area and time range by specifying longitude and latitude ranges, and start and end dates. It built the ERDDAP query string by specifying the variable name, standard name, and the desired ranges for longitude, latitude, and time.Then used the erddap function from the rerddap package to query the ERDDAP server and retrieve the data. The data is returned as a data frame in CSV format.\n\n\n\n\n\nFigure 1: ChatGPT code for access and dowloading chlrophyll-a in the Pemba Channel\n\n\n\n\nI copied the code chunk from ChatGPT snippet (Figure 1) and paste it in the chunk below:\n\nlibrary(rerddap)\n\n# Define the ERDDAP server URL and dataset ID\nserver <- \"https://coastwatch.pfeg.noaa.gov/erddap\"\ndataset_id <- \"erdMWChla1day\"\n\n# Define the study area and time range\nlon_range <- c(39, 41)\nlat_range <- c(-7, -5)\nstart_date <- \"2022-01-01\"\nend_date <- \"2022-02-01\"\n\n# Build the ERDDAP query string\nquery <- list(\n  \"chlorophyll_a\" = list(\n    \"standard_name\" = \"mass_concentration_of_chlorophyll_a_in_sea_water\",\n    \"longitude>=\" = lon_range[1], \"longitude<=\" = lon_range[2],\n    \"latitude>=\" = lat_range[1], \"latitude<=\" = lat_range[2],\n    \"time>=\" = start_date, \"time<=\" = end_date\n  )\n)\n\n# Query the ERDDAP server and retrieve the data\ndata <- rerddap::erddap(query = query, server = server, dataset_id = dataset_id, \n                        protocol = \"tabledap\", response = \"csv\")\n\nDespite the detailed explanation of chatGPT provided, when the chunk is exuted, it resulted into an error message stating that Error: ‘erddap’ is not an exported object from ‘namespace:rerddap’. It failed miserable because the erddap function used in line 24 of the chunk code above is not part of the erddap package. Therefore, as it has been emphasized, inaccurate information and lines of codes is a common mistake we expect from these kind of tools, and the essence of expert to support the error is paramount. Therefore, the code chunk below provide a correct functions and lines of code that answer the question posted in the chatGPT.\n\nlibrary(rerddap)\nlibrary(tidyverse)\n\nchl = rerddap::griddap(\n  x = \"erdMH1chla8day\",\n  longitude = c(38, 39.5),\n  latitude = c(-6.5,-5.5),\n  time = c(\"2022-01-01\", \"2022-01-31\"),\n  fmt = \"csv\") %>%\n    dplyr::as_tibble() %>%\n    dplyr::mutate(time = lubridate::as_date(time))\n\nchl\n\n# A tibble: 4,625 x 4\n   time       latitude longitude chlorophyll\n   <date>        <dbl>     <dbl>       <dbl>\n 1 2021-12-31    -5.48      38.0         NaN\n 2 2021-12-31    -5.48      38.0         NaN\n 3 2021-12-31    -5.48      38.1         NaN\n 4 2021-12-31    -5.48      38.1         NaN\n 5 2021-12-31    -5.48      38.1         NaN\n 6 2021-12-31    -5.48      38.2         NaN\n 7 2021-12-31    -5.48      38.2         NaN\n 8 2021-12-31    -5.48      38.3         NaN\n 9 2021-12-31    -5.48      38.3         NaN\n10 2021-12-31    -5.48      38.4         NaN\n# ... with 4,615 more rows\n\n\nFinally, I used the ggplot2 package (Campitelli, 2019; Wickham, 2016) to visualize the distribution of chlorophyll-a in the Pemba Channel. I specified the longitude and latitude values from the data frame, and used the z parameter to specify the chlorophyll-a values for each pixel in the plot. I also specified a color scale for the plot using the scale_fill_gradientn function and separated plots for each date of the year with The facet_wrap() function. The labs() function was used to add a title and axis labels to faceted plots.\n\n\n\n\nggplot(\n  data = chl %>% filter(time < \"2022-01-16\"),\n  aes(x = longitude, y = latitude, z = chlorophyll)\n  )+\n  metR::geom_contour_fill(bins = 120)+\n  metR::geom_contour2(aes(label = ..level..)) +\n  ggspatial::layer_spatial(tz, fill = \"#84837a\", color = \"black\", linewidth = .5)+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120), trans = scales::log10_trans(), \n                       name = \"Chl-a\\n(mgm^3)\")+\n  metR::scale_x_longitude(ticks = .3)+\n  metR::scale_y_latitude(ticks = .3) +\n coord_sf(xlim = c(38.7,39.4), ylim = c(-6.5,-5.5))+\n  facet_wrap(~time, nrow = 1) +\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\"))\n\n\n\n\nFigure 2: Chlorophyll-a concentration in the Pemba Channel"
  },
  {
    "objectID": "posts/chatgpt/index.html#drawbacks-of-using-chatbots-in-data-science",
    "href": "posts/chatgpt/index.html#drawbacks-of-using-chatbots-in-data-science",
    "title": "Can ChatGPT Replace a Data Scientist Role?",
    "section": "Drawbacks of Using Chatbots in Data Science",
    "text": "Drawbacks of Using Chatbots in Data Science\nWhile chatbots can provide some benefits for data analysis, there are also some potential drawbacks to consider:\n\nLimited Accuracy: Chatbots may provide inaccurate responses if they are not properly trained or if they lack the necessary domain knowledge.\n\n\nFor example, I threw a question to ask chatGPT to retrieve chlorophyll-a dataset in the Pemba Channel from MODIS. It did wonderful job. As plainly seen in Figure 1, it first loaded the rerddap package and defined the ERDDAP server URL and the dataset ID for the MODIS Aqua Chlorophyll-a Level 3 dataset. It then defined the study area and time range by specifying longitude and latitude ranges, and start and end dates. It built the ERDDAP query string by specifying the variable name, standard name, and the desired ranges for longitude, latitude, and time.Then used the erddap function from the rerddap package to query the ERDDAP server and retrieve the data. The data is returned as a data frame in CSV format.\n\n\n\n\n\n\nFigure 1: ChatGPT code for access and dowloading chlrophyll-a in the Pemba Channel\n\n\n\n\nI copied the code chunk from ChatGPT snippet (Figure 1) and paste it in the chunk below:\n\nlibrary(rerddap)\n\n# Define the ERDDAP server URL and dataset ID\nserver <- \"https://coastwatch.pfeg.noaa.gov/erddap\"\ndataset_id <- \"erdMWChla1day\"\n\n# Define the study area and time range\nlon_range <- c(39, 41)\nlat_range <- c(-7, -5)\nstart_date <- \"2022-01-01\"\nend_date <- \"2022-02-01\"\n\n# Build the ERDDAP query string\nquery <- list(\n  \"chlorophyll_a\" = list(\n    \"standard_name\" = \"mass_concentration_of_chlorophyll_a_in_sea_water\",\n    \"longitude>=\" = lon_range[1], \"longitude<=\" = lon_range[2],\n    \"latitude>=\" = lat_range[1], \"latitude<=\" = lat_range[2],\n    \"time>=\" = start_date, \"time<=\" = end_date\n  )\n)\n\n# Query the ERDDAP server and retrieve the data\ndata <- rerddap::erddap(query = query, server = server, dataset_id = dataset_id, \n                        protocol = \"tabledap\", response = \"csv\")\n\nDespite the detailed explanation of chatGPT provided, when the chunk is exuted, it resulted into an error message stating that Error: ‘erddap’ is not an exported object from ‘namespace:rerddap’. It failed miserable because the erddap function used in line 24 of the chunk code above is not part of the erddap package. Therefore, as it has been emphasized, inaccurate information is a common thing we expect from these kind of tools, and the essence of expert to support the error is paramount. Therefore, the code hunk below provide a correct functions and code line that answer the question posted in the chatGPT.\n\nlibrary(rerddap)\nlibrary(tidyverse)\n\nchl = rerddap::griddap(\n  x = \"erdMH1chla8day\",\n  longitude = c(38, 39.5),\n  latitude = c(-6.5,-5.5),\n  time = c(\"2022-01-01\", \"2022-01-31\"),\n  fmt = \"csv\") %>%\n    dplyr::as_tibble() %>%\n    dplyr::mutate(time = lubridate::as_date(time))\n\nchl\n\n# A tibble: 4,625 x 4\n   time       latitude longitude chlorophyll\n   <date>        <dbl>     <dbl>       <dbl>\n 1 2021-12-31    -5.48      38.0         NaN\n 2 2021-12-31    -5.48      38.0         NaN\n 3 2021-12-31    -5.48      38.1         NaN\n 4 2021-12-31    -5.48      38.1         NaN\n 5 2021-12-31    -5.48      38.1         NaN\n 6 2021-12-31    -5.48      38.2         NaN\n 7 2021-12-31    -5.48      38.2         NaN\n 8 2021-12-31    -5.48      38.3         NaN\n 9 2021-12-31    -5.48      38.3         NaN\n10 2021-12-31    -5.48      38.4         NaN\n# ... with 4,615 more rows\n\n\nFinally, we use the ggplot2 package (Campitelli, 2019; Wickham, 2016) to plot the chlorophyll-a data. I specified the longitude and latitude values from the data frame, and used the z parameter to specify the chlorophyll-a values for each pixel in the plot. I also specified a color scale for the plot using the scale_fill_gradientn function separated plots for each date of the year with The facet_wrap() function. The labs() function was used to add a title and axis labels to the plot.\n\n\n\n\nggplot(\n  data = chl %>% filter(time < \"2022-01-16\"),\n  aes(x = longitude, y = latitude, z = chlorophyll)\n  )+\n  metR::geom_contour_fill(bins = 120)+\n  metR::geom_contour2(aes(label = ..level..)) +\n  ggspatial::layer_spatial(tz, fill = \"#84837a\", color = \"black\", linewidth = .5)+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120), trans = scales::log10_trans(), \n                       name = \"Chl-a\\n(mgm^3)\")+\n  metR::scale_x_longitude(ticks = .3)+\n  metR::scale_y_latitude(ticks = .3) +\n coord_sf(xlim = c(38.7,39.4), ylim = c(-6.5,-5.5))+\n  facet_wrap(~time, nrow = 1) +\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\"))\n\n\n\n\nFigure 2: Chlorophyll-a concentration in the Pemba Channel"
  },
  {
    "objectID": "posts/chatgpt/index.html#conclusion",
    "href": "posts/chatgpt/index.html#conclusion",
    "title": "Will Your Data Science Programming Skills Be Replaced with Artificail Intelligence?",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough there existing chatbots like ChatGPT that can generate simple code, they cannot decide which feature should be prioritized or what issues that algorithm or piece of code should address. While chatbots like ChatGPT can be useful tools for data analysis and decision-making, they cannot replace the expertise and experience of a human data scientist. Chatbots are limited by their programming and lack the domain knowledge, creativity, and human interaction required for complex data science tasks. However, chatbots can provide some benefits, such as improved efficiency, faster response times, scalability, and a user-friendly interface. For the time being, only a smart data scientist can create code based on an understanding of precise requirements and specification of the data related issue that need to bring business solution."
  },
  {
    "objectID": "posts/combining_plots/index.html",
    "href": "posts/combining_plots/index.html",
    "title": "Combining plots in R",
    "section": "",
    "text": "The ggplot2 package doesn’t provide a function to arrange multiple plots in a single figure (Wickham 2016). Still, some packages allow combining multiple plots into a single figure with custom layouts, width, and height, such as cowplot (Wilke 2018), gridExtra, and patchwork (Pedersen 2020). In this post we are going to use several packages, let’us load them in our session\nSample datasets"
  },
  {
    "objectID": "posts/combining_plots/index.html#gridextra",
    "href": "posts/combining_plots/index.html#gridextra",
    "title": "Combining plots in R",
    "section": "gridExtra",
    "text": "gridExtra\nThe gridExtra package provides the grid.arrange function to combine several plots on a single figure.\n\ngridExtra::grid.arrange(hist, box, ridges)\n\n\n\n\nWe can also specify the number of rows with nrow, the number of columns with ncol, and the sizes with widths and heights, and also we can add labels at the top, bottom, left, and right of the figures.\n\ngridExtra::grid.arrange(hist, box, ridges, nrow = 2, top = \"Top Panel\", bottom = \"Bottom Panel\")"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html",
    "href": "posts/forecastingTimeseries/index.html",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "",
    "text": "Time-series analysis aims to analyse and learn the temporal behavior of datasets over a period. Examples include the investigation of long-term records of temperature , sea-level fluctuations, the effect of the El Niño/Southern Oscillation on tropical rainfall, and surface current influences on distribution of temperature and rainfall. Th e temporal pattern of a sequence of events in a time series data can be either random, clustered, cyclic, or chaotic.\nTime-series analysis provides various tools with which to detect these temporal patterns. Moreover, it helps in learning the behavior of the dataset by plotting the time series object on the graph. In R language, time series objects are handled easily using ts() function that takes the data vector and converts it into time series object as specified in function parameters. Therefore, understanding the underlying processes that produced the observed data allows us to predict future values of the variable.\nIn this post we learn how to forecast in R. We will use the prophet package (prophet?), which contain all the necessary routines for time-series analysis. Prophet is a package developed by Facebook for forecasting time series objects or data. Prophet package is based on decompose model i.e., trend, seasonality and holidays that helps in making more accurate predictive models. It is much better than the ARIMA model as it helps in tuning and adjusting the input parameters.\n\nrequire(sf)\nrequire(tidyverse)\nrequire(patchwork)\nrequire(magrittr)\nrequire(tmap)\nrequire(prophet)\n\n## basemap shapefile from tmap package\ndata(\"World\")\n\n\n# color codes\nmycolor3 = c(\"#9000B0\", \"#C900B0\", \"#C760AF\", \"#1190F9\", \"#60C8F8\", \"#90C9F8\", \"#F8F8F8\", \"#F8F800\",  \"#F8D730\", \"#f8b030\", \"#f8602f\", \"#f80000\")"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#dataset",
    "href": "posts/forecastingTimeseries/index.html#dataset",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Dataset",
    "text": "Dataset\nWe will use the NASA GISTEMP V4 dataset that combine NOAA GHCN meteorological stations and ERSST ocean temperature to form a comprehensive long record of temperature variability of the earth surface. The dataset contains monthly temperature values from 1880 to present, which is widely used to monitor the weather and climate at regional and global scale. Rather than using absolute temperature values, the dataset uses anomaly obtained by using base period (1951-1980).\nNASA’s analysis incorporates surface temperature measurements from more than 26,000 weather stations and thousands of ship- and buoy-based observations of sea surface temperatures. These raw measurements are analyzed using an algorithm that considers the varied spacing of temperature stations around the globe and urban heating effects that could skew the conclusions if not taken into account. The result of these calculations is an estimate of the global average temperature difference from a baseline period of 1951 to 1980.\nThis dataset is open and free to download as netCDF format file at GISTEMP. I have processed the file and we can load as the csv file here.\n\nglobal = read_csv(\"../data/temperature_lss_global_1990_2020_2021.csv\")\n\nThe Earth’s global average surface temperature in 2021 tied 2018 (See Figure 1) is the sixth-warmest year on record, according to independent analyses from NASA and the National Oceanic and Atmospheric Administration (NOAA). According to scientists at NASA’s Goddard Institute for Space Studies (GISS), global temperatures in 2021 were 0.85 degrees Celsius above the average for NASA’s baseline period,\n\ntemperature = global %>% filter(year == 2021)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 1: Global land and sea surface temperature anomaly for a year 2021 compared to to the 1950-1980 average\n\n\n\n\nRegardless of the COVID-19 pandemic that reduction in mobility and human activities, along with reduced industrial production, has led to lower levels of nitrogen dioxide (NO2) and and the subsequent decrease of fossil fuel burning and CO2 emissions, NASA found that the year 2020 (Figure 2) was the hottest year on record. Continuing the planet’s long-term warming trend, the year’s globally averaged temperature was 1.02 degrees Celsius warmer than the baseline 1951-1980 mean, according to scientists at NASA’s Goddard Institute for Space Studies (GISS) in New York. 2020 edged out 2016 by a very small amount, within the margin of error of the analysis, making the years effectively tied for the warmest year on record. The last seven years have been the warmest seven years on record.\n\ntemperature = global %>% filter(year == 2020)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 2: 2020! the hottest year on record"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "href": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Data Preparation & Exploration",
    "text": "Data Preparation & Exploration\nProphet works best with periodicity data with at least one year of historical data. It’s possible to use Prophet to forecast using sub-daily or monthly data, but for the purposes of this post, we’ll use the monthly periodicity of global temperature—land and ocean . Let’s us read the file into our session\n\nmonthly = read_csv(\"../data/temperature_lss.csv\")\n\nmonthly %>% FSA::headtail(n = 5)\n\n           time temperature\n1    1880-01-15  -0.2322751\n2    1880-02-15  -0.4008110\n3    1880-03-15  -0.1818604\n4    1880-04-15  -0.2003687\n5    1880-05-15  -0.1266611\n1701 2021-09-15   1.1336590\n1702 2021-10-15   1.3624321\n1703 2021-11-15   1.0853070\n1704 2021-12-15   0.9597652\n1705 2022-01-15   1.1144421\n\n\nLooking on the printed dataset, we note that we have records of land and sea temperature since January 1880 through January 2022. That is the long historical data that suits our analysis. The first thing we need to do is to create a time-series object in R. This is done by using a ts function and specify the start year and the frequency of observation. Since we have monthly records, the frequency for each year will be 12 as the chunk highlight\n\nts.temp = monthly %>% \n  pull(temperature) %>% \n  ts(start = c(1880,1), frequency = 12)\n\nBut, although we have that long historical dataset, we are more interested in the most recent records. Therefore, we filter all records since 1980 for our analysis. We can achieve this by simply passing the limiting year and month in the window function. Once we have filtered the dataset, we then convert the ts object to prophet format using a ts_to_prophet function from TSstudio package\n\nts.df = ts.temp %>% \n  window(start = c(1980,1)) %>% \n  TSstudio::ts_to_prophet()\n\nTracking global temperature trends provides a critical indicator of the impact of human activities— specifically, greenhouse gas emissions – on our planet. Visualizing the dataset as seen in Figure 3. It’s an undeniable fact the global mean temperature is constantly rising. Earth’s average temperature has risen above 1.2 degrees Celsius) since the late 19th century and the IPCC has pointed out the increase should be limited to 1.5 °C above pre-industrial levels, to have any hope of mitigating the harmful effects of climate change.\n\nts.df %>% \n  ggplot(aes(x = ds, y = y))+\n  geom_line()+\n  geom_smooth(fill = \"red\", color = \"red\", alpha = .2)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nFigure 3: Trend of global temperature"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#forecasting",
    "href": "posts/forecastingTimeseries/index.html#forecasting",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Forecasting",
    "text": "Forecasting\nThe first step in creating a forecast using Prophet is importing the prophet library into our R session. Once the prophet library is loaded into our session, we’re ready to fit a model to our historical data. We can achieve that by simply calling theprophet() function using your prepared dataframe as an input:\n\nm = ts.df %>% prophet()\n\nOnce we have used Prophet to fit the model dataset, we can now start making predictions for future dates. Prophet has a built-in helper function make_future_dataframe to create a dataframe of future dates. The make_future_dataframe function, which allows to specify the frequency and number of periods we would like to forecast into the future. By default, the frequency is set to days. Since we are using daily periodicity data in this example, we will leave freq at it’s default and set the periods argument to 365, indicating that we would like to forecast 365 days into the future.\n\nfuture = m %>% \n  make_future_dataframe(120,freq = \"month\")\n\nWe can now use the predict() function to make predictions for each row in the future dataframe.\n\nforecast = m %>% predict(future)\n\nAt this point, Prophet will have created a new dataframe assigned to the forecast variable that contains the forecasted values for future dates under a column called yhat, as well as uncertainty intervals and components for the forecast. We can visualize the forecast using Prophet’s built-in plot helper function:\n\nplot(x = m, fcst = forecast, uncertainty = T,plot_cap = T,)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  # scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank())+\n  add_changepoints_to_plot(m = m, threshold = 0.1)\n\n\n\n\nFigure 4: Forecasted sea surface temperature\n\n\n\n\nIf we want to visualize the individual forecast components, we can use Prophet’s built-in plot_components function:\n\nplot_forecast_component(m = m, fcst = forecast, name = \"trend\")\n\n\n\n\nFigure 5: Visualize the trend"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#cited-references",
    "href": "posts/forecastingTimeseries/index.html#cited-references",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Cited references",
    "text": "Cited references"
  },
  {
    "objectID": "posts/gebco/index.html",
    "href": "posts/gebco/index.html",
    "title": "Getting GEBCO Bathymetry Data and glean the power of terra and tidyterra packages for raster and vector objects",
    "section": "",
    "text": "As an Oceanography, one key parameter that need to get right is the bathymetry. Bathymetry is the science of determining the topography of the seafloor. Bathymetry data is used to generate navigational charts, seafloor profile, biological oceanography, beach erosion, sea-level rise, etc. There pretty of bathymetry data and one of the them is the GEBCO Gridded Bathymetry Data. In this post we are going to learn how to access the bathymetry dataset from GEBCO website, import the dataset into R session and make plots to reveal the elevation and bathymetry of the Pemba Channel. In addtional, the post provide an glimpse of the new packages for handling raster dataset in R. The packages I am goint to introduce to you are terra and tidyterra, that have changed the way R handles raster and vector objet and improved both the processing of raster objects and visualization."
  },
  {
    "objectID": "posts/gebco/index.html#dataset",
    "href": "posts/gebco/index.html#dataset",
    "title": "Getting GEBCO Bathymetry Data and glean the power of terra and tidyterra packages for raster and vector objects",
    "section": "Dataset",
    "text": "Dataset\nThe General bathymetric Chart of the Oceans (GEBCO) consists of an international group of experts in ocean mapping. This team provides the most authoritative publicly-available bathymetry of the world’s oceans. In this post i will illustrate how to download data from their website and use for mapping. You can obtain the data for your region of interest or for the global oceans. You can download the data from GEBCO. For this case I have downloaded the data for East African Coast as netCDF file by specifying the geogrpahical extent and choose the file type as shown in Figure 1.\n\n\n\n\n\nFigure 1: The gateway screenshot for accessing bathymetric dataset for any region in the world\n\n\n\n\nTo process the data and visualize in maps, we need several packages highlighted in the chunk below. You need to load the packages in your session first. If not in your machine, you need to install them first.\n\nrequire(tidyverse)\nrequire(ncdf4)\nrequire(sf)\nrequire(metR)\n\nThen read the file using nc_open function of the ncdf4 package (Pierce 2017) and print the file to see the metadata that describe the variables that are embedded in the file.\n\n\n\n\nnc = nc_open(\"d:/gebco_tz.nc\")\n\n\nnc\n\nFile d:/semba/shapefile/gebco/gebco_2021_n2.0_s-15.0_w35.0_e50.0.nc (NC_FORMAT_CLASSIC):\n\n     1 variables (excluding dimension variables):\n        short elevation[lon,lat]   \n            standard_name: height_above_mean_sea_level\n            long_name: Elevation relative to sea level\n            units: m\n            grid_mapping: crs\n            sdn_parameter_urn: SDN:P01::ALATZZ01\n            sdn_parameter_name: Sea floor height (above mean sea level) {bathymetric height}\n            sdn_uom_urn: SDN:P06::ULAA\n            sdn_uom_name: Metres\n\n     2 dimensions:\n        lat  Size:4080 \n            standard_name: latitude\n            long_name: latitude\n            units: degrees_north\n            axis: Y\n            sdn_parameter_urn: SDN:P01::ALATZZ01\n            sdn_parameter_name: Latitude north\n            sdn_uom_urn: SDN:P06::DEGN\n            sdn_uom_name: Degrees north\n        lon  Size:3600 \n            standard_name: longitude\n            long_name: longitude\n            units: degrees_east\n            axis: X\n            sdn_parameter_urn: SDN:P01::ALONZZ01\n            sdn_parameter_name: Longitude east\n            sdn_uom_urn: SDN:P06::DEGE\n            sdn_uom_name: Degrees east\n\n    36 global attributes:\n        title: The GEBCO_2021 Grid - a continuous terrain model for oceans and land at 15 arc-second intervals\n        summary: The GEBCO_2021 Grid is a continuous, global terrain model for ocean and land with a spatial resolution of 15 arc seconds.The grid uses as a 'base-map' Version 2.2 of the SRTM15+ data set (Tozer et al, 2019). This data set is a fusion of land topography with measured and estimated seafloor topography. It is augmented with gridded bathymetric data sets developed as part of the Nippon Foundation-GEBCO Seabed 2030 Project.\n        keywords: BATHYMETRY/SEAFLOOR TOPOGRAPHY, DIGITAL ELEVATION/DIGITAL TERRAIN MODELS\n        Conventions: CF-1.6, ACDD-1.3\n        id: DOI: 10.5285/c6612cbe-50b3-0cff-e053-6c86abc09f8f\n        naming_authority: https://dx.doi.org\n        history: Information on the development of the data set and the source data sets included in the grid can be found in the data set documentation available from https://www.gebco.net\n        source: The GEBCO_2021 Grid is the latest global bathymetric product released by the General Bathymetric Chart of the Oceans (GEBCO) and has been developed through the Nippon Foundation-GEBCO Seabed 2030 Project. This is a collaborative project between the Nippon Foundation of Japan and GEBCO. The Seabed 2030 Project aims to bring together all available bathymetric data to produce the definitive map of the world ocean floor and make it available to all.\n        comment: The data in the GEBCO_2021 Grid should not be used for navigation or any purpose relating to safety at sea.\n        license: The GEBCO Grid is placed in the public domain and may be used free of charge. Use of the GEBCO Grid indicates that the user accepts the conditions of use and disclaimer information: https://www.gebco.net/data_and_products/gridded_bathymetry_data/gebco_2019/grid_terms_of_use.html\n        date_created: 2021-07-01\n        creator_name: GEBCO through the Nippon Foundation-GEBCO Seabed 2030 Project\n        creator_email: gdacc@seabed2030.org\n        creator_url: https://www.gebco.net\n        institution: On behalf of the General Bathymetric Chart of the Oceans (GEBCO), the data are held at the British Oceanographic Data Centre (BODC).\n        project: Nippon Foundation - GEBCO Seabed2030 Project\n        creator_type: International organisation\n        geospatial_bounds: -180\n         geospatial_bounds: -90\n         geospatial_bounds: 180\n         geospatial_bounds: 90\n        geospatial_bounds_crs: WGS84\n        geospatial_bounds_vertical_crs: EPSG:5831\n        geospatial_lat_min: -90\n        geospatial_lat_max: 90\n        geospatial_lat_units: degrees_north\n        geospatial_lat_resolution: 0.00416666666666667\n        geospatial_lon_min: -180\n        geospatial_lon_max: 180\n        geospatial_lon_units: degrees_east\n        geospatial_lon_resolution: 0.00416666666666667\n        geospatial_vertical_min: -10977\n        geospatial_vertical_max: 8685\n        geospatial_vertical_units: meters\n        geospatial_vertical_resolution: 1\n        geospatial_vertical_positive: up\n        identifier_product_doi: DOI: 10.5285/c6612cbe-50b3-0cff-e053-6c86abc09f8f\n        references: DOI: 10.5285/c6612cbe-50b3-0cff-e053-6c86abc09f8f\n        node_offset: 1\n\n\nLooking on the metadata, we notice that there are three variables we need to extract from the file, these are longitude, latitude and depth. We use a ncvar_get function from ncdf4 (Pierce 2017) package to extract these variables. Note the name parsed in the function as should written as they appear in the metadata.\n\nlat = ncvar_get(nc, \"lat\")\nlon = ncvar_get(nc, \"lon\")\nbathy = ncvar_get(nc, \"elevation\")\n\nThen we can check the type of the file using a class function\n\nclass(bathy); class(lon); class(lat)\n\n[1] \"matrix\" \"array\" \n\n\n[1] \"array\"\n\n\n[1] \"array\"\n\n\nWe notice these objects comes as array. we can check the size also\n\ndim(lon); dim(lat);dim(bathy)\n\n[1] 3600\n\n\n[1] 4080\n\n\n[1] 3600 4080\n\n\nWe also notice that while lon and lat object are array, but they are vector and only bathy is the matrix. Therefore, we need to make a data frame so that we can make plots using ggplot package, which only work in the dataset that is organized as data.frame or tibble. That can be done using a expand.grid function. First we expand the lon and lat file followed with the bathy and combine them to make a tibble as the chunk below highlight. Because of the file size, only bathymetric values that fall within the pemba Channel were selected.\n\ndataset = expand.grid(lon, lat) %>% \n  bind_cols(expand.grid(bathy)) %>% \n  as_tibble() %>% \n  rename(lon = 1, lat = 2, depth = 3)%>% \n  filter(lon >38.5 & lon < 40.5 & lat > -5.8 & lat < -4)\n\nSeparate the dataset into the land and ocean based on zero (0) value as reference point, where the above sea level topography values are assumed\n\nland = dataset %>% filter(depth >0 )\nocean = dataset %>% filter(depth <= 0 )\n\nLoad the basemap shapefile\n\n\n\n\nafrica = st_read(\"d:/africa.shp\", quiet = TRUE)\n\nMake a color of land and depth that we will use later for mapping the topography and bathymetry, respectively.\n\n#make palette\nocean.pal <- c(\"#000000\", \"#000413\", \"#000728\", \"#002650\", \"#005E8C\", \"#0096C8\", \"#45BCBB\", \"#8AE2AE\", \"#BCF8B9\", \"#DBFBDC\")\nland.pal <- c(\"#467832\", \"#887438\", \"#B19D48\", \"#DBC758\", \"#FAE769\", \"#FAEB7E\", \"#FCED93\", \"#FCF1A7\", \"#FCF6C1\", \"#FDFAE0\")\n\nWe can plot the bathymetry shown in @ fig-bathy with the code highlighted in the chunk below\n\nggplot()+\n  metR::geom_contour_fill(data = ocean, aes(x = lon, y = lat, z = depth), \n                          bins = 120, global.breaks = FALSE) +\n  metR::geom_contour2(data = ocean, aes(x = lon, y = lat, z = depth, \n                                        label = ..level..), breaks = c(-200,-600), skip = 0 )+\n  scale_fill_gradientn(colours = ocean.pal, name = \"Depth (m)\", \n                       breaks = seq(-1800,0,300), label = seq(1800,0,-300))+\n  ggspatial::layer_spatial(data = africa)+\n  coord_sf(xlim = c(38.9,40), ylim = c(-5.6,-4.1))+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  ggspatial::annotation_scale()\n\n\n\n\nFigure 2: Bathymetry of the Pemba Channel, solid lines are contour lines\n\n\n\n\nSimilary, we can plot togopgraphy of the area shown in Figure 3 using the code shown below\n\nggplot()+\n  metR::geom_contour_fill(data = land, aes(x = lon, y = lat, z = depth), \n                          bins = 120, show.legend = TRUE) +\n  metR::geom_contour2(data = land, aes(x = lon, y = lat, z = depth), \n                      breaks = c(200), skip = 0 )+\n  scale_fill_gradientn(colours = land.pal, name = \"Topography\", \n                       trans = scales::sqrt_trans())+\n  ggspatial::layer_spatial(data = africa, fill = NA)+\n  coord_sf(xlim = c(38.9,40), ylim = c(-5.6,-4.1))+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 3: Elevation near the Pemba Channel, solid lines are contour lines"
  },
  {
    "objectID": "posts/gebco/index.html#the-modern-approach",
    "href": "posts/gebco/index.html#the-modern-approach",
    "title": "Getting GEBCO Bathymetry Data and glean the power of terra and tidyterra packages for raster and vector objects",
    "section": "The modern approach",
    "text": "The modern approach\nIn R ecosystem, the traditional handling of spatial data– raster and vector has changed dramatically in recent years. The widely used raster (Hijmans 2017) and sp (Bivand, Pebesma, and Gomez-Rubio 2013) packages dominated spatial analysis in R for decades. This has changed recent with the introduction of new and more convinient packages. These packages include terra, which has functions for creating, reading, manipulating, and writing raster data (Hijmans 2022). The terra package provides, among other things, general raster data manipulation functions that can easily be used to develop more specific functions. The package also implements raster algebra and most functions for raster data manipulation.\nterra replaces the raster package. The interfaces of terra and raster are similar, but terra is simpler, faster and can do more. The sister package to terra is tidyterra (Hernangómez 2023). tidyterra is a package that add common methods from the tidyverse (Wickham and Wickham 2017) for SpatRaster and SpatVectors objects created with the terra package. It also adds specific geom_spat*() functions for plotting these kind of objects with ggplot2 (Wickham 2016).\nLet’s load the packages\n\nrequire(terra)\nrequire(tidyterra)\n\nThen we use a function rast from terra package to read raster file from the working directory of the local machine.\n\ngebco = terra::rast(\"gebco.nc\")\n\n\n\n\nThen we print the the bathymetry file that we just imported\n\ngebco\n\nclass       : SpatRaster \ndimensions  : 4080, 3600, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : 35, 50, -15, 2  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : gebco_2021_n2.0_s-15.0_w35.0_e50.0.nc \nvarname     : elevation (Elevation relative to sea level) \nname        : elevation \nunit        :         m \n\n\nThe printed output is the metadata of the spatRaster with information that can assist to clear understand the file. I am not going into detail into it but he coord. ref. field shows lon/lat WGS 84, which is Geographic Coordinates with Datum WGS 84. If you just want to know the CRS from a SpatRaster, you just call crs() You also notice that the varname is the elevation, but in fact this dataset contain the altitude (elevation) for land and depth (bathymetry) for ocean and the unit of the measurement are in meters. Since the dataset is the raste, it provide the grid resolution of 0.004166667 degree, which is equivalent to a spatial resolution of 500 meters grid.\nThe geographical extent of the file is quite big range from longitude 35 to 50 and latitude -15 to 2. Since we only focus for the Pemba Channel, we need to crop the spatRaster to our area of interest. terra package has crop function for that but, thi function requires us to purse a file that has defined geographical extent. The extent can easily defined using ext function, also from terra package. The geographical extent of our area ranged from longitude 38 to 41 and latitude -6.2 to -3.\n\npemba.ext = terra::ext(38,41,-6.2,-3.8)\n\ngebco.pemba = terra::crop(\n  x = gebco, \n  y = pemba.ext\n  )\n\ngebco.pemba\n\nclass       : SpatRaster \ndimensions  : 576, 720, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : 38, 41, -6.2, -3.8  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : elevation \nmin value   :     -2610 \nmax value   :      2247 \n\n\nThe printed metadata indicated that the geographical extent meet what we defined above but it also provide the minimum and maximum values of the bathymetry and elevation values. tidyterra makes us use similar verbs of tidyverse to raster objects similar to tibble objects. For example, in the Pemba.gebco we can first filter and strip all elevation grid (elevation >0) and rename the elevation into depth with rename function and then use as.data.frame function from terra to convert spatRaster object to tibble. The chunk below summarise the above explanation;\n\ngebco.pemba.tb = gebco.pemba %>% \n  tidyterra::filter(elevation <= 0) %>% \n  tidyterra::rename(depth = elevation) %>% \n  terra::as.data.frame(xy = TRUE)\n\nIn addition of making tidyverse verbs that suits for tibble for manipulation of raster objects, tidyterra brings function for plotting raster object with ggplot2. A geom_spatraster function has enabled ability of plotting both elevation and bathymetry with color gradient that distinguish these layers in a more appealing form. It also has geom_spatraster_contour function for plotting contour from raster in ggplot. Simple feature and shapefile are also plotted with geom_spatvector function. The chunk below contains lines that illustrates the above explanation and rendered to generate Figure 4.\n\nggplot() +\n  geom_spatraster(data = gebco.pemba, show.legend = TRUE)+\n  geom_spatraster_contour(data = gebco.pemba, breaks = c(-200), \n                          linewidth = 1.2, color = \"black\") +\n  geom_spatvector(data = africa, fill = NA, color = \"black\")+\n  geom_sf_text(data = africa, aes(label = str_to_upper(CNTRY_NAME)), \n               size = 2.3, check_overlap = TRUE)+\n  coord_sf(xlim = c(38.9,40), ylim = c(-5.6,-4.1))+\n  theme_bw() +\n  theme(legend.position = \"right\", axis.title = element_blank())+\n  ggspatial::annotation_north_arrow(location = \"tr\", \n                                    height = unit(1.5, \"cm\"),\n                                    width = unit(1, \"cm\"),\n                                    pad_x = unit(0.25, \"cm\"),\n                                    pad_y = unit(0.25, \"cm\"),\n                                    rotation = NULL)+\n  ggspatial::annotation_scale(location = \"bl\")+\n  scale_fill_hypso_tint_c(\n    palette = \"gmt_globe\",\n    labels = scales::label_number(big.mark = \",\"),\n    limits = c(-2000,2000),\n    breaks = seq(-3000,3000,500),\n    guide = guide_colorbar(\n      title = \"Bathymetry and Elevation\",\n      title.hjust = .5,\n      direction = \"vertical\",\n      title.position = \"right\",\n      title.theme = element_text(angle = 90),\n      barheight = 10,\n      barwidth = .60)\n    )\n\n\n\n\nFigure 4: Bathymetry and elevation of the Pemba Channel. Solid black line is an isobar contour of 200 meter"
  },
  {
    "objectID": "posts/inset_main_map/index.html",
    "href": "posts/inset_main_map/index.html",
    "title": "Main and Inset maps with R",
    "section": "",
    "text": "In this post, We learn how we can make publication quality inset maps in R using ggplot2 package (Wickham 2016). When publishing scientific research in journals or presenting research work at a conference, showing maps of data collection sites or experimental locations is one of the key visualization elements. Maps of study sites or sampling locations can help the audience and readers to fathom the data in a better way. Mapping sounds fancy, but it needs substantial training and skill set to make high-quality maps that are reader-friendly and visually aesthetic.\nSometimes, the study sites are more dispersed and are easy to visualize in large geographic areas. However, in some cases, study sites are clustered, which makes it hard to show them on a broader scale. In that case, inset maps help us show the locations with reference to familiar geographical regions. An inset map is a smaller map featured on the same page as the main map. Traditionally, inset maps are shown at a larger scale (smaller area) than the main map. Often, an inset map is used as a locator map that shows the area of the main map in a broader, more familiar geographical frame of reference."
  },
  {
    "objectID": "posts/inset_main_map/index.html#focus-map",
    "href": "posts/inset_main_map/index.html#focus-map",
    "title": "Main and Inset maps with R",
    "section": "Focus Map",
    "text": "Focus Map\nNow, I’ll plot a focused map of Madagascar. We need to define the geographical extent of the area. For that purpose, we first need to identify the extent of the study sites and we can do that using extent function from **sf package;\n\nmadagascar %>% st_bbox()\n\n     xmin      ymin      xmax      ymax \n 42.71862 -25.60895  50.48378 -11.94543 \n\n\nThe printed results indicates tha maxima and minima of longitude and latitude, which define the geographical extent of the area. Using the min and max values of coordinates from the previous map, we can draw a polygon over the study sites and see if this extent can best visualize the data.\n\nmap.site = ggplot() +\n  ggspatial::layer_spatial(data = madagascar, fill = \"cyan4\", color = \"black\",size = .4)+\n  geom_sf(data = sampling.points, color = \"red\", size = 2)+\n  ggsci::scale_color_lancet()+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  coord_sf(xlim = c(43, 51), ylim = c(-18,-11))\n\nmap.site\n\n\n\n\nSampling points in the coastal waters of Madagascar Island\n\n\n\n\nAs you can see, the study sites are located on the northern part of Madagascar Island. However, to make a better sense of the study locations with reference to WIO region, we need to plot them on a focused scale."
  },
  {
    "objectID": "posts/inset_main_map/index.html#add-map-elements",
    "href": "posts/inset_main_map/index.html#add-map-elements",
    "title": "Main and Inset maps with R",
    "section": "Add Map elements",
    "text": "Add Map elements\nProfessional maps also include some elements like North Arrow and scale etc. We’ll add these components to our map as well. Besides that, I’ll also fill the non-land area with lightblue color for reference and distinction respectively Figure 1.\n\nmap.site = map.site +\n      guides(size = \"none\") +\n  ggspatial::annotation_north_arrow(location = \"tr\", \n                                    height = unit(1.2, \"cm\"),  \n                                    width = unit(.75, \"cm\"))+\n  ggspatial::annotation_scale(location = \"br\")\n\nmap.site\n\n\n\n\nFigure 1: Sampling points in the coastal waters of Madagascar Island"
  },
  {
    "objectID": "posts/inset_main_map/index.html#inset-map",
    "href": "posts/inset_main_map/index.html#inset-map",
    "title": "Main and Inset maps with R",
    "section": "Inset Map",
    "text": "Inset Map\nNow, I’ll create a full-scale map of WIO region with a red polygon showing the extent of study sites and the focused map. The code below produce Figure 2;\n\ninset.map = ggplot() +\n  ggspatial::layer_spatial(data = africa, fill = \"grey90\", color = \"grey90\") +\n  ggspatial::layer_spatial(data = wio, fill = \"grey60\", color = \"ivory\",size = .4)+\n  # geom_sf_text(data = wio, aes(label = country))+\n  ggrepel::geom_text_repel(data = wio.point.country, \n                           aes(x = lon, y = lat, label = country), size = 3)+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  geom_rect(aes(xmin = 43, xmax = 51, ymin = -18, ymax = -11), \n            color = \"red\", fill = NA, size = 1.2)+\n  coord_sf(xlim = c(20, 60), ylim = c(-40,15))+\n  theme_test() + \n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.title=element_blank(),\n        plot.margin = margin(0, 0, 0, 0, \"cm\"),\n        panel.background = element_rect(fill = \"lightblue\"))\n\ninset.map\n\n\n\n\nFigure 2: An inset map of the WIO region\n\n\n\n\nThis version looks better compared to the previous one. However, we need to add some information to give it a reference."
  },
  {
    "objectID": "posts/inset_main_map/index.html#final-map",
    "href": "posts/inset_main_map/index.html#final-map",
    "title": "Main and Inset maps with R",
    "section": "Final Map",
    "text": "Final Map\nNow, to combine both maps where the map of WIO region is inset on the upper left corner we use the function from cowplot package. a draw_plot function allow to places a plot somewhere onto the drawing canvas that is established using ggdraw function also from cowplot package (Wilke 2018). By default, coordinates run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. The function also allows us to specify the size of the inset map using the width and height functions.\n\ncowplot::ggdraw(plot = map.site) +\n  cowplot::draw_plot(inset.map, x = .1, y = .13, width = .4, height = .5)\n\n\n\n\nFigure 3: Map of the northwest side of Madagascar. An inset map indicate the location of the study area in the Western Indian Ocean region\n\n\n\n\n\nSummary\nTherefore , a final map shown in Figure 3 shows the locations of study sites with reference to the country and provinces and is more professional."
  },
  {
    "objectID": "posts/inset_main_map/index.html#last-updated",
    "href": "posts/inset_main_map/index.html#last-updated",
    "title": "Main and Inset maps with R",
    "section": "Last updated",
    "text": "Last updated\n\n\n\n[1] \"16 February 2023\""
  },
  {
    "objectID": "posts/newvisuals/index.html",
    "href": "posts/newvisuals/index.html",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "",
    "text": "On April, 2022, the government of the United Republic of Tanzania approved permission of TAMISEMI to recruit and employ 9,800 primary and secondary school teachers, and 7,612 health experts. A total of 165,948 applied for the positions where for Health Cadres is 42,558, and the Teaching Cadre is 123,390.\nAllocation of health and Teaching position in Centers of providing health services and schools have considered the requirements of employees in the respective regions. The allocation of new employees was based on the needs of employees in Councils with new Hospitals, Centers New health, and new completed clinics that faced shortage of medical staffs.\nIn addition, teachers were allocated to councils based on the division of space for each subject, and the qualifications. In this post, we are going to discuss"
  },
  {
    "objectID": "posts/newvisuals/index.html#tilemap",
    "href": "posts/newvisuals/index.html#tilemap",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Tilemap",
    "text": "Tilemap\n\ndistrict.tb %>% \n  hchart(type = \"tilemap\", hcaes(x = lon, y = lat, name = district, group = zone)) %>% \n  hc_chart(type = \"tilemap\") %>% \n  hc_plotOptions(\n    series = list(\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        color = \"white\",\n        style = list(textOutline = FALSE)\n      )\n    )\n  ) %>% \n  hc_tooltip(\n    headerFormat = \"\",\n    pointFormat = \"<b>{point.name}</b> is in <b>{point.region_nam}</b>\"\n    ) %>% \n  hc_xAxis(visible = FALSE) %>% \n  hc_yAxis(visible = FALSE) %>% \n  hc_size(height = 800, width = 600)"
  },
  {
    "objectID": "posts/newvisuals/index.html#packedbubble",
    "href": "posts/newvisuals/index.html#packedbubble",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "packedbubble",
    "text": "packedbubble\nA bubble chart requires three dimensions of data; the x-value and y-value to position the bubble along the value axes and a third value for its volume. Packed Bubble charts have a simpler data structure, a flat, one-dimensional array with volumes is sufficient. The bubble’s x/y position is automatically calculated using an algorithm that packs the bubbles in a cluster. The series data point configuration has support for setting colors and label values. Drag’n drop feature was also added to give the user a chance to quickly move one bubble between series and then check how their relations will change.\n\n\n\n\nwalimu.lga = walimu.clean %>% \n  separate(halmashauri, into = c(\"district\", \"b\", \"c\"), sep = \" \") %>% \n  unite(col = code, b:c, sep = \" \") %>% \n  mutate(lga = case_when(code == \"District Council\"~\"DC\",\n                         code == \"Municipal Council\"~\"MC\",\n                         code == \"City Council\"~\"CC\",\n                         code == \"Town Council\"~\"TC\",\n                         code == \"Mikindani Municipal\"~\"MC\",\n                         code == \"Ujiji Municipal\"~\"MC\"))\n\n  \nwalimu.lga.freq = walimu.lga %>% \n  group_by(district, lga) %>% \n  count()\n\ndistrict.walimu = district.tb %>% \n  left_join(walimu.lga.freq) %>% \n  select(region_nam, zone, n, district)%>% \n  separate(district, into = c(\"code\", \"aa\"), sep = 3, remove = FALSE) %>% \n  mutate(code = str_to_upper(code)) %>% \n  select(-aa)\n\n\nhc = district.walimu %>% \n   hchart(type = \"packedbubble\", hcaes(name = district, value = n, group = zone))\n\n\n\nq95 <- as.numeric(quantile(district.walimu$n, .95, na.rm = TRUE))\n\nhc %>% \n  hc_tooltip(\n    useHTML = TRUE,\n    pointFormat = \"<b>{point.name}:</b> {point.n}\"\n  ) %>% \n  hc_plotOptions(\n    packedbubble = list(\n      maxSize = \"150%\",\n      zMin = 0,\n      layoutAlgorithm = list(\n        gravitationalConstant =  0.05,\n        splitSeries =  TRUE, # TRUE to group points\n        seriesInteraction = TRUE,\n        dragBetweenSeries = TRUE,\n        parentNodeLimit = TRUE\n      ),\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        filter = list(\n          property = \"y\",\n          operator = \">\",\n          value = q95\n        ),\n        style = list(\n          color = \"black\",\n          textOutline = \"none\",\n          fontWeight = \"normal\"\n        )\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/newvisuals/index.html#sankey",
    "href": "posts/newvisuals/index.html#sankey",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Sankey",
    "text": "Sankey\nA sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called nodes and the connections are called links.Sankey diagrams can also visualize the energy accounts, material flow accounts on a regional or national level, and cost breakdowns.[1] The diagrams are often used in the visualization of material flow analysis.\nSankey diagrams emphasize the major transfers or flows within a system. They help locate the most important contributions to a flow. They often show conserved quantities within defined system boundaries.\n\n   quest.tb =  walimu.clean %>% \n      group_by(kiwango_cha_elimu, jinsi) %>% \n      summarise(value = n(), .groups = \"drop\") %>% \n      rename(source = 2, target = 1) %>% \n      filter(value > 100)%>% \n      as.data.frame()\n        \n    \n    # From these flows we need to create a node data frame: it lists every entities involved in the flow\n    nodes <- data.frame(name=c(as.character(quest.tb$source), \n                               as.character(quest.tb$target)) %>% \n                          unique())\n    \n    nodes = quest.tb %>% \n      select(-value) %>% \n      pivot_longer(cols = source:target) %>% \n      distinct(value) %>% \n      rename(name = 1) %>% \n      as.data.frame()\n    \n    # With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.\n    quest.tb$IDsource=match(quest.tb$source, nodes$name)-1 \n    quest.tb$IDtarget=match(quest.tb$target, nodes$name)-1\n    \n    \n    # Make the Network \n    networkD3::sankeyNetwork(Links = quest.tb, \n                             Nodes = nodes,\n                             Source = \"IDsource\", \n                             Target = \"IDtarget\",\n                             Value = \"value\", \n                             NodeID = \"name\", \n                             fontFamily = \"Myriad Pro\",\n                             LinkGroup = \"source\",\n                             sinksRight=FALSE,\n                             # height = 600, width = 800,\n                             # colourScale=ColourScal,\n                             nodeWidth=30, \n                             iterations = 5,\n                             fontSize=14, \n                             nodePadding=30, \n                             width = 1000, \n                             height = 400)"
  },
  {
    "objectID": "posts/newvisuals/index.html#cited-references",
    "href": "posts/newvisuals/index.html#cited-references",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Cited references",
    "text": "Cited references"
  },
  {
    "objectID": "posts/r_python/index.html",
    "href": "posts/r_python/index.html",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "",
    "text": "If you work with data science, R and Python must be the two programming languages that you use the most. Both R and Python are quite robust languages and either one of them is actually sufficient to carry out the data analysis task. However, instead of considering them as tools that supplement each other, more often you will find people dealing with data claim one language to be better than the other. Truth be told, R and Python are excellent tools in ther own right but are often conceived as rivals. One major reason for such view lies on the experts. Because data analysts have divided the data science field into camps based on the choice of the programming language they are familiar with.\nThere major two camps—R camp and Python camp—and history is the testimony that camps can not live in harmony. Members of both camps believe that their choice of language . Honestly, I do not hold to their opinion, but rather wish I have skills for both languages. So, whether you have in R or Python camp, one thing you will notice is that the problem we have in data science is simply that divergence does not lie with the tools but with the people using those tools.\nI believe there are few people in the Data Science community who use both R and Python in their analytical workflow. But majority are committed to only one programming language, but wish they had access to some functions from other language. Therefore, there is no reason that hold us to stick using this programming language or the other. Our ultimate goal should be to do better analytics and derive better insights and choice of which programming language to use should not hinder us from reaching our goals.\nThe questions that always resolute in my mind is whether can we utilize the statistical power of R along with the programming capabilities of Python?. Its undeniable truth that there are definitely some high and low points for both languages and if we can utilize the strength of both, we can end up dong a much better job. Thanks to Kevin Ushey and his colleges (2020) for developing a reticulate package. reticulate package provides a comprehensive set of tools that allows to work with R and Python in the same environment. The reticulate package provide the following facilities;\n\nCalling Python from R in a variety of ways including rmarkdown, sourcing, Python scripts, importing Python modules and using Python interactively within and R session.\nTranslation between R and Python objects—for example r_to_py function allows to construct R to Pandas data frame and py_to_r() function convert python object like data frame, matrix and etc to R\nFlexible binding to different versions of Python including virtual environments and conda environment."
  },
  {
    "objectID": "posts/r_python/index.html#tibble-to-pandas-dataframe",
    "href": "posts/r_python/index.html#tibble-to-pandas-dataframe",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Tibble to Pandas Dataframe",
    "text": "Tibble to Pandas Dataframe\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We first need to import the dataset from the package where is stored into the R session. let us load the packages that we are glint to use in this post.\n\nrequire(tidyverse)\nrequire(reticulate)\n\nOnce we have loaded the package, we then import the dataset.\n\npeng = palmerpenguins::penguins\npeng\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nThe printed result suggest that this dataset is a tibble format—a modern data frame from the tidyverse ecosystem (Wickham and Wickham 2017). Let’s visualize the dataset with pair plot in GGally package (Schloerke et al. 2020)\n\npeng %>% \n  filter(!is.na(sex)) %>% \n  GGally::ggpairs(columns = 3:6, aes(color = sex))\n\n\n\n\nFigure 1: Matrix of numerical variable in the penguins dataset\n\n\n\n\nHowever, our interest in this post is plotting this dataset using python. Therefore, we need to first import three key libraries that we will use throughtout this post. The chunk below highlight these packages and how to import them inside the python chunk.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nOnce the tibble file is in the environment, we need to convert from tibble data frame into pandas dataframe. Make a copy of pandas dataframe from tible with the r. function\n\n\n\n\n\n\nInfo\n\n\n\nnote that conversion from tibble to pandas data frame must be done in the Python chunk and not R chunk\n\n\n\npeng = r.peng\npeng\n\n       species     island  bill_length_mm  ...  body_mass_g     sex  year\n0       Adelie  Torgersen            39.1  ...         3750    male  2007\n1       Adelie  Torgersen            39.5  ...         3800  female  2007\n2       Adelie  Torgersen            40.3  ...         3250  female  2007\n3       Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4       Adelie  Torgersen            36.7  ...         3450  female  2007\n..         ...        ...             ...  ...          ...     ...   ...\n339  Chinstrap      Dream            55.8  ...         4000    male  2009\n340  Chinstrap      Dream            43.5  ...         3400  female  2009\n341  Chinstrap      Dream            49.6  ...         3775    male  2009\n342  Chinstrap      Dream            50.8  ...         4100    male  2009\n343  Chinstrap      Dream            50.2  ...         3775  female  2009\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/r_python/index.html#plotting",
    "href": "posts/r_python/index.html#plotting",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Plotting",
    "text": "Plotting\n\nPairplot\n\nfig = plt.figure()\nsns.pairplot(data = peng, hue = \"species\")\n\n\n\n\nFigure 2: The pairplot of penguins dataset\n\n\n\nplt.show()\n\n\n\n\nFigure 3: The pairplot of penguins dataset\n\n\n\n\n\n\nScatter plot\n\n\nfig = plt.figure()\n\nsns.scatterplot(\n  data = peng, \n  x = \"bill_length_mm\", \n  y = \"bill_depth_mm\", \n  hue = \"island\"\n  )\n  \n\nplt.xlabel(\"Length (mm)\")\nplt.ylabel(\"Depth (mm)\")\nplt.legend(loc = \"lower right\")\nplt.show()\n\n\n\n\nFigure 4: Scatterplot of length and depth of penguins\n\n\n\n\n\n\nHistogram\n\nfig = plt.figure()\nsns.histplot(data = peng, x = \"bill_depth_mm\", color = \"steelblue\")\nplt.xlabel(\"Bill depth (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nFigure 5: Histogram of bill depth\n\n\n\n\n\nfig = plt.figure()\nsns.histplot(data = peng[peng.island == \"Dream\"], x = \"bill_depth_mm\", color = \"steelblue\", label = \"Dream\")\nsns.histplot(data = peng[peng.island == \"Biscoe\"], x = \"bill_depth_mm\", color = \"darkorchid\", label = \"Biscoe\")\nsns.histplot(data = peng[peng.island == \"Torgersen\"], x = \"bill_depth_mm\", color = \"lightblue\", label = \"Torgersen\")\nplt.xlabel(\"Bill depth (mm)\")\nplt.ylabel(\"Frequency\")\nplt.legend(loc = \"upper right\")\nplt.show()\n\n\n\n\nFigure 6: Histogram of bill depth\n\n\n\n\n##3 Density\n\nfig = plt.figure()\nsns.kdeplot(data = peng, x = \"bill_length_mm\", shade = \"steelblue\")\nplt.axvline(43.40, color=\"k\", linestyle=\"--\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 7: Density distribution of of bill depth\n\n\n\n\nThe difference of bill length among the three species is interesting. Let’s look at the density plots of these species:\n\nfig = plt.figure()\nsns.kdeplot(data = peng[peng.species == \"Adelie\"], x = \"bill_length_mm\", label = \"Adelie\", shade = \"steelblue\")\nsns.kdeplot(data = peng[peng.species == \"Chinstrap\"], x = \"bill_length_mm\", label = \"Chinstrap\", shade = \"orange\")\nsns.kdeplot(data = peng[peng.species == \"Gentoo\"], x = \"bill_length_mm\", label = \"Gentoo\", shade = \"green\")\nplt.legend(loc = \"upper right\")\nplt.xlabel(\"Bill length (mm)\")\n\nplt.show()\n\n\n\n\nFigure 8: Density plot of bill length by species\n\n\n\n\n\n\nBoxplot\n\n\nfig = plt.figure()\nsns.boxplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 9: Boxplot of bill depth by island and species\n\n\n\n\n\n\nfig = plt.figure()\nsns.violinplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 10: Violin plots of bill depth by island and species\n\n\n\n\n\n\nfig = plt.figure()\nsns.violinplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nsns.boxplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 11: Violin and Boxplot of bill depth by island and species"
  },
  {
    "objectID": "posts/r_python/index.html#pandas-dataframe-to-tibble",
    "href": "posts/r_python/index.html#pandas-dataframe-to-tibble",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Pandas Dataframe to Tibble",
    "text": "Pandas Dataframe to Tibble\nThe power of multilingual is clearly demonstrated with Rstudio, which allows you to swap dataset between R and python. In the previous section we created a peng dataset in python from R. In this session we are going to use this python dataset and convert it back to R. A py function from reticulate package is used as the chunk below illustrates:\n\npeng.r = reticulate::py$peng\npeng.r %>% as_tibble()\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <dbl>   <dbl> <fct> <dbl>\n 1 Adelie  Torgersen           39.1          18.7     1.81e2  3.75e3 male   2007\n 2 Adelie  Torgersen           39.5          17.4     1.86e2  3.8 e3 fema~  2007\n 3 Adelie  Torgersen           40.3          18       1.95e2  3.25e3 fema~  2007\n 4 Adelie  Torgersen           NA            NA      -2.15e9 -2.15e9 <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3     1.93e2  3.45e3 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6     1.9 e2  3.65e3 male   2007\n 7 Adelie  Torgersen           38.9          17.8     1.81e2  3.62e3 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6     1.95e2  4.68e3 male   2007\n 9 Adelie  Torgersen           34.1          18.1     1.93e2  3.48e3 <NA>   2007\n10 Adelie  Torgersen           42            20.2     1.9 e2  4.25e3 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\n\n\n\n\n\n\nInfo\n\n\n\nnote that conversion from pandas to tibble data frame must be done in the R chunk and not Python chunk"
  },
  {
    "objectID": "posts/satelliteData/index.html",
    "href": "posts/satelliteData/index.html",
    "title": "Getting and Processing Satellite Data Made Easier in R",
    "section": "",
    "text": "The amount of data being generated by satellites has soared in recent years. The proliferation of remote sensing data can be explained by recent advancements in satellite technologies. However, according to Febvre1, this advancement set another challenge of handling and processing pentabyte of data satellite generates. Thanks to ERDDAP, for being in front-line to overcome this challenge. We can obtain satellite data within the geographical extent of the area we are interested. The ERDDAP server provides simple and consistent way to subset and download oceanographic datasets from satellites and buoys to your area of interest. ERDDAP is providing free public access to huge amounts of environmental datasets. Current the ERDDAP server has a list of 1385 datasets.This server allows scientist to request data of a specific area of interest in two forms—grids or tabular data.\nxtractomatic is R package developed by Roy Mendelssohn2 that works with ERDDAP servers. The xtractomatic functions were originally developed for the marine science, to match up ship track and tagged animals with satellites data. Some of the satellite data includes sea-surface temperature, sea-surface chlorophyll, sea-surface height, sea-surface salinity, and wind vector. However, the package has been expanded and it can now handle gridded environmental satellite data from ERDAPP server.\nIn this post I will show two main routine operations extractomatic package can do. The first one is to match up drifter observation data with modis sea surface temperature satellite data using xtracto function. The second operation involves xtracto_3D to extract gridded data of sea surface temperature and chlorophyll a, both acquired by MODIS sensors."
  },
  {
    "objectID": "posts/seabornjointplot/index.html",
    "href": "posts/seabornjointplot/index.html",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going focus on jointplot. jointplot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. Seaborn has nifty function called jointplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seabornjointplot/index.html#loading-libraries",
    "href": "posts/seabornjointplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seabornjointplot/index.html#dataset",
    "href": "posts/seabornjointplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year.\nThe joint plot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. To plot a joint plot, you need to call the jointplot() function. The following script plots a joint plot for bill_length_mm and bill_depth_mm columns of the df dataset.\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005A814D00>\n\nplt.show()\n\n\n\n\nAssigning a hue variable will add conditional colors to the scatter plot and draw separate density curves (using kdeplot()) on the marginal axes. In this case we specify hue = \"island\"\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue = \"island\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D55D160>\n\nplt.show()\n\n\n\n\nSeveral different approaches to plotting are available through the kind parameter. Setting kind=“kde” will draw both bivariate and univariate KDEs:\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", kind=\"kde\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D707310>\n\nplt.show()\n\n\n\n\nSet kind=\"reg\" to add a linear regression fit (using regplot()) and univariate KDE curves:\n\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"reg\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D8163D0>\n\nplt.show()\n\n\n\n\n\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"hex\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005DC64610>\n\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html",
    "href": "posts/seabornVisualization/index.html",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "",
    "text": "Wikipedia (2023) describe data visualization as an interdisciplinary field that deals with the graphic representation of data and information. It is a particularly efficient way of communicating when the data are processed to generate information that is shared.\nIt is also the study of visual representations of abstract data to reinforce human cognition using common graphics, such as charts, plots, infographics, maps, and even animations. The abstract data include both numerical and non-numerical data, such as text and geographic information.\nFurthermore, it is related to infographics and scientific visualization to identify important patterns in the data that can be used for organizational decision making. Visualizing data graphically can reveal trends that otherwise may remain hidden from the naked eye.\nIn the following is the series of post that focuse plotting with seaborn library in Python, we will learn the most commonly used plots using Seaborn library in Python (Waskom 2021; Bisong and Bisong 2019). We will also touches on different types of plots using Maplotlib (Bisong and Bisong 2019), and Pandas (Betancourt et al. 2019) libraries. In this post we will focus on the distplot."
  },
  {
    "objectID": "posts/seabornVisualization/index.html#loading-libraries",
    "href": "posts/seabornVisualization/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#dataset",
    "href": "posts/seabornVisualization/index.html#dataset",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We first need to import the dataset from the package where is stored into the R session. let us load the packages that we are glint to use in this post.\n\npengr = palmerpenguins::penguins\npengr\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nOnce the tibble file is in the environment, we need to convert from tibble data frame into pandas dataframe. Make a copy of pandas dataframe from tibble with the r. function. please note that the conversion of tibble data frame to pandas data frame must be inside the Python chunk as chunk below;\n\npengp = r.pengr\n\nLet’s use head function to explore the first five rows on the converted penguin pandas data frame\n\npengp.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nThe pengp dataset comprise various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains eight variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year. You do not need to download this dataset as it comes with the palmerpenguin library in R. We will use this dataset to plot some of the seaborn plots. Lets begin plotting\nAlternatively, you can load the package as\n\n\ndf = sns.load_dataset(\"penguins\")\ndf.head()\n\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#univariable-distribution",
    "href": "posts/seabornVisualization/index.html#univariable-distribution",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Univariable distribution",
    "text": "Univariable distribution\nThe distplot, also commonly refers as the distribution plot, is widely used to plot a histogram of data for a specific variable in a dataset. To make this plot seaborn has a dedicated function called displot\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nThe new displot functions support the kernel density estimate line, by passing kde=True\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kde = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nTo change the distribution from counts to density, we simply parse an argument stat=\"density\"\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kde = True, stat = \"density\")\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#kdeplot",
    "href": "posts/seabornVisualization/index.html#kdeplot",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "kdeplot",
    "text": "kdeplot\nWhen you want to draw the density plot alone without overlay it to the histogram as presented using the displot function, seaboarn has a kdeplot function\n\n\nfig = plt.figure()\nsns.kdeplot(pengp.bill_length_mm)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\ndisplot still can draw the kde plot, however, you need to parse an argument kind=\"kde\" in displot:\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kind = \"kde\", rug = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nIf you parse rug = True function, wll add the rug in the plots\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kind = \"kde\", rug = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\naa = pengp[[\"bill_length_mm\", \"bill_depth_mm\"]]\n\nfig = plt.figure()\nsns.kdeplot(data = aa)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nPlot conditional distributions with hue mapping of a second variable. Unlike the previous plot, for this kind you need to specify the x-variable and the hue in the dataset;\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nStack the conditional distributions by simply parsing argument multiple = \"stack\"\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\", multiple = \"stack\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nmultiple = \"fill\" simply normalize the stacked distribution at each value in the grid\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\", multiple = \"fill\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nEstimate the cumulative distribution function(s), normalizing each subset:\n\n\nfig = plt.figure()\n\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\",  cumulative=True, common_norm=False, common_grid=True)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#bivariate-distribution",
    "href": "posts/seabornVisualization/index.html#bivariate-distribution",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Bivariate distribution",
    "text": "Bivariate distribution\nFor bivariates, we are going to use geyser dataset. Old Faithful is a cone geyser in Yellowstone National Park in Wyoming, United States. It is a highly predictable geothermal feature and has erupted every 44 minutes to two hours since 2000. We do not need to download this dataset as it comes with the seaborn package.\n\ngeyser = sns.load_dataset(\"geyser\")\ngeyser.head()\n\n   duration  waiting   kind\n0     3.600       79   long\n1     1.800       54  short\n2     3.333       74   long\n3     2.283       62  short\n4     4.533       85   long\n\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\")\nplt.show()\n\n\n\n\nMap a third variable with a hue semantic to show conditional distributions:\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\")\nplt.show()\n\n\n\n\nFill the contour by parsing fill = True\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\", fill = True)\nplt.show()\n\n\n\n\nShow fewer contour levels, covering less of the distribution by parsing a levels and thresh functions in the kdeplot:\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\", levels = 5, thresh = .2)\nplt.show()"
  },
  {
    "objectID": "posts/seaborn_barplot/index.html",
    "href": "posts/seaborn_barplot/index.html",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going learn barplot. The bar plot is used to capture the relationship between a categorical and numerical column. For each unique value in a categorical column, a bar is plotted, which by default, displays the mean value for the data in a numeric column specified by the bar plot. Seaborn has nifty function called barplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seaborn_barplot/index.html#loading-libraries",
    "href": "posts/seaborn_barplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_barplot/index.html#dataset",
    "href": "posts/seaborn_barplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year.\nNext, we will call the barplot() function from the Seaborn library to plot a bar plot that displays the average length of penguin species.\n\nfig = plt.figure()\nsns.barplot(data=df, x=\"species\", y=\"bill_length_mm\")\nplt.show()\n\n\n\n\nFigure 1: Average length by island\n\n\n\n\nThe Figure 1 shows that the average length of penguins from the three island. We can parse an argument hue = \"sex\" to stack the plot as Figure 2 shows.\n\n\nfig = plt.figure()\nsns.barplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.show()\n\n\n\n\nFigure 2: Average length by species and island"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html",
    "href": "posts/seaborn_boxplot/index.html",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going learn how to plot boxplot in seaborn. The boxplot is used to plot the quantile information of numeric variables in a dataset. To plot a box plot, the boxplot() function is used. To plot a horizontal boxplot, the variable name of the dataset is passed to the x-axis."
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#loading-libraries",
    "href": "posts/seaborn_boxplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#dataset",
    "href": "posts/seaborn_boxplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year."
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#boxplot",
    "href": "posts/seaborn_boxplot/index.html#boxplot",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Boxplot",
    "text": "Boxplot\nNext, we will call the boxplot() function from the Seaborn library to plot a bar plot that displays the average length of penguin species.\n\nfig = plt.figure()\nsns.boxplot(data=df, x=\"species\", y=\"bill_length_mm\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 1: Boxplot length by species\n\n\n\n\nThe Figure 1 shows that the average length of penguins from the three island. We can parse an argument hue = \"sex\" to stack the plot as Figure 2 shows.\n\n\nfig = plt.figure()\nsns.boxplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 2: Boxplot of length by species and island\n\n\n\n\nDraw a vertical boxplot with nested grouping by two variables:\n\n\nfig = plt.figure()\nsns.boxplot(data=df, y=\"species\", x=\"bill_length_mm\", hue = \"sex\")\nplt.xlabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 3: Boxplot of length by species and island"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#violin",
    "href": "posts/seaborn_boxplot/index.html#violin",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Violin",
    "text": "Violin\nViolin plots are similar to Box plots. However, unlike Box plots that plot quantile information, the Violin plots plot the overall distribution of values in the numeric columns. The following script plots two Violin plots for the passengers traveling alone and for the passengers traveling along with another passenger. The violinplot() function is used to plot a swarm plot with Seaborn.\n\n\nfig = plt.figure()\nsns.violinplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 4: Violin plot of length by species and island\n\n\n\n\nWe can split the violin plot with split = True argument. One key advantage of splited violins is that take up less space (Figure 5):\n\n\nfig = plt.figure()\nsns.violinplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\", \nsplit = True)\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 5: Violin plot of length by species and island"
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html",
    "href": "posts/seaborn_lineplot/index.html",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "",
    "text": "In visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going focus on jointplot. jointplot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. Seaborn has nifty function called jointplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html#loading-libraries",
    "href": "posts/seaborn_lineplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html#dataset",
    "href": "posts/seaborn_lineplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a flights dataset, which has 10 years of monthly airline passenger data. We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset('flights')\n\n\ndf\n\n     year month  passengers\n0    1949   Jan         112\n1    1949   Feb         118\n2    1949   Mar         132\n3    1949   Apr         129\n4    1949   May         121\n..    ...   ...         ...\n139  1960   Aug         606\n140  1960   Sep         508\n141  1960   Oct         461\n142  1960   Nov         390\n143  1960   Dec         432\n\n[144 rows x 3 columns]\n\n\nA printed df dataset shows that a flight dataset is made up of three variables — year, month, and number of passenger between January 1949 and December 1960, which are arranged in long format. To draw a line plot using long-form data, we simply assign the x and y variables\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 1: Annual number of flight passenger\n\n\n\n\nHowever, looking at Figure 2, we notice that the confidence level is aslo plotted thought we did not specify them. The reason is that each year has twelve records of monthly number of passengers, which when plotted are also estimated to show the variability of passenger on that year. So if we want to draw only the line, we may filter a specific year, for this case I only want to plot passengers for July during the same period.\nFirst, we need to filter the dataset to July and assign a new dataset as df_july. The chunk below shows a line of code that filter passenger between 1949 to 1960 for July only.\n\ndf_july = df[df.month == 'Jul']\n\nThen plot.\n\nfig = plt.figure()\nsns.lineplot(data=df_july, x=\"year\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 2: Number of flight passenger for July during the period\n\n\n\n\nBecause I still learning how to deal with time in python, I simply switch to R as quarto allows me to swap code for the two languages within the same environment. I switch to R code and then convert the panda data frame to tibble using the reticulate package (Ushey, Allaire, and Tang 2020). The package allows us to convert pandas dataframe to tibble using a py function within R chunk;\n\ntb = reticulate::py$df\n\nThen within R chunk, we can add a new column date contain date for each month\n\ntb = tb |>\n  dplyr::mutate(date = \nseq(lubridate::my(011949), lubridate::my(121960), by = \"month\"))\n\ntb |> head()\n\n  year month passengers       date\n1 1949   Jan        112 1949-01-01\n2 1949   Feb        118 1949-02-01\n3 1949   Mar        132 1949-03-01\n4 1949   Apr        129 1949-04-01\n5 1949   May        121 1949-05-01\n6 1949   Jun        135 1949-06-01\n\n\nThe printed tibble has one added variable date. Since we have created this dataset, we can convert it back to python pandas dataframe by simply using r. funtion within the python chunk;\n\npdf = r.tb\npdf\n\n       year month  passengers        date\n0    1949.0   Jan       112.0  1949-01-01\n1    1949.0   Feb       118.0  1949-02-01\n2    1949.0   Mar       132.0  1949-03-01\n3    1949.0   Apr       129.0  1949-04-01\n4    1949.0   May       121.0  1949-05-01\n..      ...   ...         ...         ...\n139  1960.0   Aug       606.0  1960-08-01\n140  1960.0   Sep       508.0  1960-09-01\n141  1960.0   Oct       461.0  1960-10-01\n142  1960.0   Nov       390.0  1960-11-01\n143  1960.0   Dec       432.0  1960-12-01\n\n[144 rows x 4 columns]\n\n\nThen we can now plot a line and use the new column date we just created instead of year.\n\nfig = plt.figure()\nsns.lineplot(data=pdf, x=\"date\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 3: Monthly Number of flight passengers\n\n\n\n\nFigure 3 and Figure 2 are almost similar but while Figure 3 used year as x-axis, Figure 3 used date in the x-axis. You can see that Figure 3 clearly shows the variability of passenger within the year, which was not possible with Figure 2.\nWe can assign a grouping semantic (hue, size, or style) to plot separate lines\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\", hue = \"month\")\nplt.ylabel(\"Passengers\")\nplt.show()\n\n\n\n\nFigure 4: Number of flight passenger by month during the period\n\n\n\n\nSimilarly, we can assign multiple semanti variables of the same variable that makes the plot more appealing ore easily to distinguish between the assigned parameters.\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\", hue = \"month\", style = \"month\")\nplt.ylabel(\"Passengers\")\nplt.show()\n\n\n\n\nFigure 5: Number of flight passenger by month during the period"
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html",
    "href": "posts/simplefeature_pointline/index.html",
    "title": "Manipulating Simple features: point to polyline",
    "section": "",
    "text": "In this post we are going to learn one of the key skills that one dealing with spatial data need to know. That’s is how to read a file that contain geographical information (longitude and latitude) and associated attribute information for each location. In its native tabular form, the geographical information are of no use at all until are converted and presented as spatial information. While industrial leading software like ArcMAP and QGIS provide the tools necessary for that conversion, but these software requires us to click the function every time we want to use them to do the process.\nUnlike ArcMAP and QGIS, R software has tools that allows automate processing and conversion of geographical information into spatial data in an intuitive approach, which offers a possible way to reproduce the same. Before we proceed with our task, we need to load some of the packages whose function are needed for this post. We simply load these packages using require function of R (r-base?). The chunk below load these packages;"
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#dataset",
    "href": "posts/simplefeature_pointline/index.html#dataset",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use Argo float dataset. The step involved to prepare this dataset are explained in Getting and Processing Satellite Data Made Easier in R Let’s load this dataset. We import this dataset using read_csv function from readr package (Wickham, Hester, and Francois 2017)\n\nargo.surface.tb = read_csv(\"../data/argo_profile_cast_location.csv\") %>% \n  janitor::clean_names()\n\nargo.surface.tb\n\n# A tibble: 217 x 8\n    scan date       time     longitude latitude depth temperature salinity\n   <dbl> <date>     <time>       <dbl>    <dbl> <dbl>       <dbl>    <dbl>\n 1   1.5 2008-10-28 01:43:57      64.0    -21.6     0        23.5     35.4\n 2   1.5 2008-11-07 02:17:07      64.2    -21.3     0        24.6     35.0\n 3   1.5 2008-11-17 02:30:18      64.1    -21.0     0        25.0     35.0\n 4   1.5 2008-11-27 02:37:35      63.6    -21.1     0        25.7     35.0\n 5   1.5 2008-12-07 05:03:59      63.4    -21.3     0        26.1     35.0\n 6   1.5 2008-12-17 03:03:40      63.3    -21.5     0        27.0     35.1\n 7   1.5 2008-12-27 05:09:53      63.5    -21.6     0        26.7     35.1\n 8   1.5 2009-01-06 02:40:37      63.9    -21.6     0        28.4     35.1\n 9   1.5 2009-01-16 04:57:04      64.4    -21.5     0        28.0     35.1\n10   1.5 2009-01-26 02:30:53      64.8    -21.2     0        28.1     35.0\n# ... with 207 more rows\n\n\nThe dataset has 217 rows and eight variables with date and time stamp the argo surfaced and pushed profile data into the satellite after roving the water column after ten days. The geographical location (longitude and latitude) of the float when was at the surface with the associated temperature and salinity at the surface represent with zero depth."
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#simple-feature",
    "href": "posts/simplefeature_pointline/index.html#simple-feature",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Simple feature",
    "text": "Simple feature\nThe main task of this post is to convert this dataset that contain 217 geographical information into simple feature. Wikipedia contributors (2022) describe simple feature as set of standards that specify a common storage and access model of geographic feature made of mostly two-dimensional geometries used by geographic information systems. In general, simple feature is a model of a non-topological way to store geospatial data in a database. The three common types of simple feature are point, lines and polygon, each represent a particular type of geographical feature on earth. Further, simple features refers a formal standard that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\nPebesma (2018) developed a simple feature packages (sf) that is dedicated to deal with simple feature objects in R. The packages has hundred of functions that make dealing with spatial information in R environment much easier than before. Its not the focus of this post to describe the package, but if you wish to dive on this topic you can simply consult book such as geocompuation in R (Lovelace, Nowosad, and Muenchow 2019).\nFor this post we begin by looking on how we can convert tabular data with geographical information into simple feature. That is achieved using st_as_sf function of sf package (Pebesma 2018) and parse the argument coords = c(\"longitude\", \"latitude\") that bind the geographical coordinate and argument the datum crs = 4326 to define the datuma and the geographical coordinate system, which is WGS 1984 (epsg = 4326). The chunk below has code of the above description;\n\nargo.surface.sf = argo.surface.tb %>% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nargo.surface.sf\n\nSimple feature collection with 217 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 39.94 ymin: -22.623 xmax: 65.075 ymax: -1.233\nGeodetic CRS:  WGS 84\n# A tibble: 217 x 7\n    scan date       time     depth temperature salin~1         geometry\n * <dbl> <date>     <time>   <dbl>       <dbl>   <dbl>      <POINT [°]>\n 1   1.5 2008-10-28 01:43:57     0        23.5    35.4  (64.03 -21.644)\n 2   1.5 2008-11-07 02:17:07     0        24.6    35.0  (64.198 -21.28)\n 3   1.5 2008-11-17 02:30:18     0        25.0    35.0  (64.068 -21.01)\n 4   1.5 2008-11-27 02:37:35     0        25.7    35.0 (63.607 -21.102)\n 5   1.5 2008-12-07 05:03:59     0        26.1    35.0 (63.429 -21.253)\n 6   1.5 2008-12-17 03:03:40     0        27.0    35.1 (63.322 -21.458)\n 7   1.5 2008-12-27 05:09:53     0        26.7    35.1 (63.472 -21.558)\n 8   1.5 2009-01-06 02:40:37     0        28.4    35.1 (63.901 -21.599)\n 9   1.5 2009-01-16 04:57:04     0        28.0    35.1 (64.447 -21.493)\n10   1.5 2009-01-26 02:30:53     0        28.1    35.0 (64.835 -21.207)\n# ... with 207 more rows, and abbreviated variable name 1: salinity\n\n\nA printed object is the metadata of the file with key information that tells us that is a simple feature with 217 simple features (recores) and six variables (column) and the type of the object are points. The metadata also display the bounding extent covered with these points and the datum of WGS84. We can check whether the conversion is successful by simply map the simple feature object we created into an interactive map shown in Figure 1.\n\nargo.surface.sf %>% \n  tm_shape() +\n  tm_bubbles(\n    size = \"temperature\", \n    col = \"salinity\", \n    scale = c(0.3,.8),\n    border.col = \"black\", \n    border.alpha = .5, \n    style=\"fixed\", \n    breaks=c(-Inf, seq(34.8, 35.6, by=.2), Inf),\n    palette=\"-RdYlBu\", contrast=1, \n    title.size=\"Temperature\", \n    title.col=\"Salinity (%)\", \n    id=\"Date\",\n    popup.vars=c(\"Temperature: \"=\"temperature\", \"Salinity: \"=\"salinity\"),\n    popup.format=list(temperature=list(digits=2))\n    ) \n\n\n\n\n\nFigure 1: Map with bubble size representing temperature variation and the color is the salinity gradients."
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#trajectory",
    "href": "posts/simplefeature_pointline/index.html#trajectory",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Trajectory",
    "text": "Trajectory\nLooking on Figure 1 we notice that the points are following a certain. This path is commonly known as trajectory – a path that an Argo float follows through in the Indian Ocean over its lifespan. Therefore, we ought to convert the point simple feature into trajectory. Fortunate, a combination of function from dplyr package (Wickham et al. 2019) and sf (Pebesma 2018) has made it possible. Though we can create a trajectory for the entire lifespan it recorded profiles in the area, but for exploration purpose, I first created a year variable and extract year variable from date variable and then use the year variable to group the information by year and then create trajectories that are grouped by year.\n\nargo_traj = argo.surface.sf %>% \n  dplyr::mutate(year = lubridate::year(date) %>% as.factor()) %>% \n  dplyr::group_by(year) %>% \n  dplyr::summarise() %>% \n  sf::st_cast(to = \"LINESTRING\")\n\ntm_shape(shp = argo_traj)+\n  tm_lines(col = \"year\", lwd = 3, stretch.palette = TRUE)\n\n\n\n\n\nFigure 2: Incorrect trajectories of the Argo float separated by year\n\n\n\nWe notice that Figure 2 trajectories are un ordered and very confusing. In fact the trajectories in Figure 2 do not reflect the pathway of the Argo float. The problem like this arise especially when you forget to parse an argument do_uninon = FALSE in the summarise function. But if we simply parse that argument, as the chunk below highlight, we correct the observed error and create a trajectory that reflect the pathwary of the Argo float shown in Figure 3\n\nargo_traj = argo.surface.sf %>% \n  mutate(year = lubridate::year(date) %>% as.factor()) %>% \n  arrange(date) %>% \n  group_by(year) %>% \n  summarise(do_union = FALSE) %>% \n  st_cast(to = \"LINESTRING\")\n\ntm_shape(shp = argo_traj)+\n  tm_lines(col = \"year\", lwd = 3, stretch.palette = TRUE)\n\n\n\n\n\nFigure 3: Correct trajectories of Argo floats separated by year"
  },
  {
    "objectID": "posts/spatialState/index.html",
    "href": "posts/spatialState/index.html",
    "title": "Spatial Data is Maturing in R",
    "section": "",
    "text": "R is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications\nR is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications. As data becomes more geographical, there is a growing necessity to make spatial data more accessible and easy to process. While there are plenty of tools out there that can make your life much easier when processing spatial data (e.g. QGIS and ArcMap) using R to conduct spatial analysis can be just as easy. This is especially true if you’re new to some of these packages and don’t feel like reading through all of the documentation to learn the package or, even more tedious, writing hundreds of lines of your own code to do something relatively simple. In this article I discuss a few packages that make common spatial statistics methods easy to perform in R (Bivand 2006).\nWe will conduct a high-level assessment of the R packages that are dedicated for spatial analysis. By showing network connection across package dependencies — which packages utilize code from another package to execute a task – we will undertake a high-level assessment of the condition of spatial in R. For comparison, we’ll compare our Analysis of Spatial Data task view to the tidyverse, one of R’s most well-known collections of packages, as well as the venerable Environmetrics task view, which includes numerous environmental analysis tools. To accomplish so, we’ll need to write some R code and install the following packages:\nWe will use the handy CRAN_package_db function from the tools package which conveniently grabs information from the DESCRIPTION file of every package on CRAN and turns it into a dataframe.\nHere we are interested with the package and imports columns, so we will select them and drop the rest from the dataset. Then, we parse clean and tidy the columns in the dataset to make it a little easier to work with:"
  },
  {
    "objectID": "posts/spatialState/index.html#package-connectivity",
    "href": "posts/spatialState/index.html#package-connectivity",
    "title": "Spatial Data is Maturing in R",
    "section": "Package Connectivity",
    "text": "Package Connectivity\nLet’s start with a look at the tidyverse. We can take the unusual step of actually employing a function from the tidyverse package (aptly titled tidyverse_packages), which identifies those packages that are formally part of the tidyverse. To see package connection, we filter for those packages and their imports, convert to tbl_graph, then plot using ggraph:\n\ntidyverse_tbl <- tidied_cran_imports %>% \n  filter(package %in% tidyverse_packages()) %>%\n  filter(imports %in% tidyverse_packages()) %>%\n  as_tbl_graph()\n\n\nggraph(tidyverse_tbl, layout = \"nicely\")  + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .12) +\n  theme_void()\n\n\n\n\nMany intersecting lines traverse in all directions, as one might anticipate, because many packages in tidyverse import other packages. As the tidyverse develops, this is to be expected.\n\nenv_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Environmetrics\") %>% \n  unlist(use.names = FALSE)\n\n\nenv_tbl <- tidied_cran_imports %>%\n  filter(package %in% env_packages) %>%\n  filter(imports %in% env_packages) %>%\n  as_tbl_graph()\n\nenv_tbl %>% \n  ggraph(layout = 'nicely') + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .3) +\n  theme_void()\n\n\n\n\nNext, let’s look at the Spatial Analysis task view, where we might not expect to see the same level of connectedness. The infrastructure underlying CRAN task views, the ctv package, (sort of) provides a function to obtain a vector of package names for a given task view, which we can use to make a network plot:\n\nspatial_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Spatial\") %>% \n  unlist(use.names = FALSE)\n\nWe then pull the packages that are in spatial analysis task view that are found in all packages that are tidied and convert them to ggraph table and plot the network\n\nsp_tbl <- tidied_cran_imports %>%\n  filter(package %in% spatial_packages) %>%\n  filter(imports %in% spatial_packages) %>%\n  as_tbl_graph()\n\nsp_tbl %>% \n  ggraph(layout = 'fr') + \n  geom_edge_link(colour = \"grey\") + \n  geom_node_point(colour=\"lightblue\", size=2) + \n  geom_node_text(aes(label=name), repel=FALSE, check_overlap = TRUE, nudge_y = .2) +  \n  theme_graph()\n\n\n\n\n\nsp_tbl %>% \n  ggraph(layout = 'linear',circular = TRUE) + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = TRUE, check_overlap = TRUE) +\n  theme_void()\n\n\n\n\nThere is clearly some connectivity among spatial-related packages, which serves as a reminder that task views on CRAN aren’t the only location where users find packages to use. Some programs, like sf, establish a hub of related packages because they share a package maintainer, while others, like sp, investigate spatial systems using a wide range of spatial packages. The graph below shows the number of downloads of the cranlogs package from the RStudio CRAN mirror over the last year.\n\nkgcount <- cran_downloads(packages = spatial_packages, \n                           from = Sys.Date()-1*365, \n                           to = Sys.Date())\n\n\nkgcount %>%\n  group_by(package) %>%\n  summarise(downloads = sum(count)) %>%\n  filter(downloads >= 450000) %>% \n  arrange(desc(downloads)) %>% \n  hchart(type = \"bar\", hcaes(x = package, y = downloads)) %>% \n  hc_xAxis(title = list(text = \"Mothly downloads\")) %>% \n  hc_yAxis(title = FALSE)"
  },
  {
    "objectID": "posts/tidymodels_classification/index.html",
    "href": "posts/tidymodels_classification/index.html",
    "title": "Machine learning with tidymodels: Classification Models",
    "section": "",
    "text": "Classification is a form of machine learning in which you train a model to predict which category an item belongs to. Categorical data has distinct ‘classes’, rather than numeric values. For example, a health clinic might use diagnostic data such as a patient’s height, weight, blood pressure, blood-glucose level to predict whether or not the patient is diabetic.\nClassification is an example of a supervised machine learning technique, which means it relies on data that includes known feature values (for example, diagnostic measurements for patients) as well as known label values (for example, a classification of non-diabetic or diabetic). A classification algorithm is used to fit a subset of the data to a function that can calculate the probability for each class label from the feature values. The remaining data is used to evaluate the model by comparing the predictions it generates from the features to the known class labels.\nThe best way to learn about classification is to try it for yourself, so that’s what you’ll do in this exercise.\nWe’ll require some packages to knock-off this module. You can have them installed as:\n\ninstall.packages(c('tidyverse', 'tidymodels', 'ranger', 'tidyverse', 'forecats', 'skimr', 'paletteer', 'nnet', 'here'))\n\nOnce you have installed the package, you can load the required packages\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(forcats)\n\n\n\nOnce the packages are loaded then we are going to import the dataset into the session. In this post we will explore a multi-class classification problem using the Covertype Data Set, which I obtained from the UCI Machine Learning Repository. This data set provides a total of 581,012 instances. The goal is to differentiate seven forest community types using several environmental variables including elevation, topographic aspect, topographic slope, horizontal distance to streams, vertical distance to streams, horizontal distance to roadways, hillshade values at 9AM, hillshade values at noon, hillshade values at 3PM, horizontal distance to fire points, and a wilderness area designation, a binary and nominal variable.\n\ncover.type = read_csv(\"../data/ml/covtype.csv\")\n\n\n\n\n\ncover.type %>% \n  glimpse()\n\nRows: 581,012\nColumns: 55\n$ Elevation                          <dbl> 2596, 2590, 2804, 2785, 2595, 2579,~\n$ Aspect                             <dbl> 51, 56, 139, 155, 45, 132, 45, 49, ~\n$ Slope                              <dbl> 3, 2, 9, 18, 2, 6, 7, 4, 9, 10, 4, ~\n$ Horizontal_Distance_To_Hydrology   <dbl> 258, 212, 268, 242, 153, 300, 270, ~\n$ Vertical_Distance_To_Hydrology     <dbl> 0, -6, 65, 118, -1, -15, 5, 7, 56, ~\n$ Horizontal_Distance_To_Roadways    <dbl> 510, 390, 3180, 3090, 391, 67, 633,~\n$ Hillshade_9am                      <dbl> 221, 220, 234, 238, 220, 230, 222, ~\n$ Hillshade_Noon                     <dbl> 232, 235, 238, 238, 234, 237, 225, ~\n$ Hillshade_3pm                      <dbl> 148, 151, 135, 122, 150, 140, 138, ~\n$ Horizontal_Distance_To_Fire_Points <dbl> 6279, 6225, 6121, 6211, 6172, 6031,~\n$ Wilderness_Area1                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ Wilderness_Area2                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Wilderness_Area3                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Wilderness_Area4                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type1                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type2                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type3                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type4                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type5                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type6                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type7                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type8                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type9                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type10                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type11                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type12                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type13                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type14                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type15                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type16                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type17                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type18                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,~\n$ Soil_Type19                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type20                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type21                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type22                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type23                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type24                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type25                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type26                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type27                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type28                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type29                        <dbl> 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,~\n$ Soil_Type30                        <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ Soil_Type31                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type32                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type33                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type34                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type35                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type36                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type37                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type38                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type39                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type40                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Cover_Type                         <dbl> 5, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 2,~\n\n\nThe seven community types are:\n\n1 = Spruce/Fir\n2 = Lodgepole Pine\n3 = Ponderosa Pine\n4 = Cottonwood/Willow\n5 = Aspen\n6 = Douglas Fir\n7 = Krummholz\n\nWe need to recode the cover type with the corresponding names as follows;\n\ncover.type %>% \n  distinct(Cover_Type)\n\n# A tibble: 7 x 1\n  Cover_Type\n       <dbl>\n1          5\n2          2\n3          1\n4          7\n5          3\n6          6\n7          4\n\ncover.type = cover.type %>% \n  mutate(cover = case_when(Cover_Type == 1 ~ \"Spruce\",\n                               Cover_Type == 2 ~ \"Lodgepole\",\n                               Cover_Type == 3 ~ \"Ponderosa\",\n                               Cover_Type == 4 ~ \"Cottonwood\",\n                               Cover_Type == 5 ~ \"Aspen\",\n                               Cover_Type == 6 ~ \"Douglas\",\n                               Cover_Type == 7 ~ \"Krummholz\")\n         )\n\nI then use dplyr count function to to compute the number of records from each community type\n\ncover.type %>% \n  group_by(cover) %>% \n  summarise(n = n()) %>% \n  mutate(area_ha = (n*900)/4063, \n         pct = n/sum(n) * 100, \n         across(is.numeric, round, 2)) %>% \n  arrange(-n)\n\n# A tibble: 7 x 4\n  cover           n area_ha   pct\n  <chr>       <dbl>   <dbl> <dbl>\n1 Lodgepole  283301  62754. 48.8 \n2 Spruce     211840  46925. 36.5 \n3 Ponderosa   35754   7920.  6.15\n4 Krummholz   20510   4543.  3.53\n5 Douglas     17367   3847.  2.99\n6 Aspen        9493   2103.  1.63\n7 Cottonwood   2747    608.  0.47\n\n\nThe printed output suggests significant data imbalance. In order to speed up the tuning and training process, I then select out 500 samples from each class using a stratified random sample. For potentially improved results, I should use all available samples. However, this would take a lot longer to execute.\n\nset.seed(123)\n\ncover.type.sample = cover.type %>% \n  group_by(cover) %>% \n  sample_n(size = 500) %>% \n  ungroup()\n\ncover.type.sample %>% \n  group_by(cover) %>% \n  summarise(n = n())\n\n# A tibble: 7 x 2\n  cover          n\n  <chr>      <int>\n1 Aspen        500\n2 Cottonwood   500\n3 Douglas      500\n4 Krummholz    500\n5 Lodgepole    500\n6 Ponderosa    500\n7 Spruce       500\n\n\nNext, I use the parsnips package (Kuhn & Vaughan, 2020) to define a random forest implementation using the ranger engine in classification mode. Note the use of tune() to indicate that I plan to tune the mtry parameter. Since the data have not already been split into training and testing sets, I use the initial_split() function from rsample to define training and testing partitions followed by the training() and testing() functions to create new datasets for each split (Kuhn, Chow, & Wickham, 2020).\n\n\n\n\nrf_model = rand_forest(mtry=tune(), trees=500) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n\n\n\n\nset.seed(42)\n\ncover_split = cover.type.sample %>% \n  initial_split(prop=.75, strata=cover)\n\ncover_train = cover_split %>% training()\ncover_test = cover_split %>% testing()\n\nI would like to normalize all continuous predictor variables and create a dummy variable from the single nominal predictor variable (“wilderness”). I define these transformations within a recipe using functions available in recipes package (Kuhn & Wickham, 2020a). This also requires defining the formula and the input data. Here, I am referencing only the training set, as the test set should not be introduced to the model at this point, as this could result in a later bias assessment of model performance. The all_numeric(), all_nominal(), and all_outcomes() functions are used to select columns on which to apply the desired transformations.\n\ncover_recipe = cover_train %>% \n  recipe(cover~.) %>%\n  step_normalize(all_numeric()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\nThe model and pre-processing recipe are then combined into a workflow.\n\ncover_wf = workflow() %>%\n  add_model(rf_model) %>% \n  add_recipe(cover_recipe)\n\nI then use yardstick (yerdstick?) and the metric_set() function to define the desired assessment metrics, in this case only overall accuracy. To prepare for hyperparameter tuning using five-fold cross validation, I define folds using the vfold_cv() function from rsample. Similar to the training and testing split above, the folds are stratified by the community type to maintain class balance within each fold. Lastly, I then define values of mtry to test using the dials package. It would be better to test more values and maybe optimize additional parameters. However, I am trying to decrease the time required to execute the example.\n\n#Define metrics\nmy_mets = metric_set(accuracy)\n\n#Define folds\nset.seed(42)\ncover_folds = vfold_cv(cover_train, v=5, strata=cover)\n\n#Define tuning grid\nrf_grid = grid_regular(mtry(range = c(1, 12)),\n                        levels = 6)\n\nNow that the model, pre-processing steps, workflow, metrics, data partitions, and mtry values to try have been defined, I tune the model using tune_grid() from the tune package. Note that this may take several minutes. Specifically, I make sure to use the defined workflow so that the pre-processing steps defined using the recipe are used. Once completed, I collect the resulting metrics for each mtry value for each fold using collect_metrics() from tune. The summarize parameter is set to FALSE because I want to obtain all results for each fold, as opposed to aggregated results. I then calculate the minimum, maximum, and median overall accuracies for each fold using dplyr and plot the results using ggplot2.\n\nrf_tuning = cover_wf %>% \n  tune_grid(resamples=cover_folds, grid = rf_grid, metrics=my_mets)\n\n\ntune_result = rf_tuning %>% \n  collect_metrics(summarize=FALSE) %>%\n  filter(.metric == 'accuracy') %>%  \n  group_by(mtry) %>%  \n  summarize(min_acc = min(.estimate),             \n            median_acc = mean(.estimate),             \n            max_acc = max(.estimate))\n\n\nggplot(tune_result, aes(y=median_acc, x=mtry))+\n  geom_point()+\n  geom_errorbar(aes(ymin=min_acc, ymax=max_acc), width = .4)+\n  theme_bw()+\n  labs(x=\"mtry Parameter\", y = \"Accuracy\")\n\n\n\n\nThe best mtry parameter is defined using the select_best() function from tune. The workflow is then finalized and the model is trained using last_fit() from tune. The collect_predictions() function from tune is used to obtain the class prediction for each sample in the withheld test set.\n\nbest_rf_model = rf_tuning %>% \n  select_best(metric=\"accuracy\")\n\nfinal_cover_wf = cover_wf %>% \n  finalize_workflow(best_rf_model)\n\nfinal_cover_fit = final_cover_wf %>% \n  last_fit(split=cover_split, metrics=my_mets) %>% \n  collect_predictions()\n\nLastly, I use the conf_mat() function from the yardstick package to obtain a multi-class error matrix from the reference and predicted classes for each sample in the withheld testing set.\n\nfinal_cover_fit %>% \n  conf_mat(truth=cover, estimate=.pred_class)\n\n            Truth\nPrediction   Aspen Cottonwood Douglas Krummholz Lodgepole Ponderosa Spruce\n  Aspen        125          0       0         0         0         0      0\n  Cottonwood     0        125       0         0         0         0      0\n  Douglas        0          0     125         0         0         0      0\n  Krummholz      0          0       0       125         0         0      0\n  Lodgepole      0          0       0         0       124         0      0\n  Ponderosa      0          0       0         0         1       125      0\n  Spruce         0          0       0         0         0         0    125\n\n\nPassing the matrix to summary() will provide a set of assessment metrics calculated from the error matrix.\n\nfinal_cover_fit %>% \n  conf_mat(truth=cover, estimate=.pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.999\n 2 kap                  multiclass     0.999\n 3 sens                 macro          0.999\n 4 spec                 macro          1.00 \n 5 ppv                  macro          0.999\n 6 npv                  macro          1.00 \n 7 mcc                  multiclass     0.999\n 8 j_index              macro          0.999\n 9 bal_accuracy         macro          0.999\n10 detection_prevalence macro          0.143\n11 precision            macro          0.999\n12 recall               macro          0.999\n13 f_meas               macro          0.999\n\n\n\n\n\nSimilar to the tidyverse (Wickham & Wickham, 2017), tidymodels (Kuhn & Wickham, 2020b) is a very powerful framework for creating machine learning workflows and experimental environments using a common philosophy and syntax. Although this introduction was brief and there are many more components that could be discussed, this can serve as a starting point for continued learning and experimentation. Check out the tidymodels website for additional examples and tutorials."
  },
  {
    "objectID": "posts/tidymodels_classificationTrain/index.html",
    "href": "posts/tidymodels_classificationTrain/index.html",
    "title": "Machine learning with tidymodels: Binary Classification Model",
    "section": "",
    "text": "In this post, we’ll learn how to create Machine learning models using R. Machine learning is the foundation for predictive modeling and artificial intelligence. We’ll learn the core principles of machine learning and how to use common tools and frameworks to train, evaluate, and use machine learning models. in this post we are going to manipulate and visualize diabetes dataset and train and evaluate binary regression models. But before we proceed, we need some packages to accomplish the step in this post. We can have them installed as:\n\ninstall.packages(c('tidyverse', 'tidymodels', 'ranger', 'vip', 'palmerpenguins', 'skimr', 'paletteer', 'nnet', 'here'))\n\nThen we load the packages in the session\n\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(magrittr)\nrequire(patchwork)\n\n\n\nLet’s start by looking at an example of binary classification, where the model must predict a label that belongs to one of two classes. In this exercise, we’ll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data.\n\n\nLet’s first import a file of patients data direct from the internet with read_csv function of readr package (Wickham, Hester, & Francois, 2017), part of the tidyverse ecosystem (Wickham & Wickham, 2017);\n\ndiabetes <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/diabetes.csv\")\n\nWe then print the dataset to explore the variables and records contained in the dataset;\n\ndiabetes\n\n# A tibble: 15,000 x 10\n   PatientID Pregn~1 Plasm~2 Diast~3 Trice~4 Serum~5   BMI Diabe~6   Age Diabe~7\n       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <dbl>   <dbl>\n 1   1354778       0     171      80      34      23  43.5  1.21      21       0\n 2   1147438       8      92      93      47      36  21.2  0.158     23       0\n 3   1640031       7     115      47      52      35  41.5  0.0790    23       0\n 4   1883350       9     103      78      25     304  29.6  1.28      43       1\n 5   1424119       1      85      59      27      35  42.6  0.550     22       0\n 6   1619297       0      82      92       9     253  19.7  0.103     26       0\n 7   1660149       0     133      47      19     227  21.9  0.174     21       0\n 8   1458769       0      67      87      43      36  18.3  0.236     26       0\n 9   1201647       8      80      95      33      24  26.6  0.444     53       1\n10   1403912       1      72      31      40      42  36.9  0.104     26       0\n# ... with 14,990 more rows, and abbreviated variable names 1: Pregnancies,\n#   2: PlasmaGlucose, 3: DiastolicBloodPressure, 4: TricepsThickness,\n#   5: SerumInsulin, 6: DiabetesPedigree, 7: Diabetic\n\n\nThough the printed output tell us that there are fifteen thousand records and ten variable, but sometimes you may wish to explore the internal structure of the dataset. The glimpse function from dplyr package can do that by listing the available variables and their types;\n\ndiabetes %>% \n  glimpse()\n\nRows: 15,000\nColumns: 10\n$ PatientID              <dbl> 1354778, 1147438, 1640031, 1883350, 1424119, 16~\n$ Pregnancies            <dbl> 0, 8, 7, 9, 1, 0, 0, 0, 8, 1, 1, 3, 5, 7, 0, 3,~\n$ PlasmaGlucose          <dbl> 171, 92, 115, 103, 85, 82, 133, 67, 80, 72, 88,~\n$ DiastolicBloodPressure <dbl> 80, 93, 47, 78, 59, 92, 47, 87, 95, 31, 86, 96,~\n$ TricepsThickness       <dbl> 34, 47, 52, 25, 27, 9, 19, 43, 33, 40, 11, 31, ~\n$ SerumInsulin           <dbl> 23, 36, 35, 304, 35, 253, 227, 36, 24, 42, 58, ~\n$ BMI                    <dbl> 43.50973, 21.24058, 41.51152, 29.58219, 42.6045~\n$ DiabetesPedigree       <dbl> 1.21319135, 0.15836498, 0.07901857, 1.28286985,~\n$ Age                    <dbl> 21, 23, 23, 43, 22, 26, 21, 26, 53, 26, 22, 23,~\n$ Diabetic               <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,~\n\n\nThis data consists of 15,000 patients with 10 variables that were used to diagnose diabetes. In this post we tread a Diabetic as outome and the remaining variables as predictor. A predictor variable is used to predict the occurrence and/or level of another variable, called the outcome variable. Let’s tidy and reorganize in format that easy for model to understand. The variable names do not follow the recommended standard therefore we clean them using clean_names function from janitor package (Firke, 2020). We also notice that some of the variable like PatientId adds no effect in the model and was dropped from the dataset.\nFurther, since we are going to use classification algorithm in this we need to convert our our predictor variable–diabetic–from numerical to factor. This is the outcome (label) and other variable like Pregnancies, PlasmaGlucose, DiastolicBloodPressure, BMI and so on are the predictor (features) we will use to predict the Diabetic label.\n\ndiabetes.tidy = diabetes %>% \n  janitor::clean_names() %>% \n  select(-patient_id) %>% \n  mutate(diabetic = if_else(diabetic == 1, \"Diabetic\", \"Non diabetic\"),\n         diabetic = as.factor(diabetic))\n\ndiabetes.tidy\n\n# A tibble: 15,000 x 9\n   pregnancies plasma_gluc~1 diast~2 trice~3 serum~4   bmi diabe~5   age diabe~6\n         <dbl>         <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <dbl> <fct>  \n 1           0           171      80      34      23  43.5  1.21      21 Non di~\n 2           8            92      93      47      36  21.2  0.158     23 Non di~\n 3           7           115      47      52      35  41.5  0.0790    23 Non di~\n 4           9           103      78      25     304  29.6  1.28      43 Diabet~\n 5           1            85      59      27      35  42.6  0.550     22 Non di~\n 6           0            82      92       9     253  19.7  0.103     26 Non di~\n 7           0           133      47      19     227  21.9  0.174     21 Non di~\n 8           0            67      87      43      36  18.3  0.236     26 Non di~\n 9           8            80      95      33      24  26.6  0.444     53 Diabet~\n10           1            72      31      40      42  36.9  0.104     26 Non di~\n# ... with 14,990 more rows, and abbreviated variable names 1: plasma_glucose,\n#   2: diastolic_blood_pressure, 3: triceps_thickness, 4: serum_insulin,\n#   5: diabetes_pedigree, 6: diabetic\n\n\nOur primary goal of data exploration is to try to understand the existing relationship patterns between variables in the dataset. Therefore, we should assess any apparent correlation using picture through data visualization. To make it easy to plot multiple plot at once, the format of our dataset at this stage is wide, and that prevent us doing that. In order to plot all variable in single plot, we must first convert the dataset from wide format to long format, and we can do that using pivot_longer function from dplyr package (Wickham, François, Henry, & Müller, 2019).\n\ndiabetes.tidy.long = diabetes.tidy %>% \n  pivot_longer(\n    cols = !diabetic, \n    names_to = \"features\", \n    values_to = \"values\"\n               )\n\nOnce we have pivoted the data to long format, we can create plot in a single plot using multiple facet;\n\ntheme_set(theme_light())\n\ndiabetes.tidy.long %>% \n  ggplot(aes(x = diabetic, y = values, fill = features)) +\n  geom_boxplot() + \n  facet_wrap(~ features, scales = \"free\", ncol = 4) +\n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  scale_y_continuous(name = \"Values\", trans = scales::sqrt_trans())+\n  theme(legend.position = \"none\", axis.title.x = element_blank(), \n        strip.background = element_rect(fill = \"grey60\"), \n        strip.text = element_text(color = \"white\", face = \"bold\"))\n\n\n\n\nFigure 1: Boxplot of the predictor variables between diabetic and non-diabetic patients\n\n\n\n\nThe values of the predictor vary between diabetic and non-diabetic individuals. In particular, with exception of diabetes_pedigree and triceps_thickness, other predictors show that diabetic individual with relatively high values than non-diabetic (Figure 1). These features may help predict whether or not a patient is diabetic.\n\n\n\nOur dataset includes known values for the label, so we can use this to train a classifier so that it finds a statistical relationship between the features and the label value; but how will we know if our model is any good? How do we know it will predict correctly when we use it with new data that it wasn’t trained with?\nIt is best practice to hold out part of the data for testing in order to get a better estimate of how models will perform on new data by comparing the predicted labels with the already known labels in the test set. Well, we can take advantage of the fact we have a large dataset with known label values, use only some of it to train the model, and hold back some to test the trained model - enabling us to compare the predicted labels with the already known labels in the test set.\nIn R, the tidymodels framework provides a collection of packages for modeling and machine learning using tidyverse principles (Kuhn & Wickham, 2020). For instance, rsample, a package in tidymodels, provides infrastructure for efficient data splitting and resampling (Kuhn, Chow, & Wickham, 2020):\n\ninitial_split(): specifies how data will be split into a training and testing set\ntraining() and testing() functions extract the data in each split\n\n\n# Split data into 70% for training and 30% for testing\nset.seed(2056)\ndiabetes_split <- diabetes.tidy %>% \n  initial_split(prop = 0.70)\n\n\n# Extract the data in each split\ndiabetes_train <- training(diabetes_split)\ndiabetes_test <- testing(diabetes_split)\n\n\n# Print the number of cases in each split\ncat(\"Training cases: \", nrow(diabetes_train), \"\\n\",\n    \"Test cases: \", nrow(diabetes_test), sep = \"\")\n\nTraining cases: 10500\nTest cases: 4500\n\n\n\n\n\n\nOnce we have separated the dataset into train and test set, now we’re ready to train our model by fitting the training features to the training labels (diabetic). There are various algorithms we can use to train the model.\n\n\nIn this section, we’ll use Logistic Regression, which is a well-established algorithm for classification. Logistic regression is a binary classification algorithm, meaning it predicts two categories. There are quite a number of ways to fit a logistic regression model in tidymodels. For now, let’s fit a logistic regression model via the default stats::glm() engine.\n\n# Make a model specification\nlogreg_spec <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\n\n\n# Print the model specification\nlogreg_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nAfter a model has been specified, the model can be estimated or trained using the fit() function, typically using a symbolic description of the model (a formula) and some data.\n\n# Train a logistic regression model\nlogreg_fit <- logreg_spec %>% \n  fit(diabetic ~ ., data = diabetes_train)\n\n\n# Print the model object\nlogreg_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetic ~ ., family = stats::binomial, \n    data = data)\n\nCoefficients:\n             (Intercept)               pregnancies            plasma_glucose  \n                8.624243                 -0.266296                 -0.009615  \ndiastolic_blood_pressure         triceps_thickness             serum_insulin  \n               -0.012297                 -0.022807                 -0.003932  \n                     bmi         diabetes_pedigree                       age  \n               -0.049028                 -0.923262                 -0.056876  \n\nDegrees of Freedom: 10499 Total (i.e. Null);  10491 Residual\nNull Deviance:      13290 \nResidual Deviance: 9221     AIC: 9239\n\n\nThe model print out shows the coefficients learned during training. Now we’ve trained the model using the training data, we can use the test data we held back to evaluate how well it predicts using parsnip::predict(). Let’s start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:\n\n# Make predictions then bind them to the test set\nresults <- diabetes_test %>% \n  select(diabetic) %>% \n  bind_cols(\n    logreg_fit %>% predict(new_data = diabetes_test)\n    )\n\n# Compare predictions\nresults %>% \n  slice_head(n = 10)\n\n# A tibble: 10 x 2\n   diabetic     .pred_class \n   <fct>        <fct>       \n 1 Non diabetic Non diabetic\n 2 Non diabetic Non diabetic\n 3 Non diabetic Non diabetic\n 4 Non diabetic Non diabetic\n 5 Diabetic     Diabetic    \n 6 Non diabetic Non diabetic\n 7 Non diabetic Non diabetic\n 8 Diabetic     Non diabetic\n 9 Non diabetic Non diabetic\n10 Non diabetic Non diabetic\n\n\nComparing each prediction with its corresponding “ground truth” actual value isn’t a very efficient way to determine how well the model is predicting. Fortunately, tidymodels has a few more tricks up its sleeve: yardstick - a package used to measure the effectiveness of models using performance metrics (Kuhn & Vaughan, 2020). The most obvious thing you might want to do is to check the accuracy of the predictions - in simple terms, what proportion of the labels did the model predict correctly? yardstick::accuracy() does just that!\n\n# Calculate accuracy: proportion of data predicted correctly\naccuracy(\n  data = results, \n  truth = diabetic, \n  estimate = .pred_class\n  )\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.789\n\n\nThe accuracy is returned as a decimal value - a value of 1.0 would mean that the model got 100% of the predictions right; while an accuracy of 0.0 is, well, pretty useless! Accuracy seems like a sensible metric to evaluate (and to a certain extent it is), but you need to be careful about drawing too many conclusions from the accuracy of a classifier. Remember that it’s simply a measure of how many cases were predicted correctly. Suppose only 3% of the population is diabetic. You could create a classifier that always just predicts 0, and it would be 97% accurate - but not terribly helpful in identifying patients with diabetes!\nFortunately, there are some other metrics that reveal a little more about how our classification model is performing.One performance metric associated with classification problems is the confusion matrix. A confusion matrix describes how well a classification model performs by tabulating how many examples in each class were correctly classified by a model. In our case, it will show you how many cases were classified as negative (0) and how many as positive (1); the confusion matrix also shows you how many were classified into the wrong categories. The conf_mat() function from yardstick calculates this cross-tabulation of observed and predicted classes.\n\n# Confusion matrix for prediction results\nresults %>% \n  conf_mat(truth = diabetic, estimate = .pred_class)\n\n              Truth\nPrediction     Diabetic Non diabetic\n  Diabetic          897          293\n  Non diabetic      657         2653\n\n\nThe descriptive statistic of confusion matrix presented above can be presented visually as shown in Figure 2\n\n# Visualize conf mat\nupdate_geom_defaults(geom = \"rect\", new = list(fill = \"midnightblue\", alpha = 0.7))\n\nresults %>% \n  conf_mat(diabetic, .pred_class) %>% \n  autoplot()\n\n\n\n\nFigure 2: Visualize confusion matrix plot\n\n\n\n\ntidymodels provides yet another succinct way of evaluating all these metrics. Using yardstick::metric_set(), you can combine multiple metrics together into a new function that calculates all of them at once.\n\n# Combine metrics and evaluate them all at once\neval_metrics <- \n  metric_set(ppv, recall, accuracy, f_meas)\n\n\neval_metrics(\n  data = results, \n  truth = diabetic, \n  estimate = .pred_class\n  )\n\n# A tibble: 4 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 ppv      binary         0.754\n2 recall   binary         0.577\n3 accuracy binary         0.789\n4 f_meas   binary         0.654\n\n\nUntil now, we’ve considered the predictions from the model as being either 1 or 0 class labels. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on probability; so what actually gets predicted by a binary classifier is the probability that the label is true \\(P(y)\\) ) and the probability that the label is false \\(1−P(y)\\). A threshold value of 0.5 is used to decide whether the predicted label is a 1 \\(P(y)>0.5\\) or a 0 \\(P(y)<=0.5\\). Let’s see the probability pairs for each case:\n\n# Predict class probabilities and bind them to results\nresults <- results %>% \n  bind_cols(logreg_fit %>% \n              predict(new_data = diabetes_test, type = \"prob\"))\n\n  \n\n\n# Print out the results\nresults %>% \n  slice_head(n = 10)\n\n# A tibble: 10 x 4\n   diabetic     .pred_class  .pred_Diabetic `.pred_Non diabetic`\n   <fct>        <fct>                 <dbl>                <dbl>\n 1 Non diabetic Non diabetic         0.417                 0.583\n 2 Non diabetic Non diabetic         0.0985                0.902\n 3 Non diabetic Non diabetic         0.0469                0.953\n 4 Non diabetic Non diabetic         0.0561                0.944\n 5 Diabetic     Diabetic             0.581                 0.419\n 6 Non diabetic Non diabetic         0.331                 0.669\n 7 Non diabetic Non diabetic         0.288                 0.712\n 8 Diabetic     Non diabetic         0.270                 0.730\n 9 Non diabetic Non diabetic         0.275                 0.725\n10 Non diabetic Non diabetic         0.131                 0.869\n\n\nThe decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the true positive rate (which is another name for recall) and the false positive rate \\((1 - specificity)\\) for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a received operator characteristic (ROC) chart, like this:\n\n\n\n\n\n\nImportant\n\n\n\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n\n\n\n# Make a roc_chart\nresults %>% \n  roc_curve(truth = diabetic, .pred_Diabetic) %>% \n  autoplot()\n\n\n\n\nFigure 3: The ROC curve\n\n\n\n\nThe Figure 3 shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a \\(\\frac{50}{50}\\) random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\nThe area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. The closer to 1 this value is, the better the model. Once again, tidymodels includes a function to calculate this metric: yardstick::roc_auc()\n\n# Compute the AUC\nresults %>% \n  roc_auc(diabetic, .pred_Diabetic)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n\n\n\n\n\nWe have been dealing with logistic regression, which is a linear algorithm. tidymodels provide a swift approach to change algorithms in the model. For instance , we can change the logistic regression to other kind of classification algorithms inluding:\n\nSupport Vector Machine algorithms: Algorithms that define a hyperplane that separates classes.\nTree-based algorithms: Algorithms that build a decision tree to reach a prediction\nEnsemble algorithms: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n\nTo make life simple, let us train the model using an ensemble algorithm named Random Forest that averages the outputs of multiple random decision trees. Random forests help to reduce tree correlation by injecting more randomness into the tree-growing process. More specifically, instead of considering all predictors in the data, for calculating a given split, random forests pick a random sample of predictors to be considered for that split.\nGiven the experience of the logistic regression model, the power of tidymodels is consistence and therefore we do not need to start over, the only thing we need to do is simply to specify and fit a random forest algorithm.\n\n# Preprocess the data for modelling\ndiabetes_recipe <- recipe(diabetic ~ ., data = diabetes_train) %>% \n  step_mutate(age = factor(age)) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_dummy(age)\n\n# specify a random forest model specification\nrf_spec <- rand_forest() %>% \n  set_engine(\"ranger\", importance = \"impurity\") %>% \n  set_mode(\"classification\")\n\n# Bundle recipe and model spec into a workflow\nrf_wf <- workflow() %>% \n  add_recipe(diabetes_recipe) %>% \n  add_model(rf_spec)\n\n# Fit a model\nrf_wf_fit <- rf_wf %>% \n  fit(data = diabetes_train)\n\n# # spefiy the model\n# rf.fit  = rand_forest() %>% \n#   set_engine(engine = \"ranger\") %>% \n#   set_mode(mode = \"classification\") %>% \n#   # fit the model\n#   fit(diabetic ~ ., data = diabetes_train)\n\nThen we make prediction of the fitted model with the test dataset\n\nrf.prediction = rf_wf_fit %>% \n  predict(new_data = diabetes_test)\n\nrf.prediction.prob  =  rf_wf_fit %>% \n  predict(new_data = diabetes_test, type = \"prob\")\n\nrf.prediction =  rf.prediction %>% \n  bind_cols(rf.prediction.prob)\n\nrf.result = diabetes_test %>% \n  select(diabetic) %>% \n  bind_cols(rf.prediction)\n\nrf.result\n\n# A tibble: 4,500 x 4\n   diabetic     .pred_class  .pred_Diabetic `.pred_Non diabetic`\n   <fct>        <fct>                 <dbl>                <dbl>\n 1 Non diabetic Non diabetic        0.275                 0.725 \n 2 Non diabetic Non diabetic        0.0137                0.986 \n 3 Non diabetic Non diabetic        0.00739               0.993 \n 4 Non diabetic Non diabetic        0.0108                0.989 \n 5 Diabetic     Diabetic            0.910                 0.0902\n 6 Non diabetic Non diabetic        0.103                 0.897 \n 7 Non diabetic Non diabetic        0.237                 0.763 \n 8 Diabetic     Non diabetic        0.270                 0.730 \n 9 Non diabetic Non diabetic        0.0484                0.952 \n10 Non diabetic Non diabetic        0.0326                0.967 \n# ... with 4,490 more rows\n\n\nThe printed predicted of the random forest looks complelling, let’s evaluate its metrics!\n\nrf.result %>% \n  conf_mat(truth = diabetic, estimate = .pred_class)\n\n              Truth\nPrediction     Diabetic Non diabetic\n  Diabetic         1368           99\n  Non diabetic      186         2847\n\n\n\nrf.result %>% \n  conf_mat(truth = diabetic, estimate = .pred_class) %>% \n  autoplot()\n\n\n\n\nWe notice that there a considerable increase in the number of True Positives and True Negatives, which is a step in the right direction. Let’s take a look at other evaluation metrics\n\nrf.result %>% \n  accuracy(truth = diabetic, estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.937\n\n\nThere is considerable improvement of the accuracy from .75 to around 0.93. The high accuracy of the random forest is also clearly visible in ROC curve (Figure 4) with considervable high value of area under the curve (AUC).\n\nroc.values = rf.result %>% \n  roc_auc(truth = diabetic, .pred_Diabetic) %>% \n  pull(.estimate) %>% \n  round(2)\n\nrf.result %>% \n  roc_curve(truth = diabetic, .pred_Diabetic) %>% \n  autoplot()+\n  annotate(geom = \"text\", x = .75, y = .25, label = paste(\"AUC-value = \", roc.values))\n\n\n\n\nFigure 4: ROC curve of random forest\n\n\n\n\nFigure 5 is a Variable Importance Plot (VIP) that illustrates importance of each predictor variables used to predict the diabetic outcome in a random forest model.\n\n# Load vip\nlibrary(vip)\n\n# Extract the fitted model from the workflow\nrf_wf_fit %>% \n  extract_fit_parsnip() %>% \n# Make VIP plot\n  vip()\n\n\n\n\nFigure 5: Model-specific VIPs of tree-based model random forest\n\n\n\n\n\n\n\n\nWe notice that random forest has high predictive power compared to the logistic regression model and hence you can choose the model that give you high accuracy in prediction. The random forest lines of code is bunded in single chunk;\n\n## spliting\nmy.split = diabetes.tidy %>% \n  initial_split(prop = .8)\n## test set\nmy.test = my.split %>%  testing()\n## train set\nmy.train = my.split %>% training()\n\n# spefiy the model\nmy.fit  = rand_forest() %>% \n  set_engine(engine = \"ranger\") %>% \n  set_mode(mode = \"classification\") %>% \n  # fit the model\n  fit(diabetic ~ ., data = my.train)\n\n# test the model\nmy.prediction = my.fit %>% \n  predict(new_data = my.test)\n\nmy.prediction.prob  =  my.fit %>% \n  predict(new_data = my.test, type = \"prob\")\n\nmy.prediction =  my.prediction %>% \n  bind_cols(my.prediction.prob)\n\nmy.result = my.test %>% \n  select(diabetic) %>% \n  bind_cols(my.prediction)\n\n# validate the model\n\nmy.result %>% \n  conf_mat(truth = diabetic, estimate = .pred_class)\n\nmy.result %>% \n  accuracy(truth = diabetic, estimate = .pred_class)\n\n# Combine metrics and evaluate them all at once\neval_metrics <- \n  metric_set(ppv, recall, accuracy, f_meas)\n\n\neval_metrics(\n  data = my.result, \n  truth = diabetic, \n  estimate = .pred_class\n  )\n\n## plotting\n\nmy.result %>% \n  roc_curve(truth = diabetic, .pred_Diabetic) %>% \n  autoplot()\n\nmy.result %>% \n  roc_auc(truth = diabetic, .pred_Diabetic)"
  },
  {
    "objectID": "posts/tidymodels_regression/index.html",
    "href": "posts/tidymodels_regression/index.html",
    "title": "Machine learning with tidymodels: Linear and Bayesian Regression Models",
    "section": "",
    "text": "We live in age of increasing data and as a data scientist, our role primarily involves exploring and analyzing data. However, with the amount of data data exploration and analysis become a thing for automation and use of intelligent tools to help us glean key information from data. The results of an analysis might form the basis of a report or a machine learning model, but it all begins with data. The two popular tools commonly used are R and Python. R is one of the most popular programming languages for data scientists. R is an elegant environment that’s designed to support data science, and you can use it for many purposes.\nAfter decades of open-source development, R provides extensive functionality that’s backed by a massive set of powerful statistical modeling, machine learning, visualization and data wrangling packages. Some of these package that has revolutionized R are tidyverse and tidymodel package. tidyverse is a collection of R packages that make data science faster, easier, and more fun and tidymodels is a collection of R packages for modeling and statistical analysis.\nAs a data scientist, you need to distinguish between regression predictive models and classification predictive models. Clear understanding of these models helps to choose the best one for a specific use case. In a nutshell, regression predictive modelsand classification predictive models fall under supervised machine learning. The main difference between these two models is the output: while in regression produce an output as numerical (or continuous), the output of classification is categorical (or discrete).\nA big part of machine learning is classification — we want to know what class or group an observation belongs to. Therefore, the ability to precisely classify observations to their groups is valuable for various business applications like predicting the future based on historical data. For example, when provided with a dataset about environmental variables, and you are asked to predict productivity, that is a regression task because productivity measured in term of chlorophyll concentration will be a continuous output.\nIn this post we will focus on regression. We will learn the steps of modelling using tidymodels (Kuhn and Wickham, 2020a). We first explore the data and check if it fit for modelling, we then split the dataset into a training and testing sets. Next, we will create a recipe object and define our model. Lastly, we will train a specified model and evaluate its performance. I will use the same dataset for three different model’s algorithms. Example of the common regression algorithms include random forest, linear regression, support vector regression (SVR) and bayes. Some algorithms, such as logistic regression, have the name regression in their functions but they are not regression algorithms.\nTo follow use code in this article, you will need tidymodels (Kuhn and Wickham, 2020a) and tidyverse packages (Wickham and Wickham, 2017) installed in your machine. You can install them from CRAN. The chunk below highlight lines of code to install the packages if they are yet in your PC.\n\nmodel.packages = c(\"tidymodels\", \"tidyverse\")\n\ninstall.packages(model.packages)\n\nsimilar to the tidyverse, tidymodels consists of several linked packages that use a similar philosophy and syntax. Here is a brief explanation of the component packages.\n\nparsnips: used to define models using a common syntax; makes it easier to experiment with different algorithms\nworkflows: provides tools to create workflows, or the desired machine learning pipeline, including pre-processing, training, and post-processing\nyardstick: a one-stop-shop to implement a variety of assessment metrics for regression and classification problems\ntune: for hyperparameter tuning\ndials: for managing parameters and tuning grids\nrsample: tools for data splitting, partitioning, and resampling\nrecipes: tools for pre-processing and data engineering\n\nOnce installed, you can load the packages in your session. I begin by reading in the required packages. Here, tidymodels is used to load the component packages discussed above along with some other packages (e.g., ggplot2, purr, and dplyr).\n\nrequire(tidymodels)"
  },
  {
    "objectID": "posts/tidymodels_regression/index.html#random-forest",
    "href": "posts/tidymodels_regression/index.html#random-forest",
    "title": "Machine learning with tidymodels: Linear and Bayesian Regression Models",
    "section": "Random Forest",
    "text": "Random Forest\nWe begin by fitting a random forest model.\n\nMake random forest model\nWe specify the model using the parsnip package (Kuhn and Vaughan, 2020a). This package provides a tidy, unified interface to models for a range of models without getting bogged down in the syntactical minutiae of the underlying packages. The parsnip package help us to specify ;\n\nthe type of model e.g random forest,\nthe mode of the model whether is regression or classification\nthe computational engine is the name of the R package.\n\nBased on the information above, we can use parsnip package to build our model as;\n\nrf.model = rand_forest() %>%\n  set_engine(engine = \"ranger\") %>%\n  set_mode(mode = \"regression\")\n\nrf.model\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger \n\n\n\n\nTrain random forest model\nOnce we have specified the model type, engine and mode, the model can be trained with the fit function. We simply parse into the fit the specified model and the transformed training set extracted from the prepped recipe.\n\nrf.fit = rf.model %>%\n  fit(chl ~ ., data = crtr.training)\n\n\n\npredict with random forest\nTo get our predicted results, we use the predict() function to find the estimated chlorophyll-a. First, let’s generate the estimated chlorophyll-a values by simply parse the random forest model rf.model we specified and the transformed testing set we created from a prepped recipe. We also stitch the predicted values to the transformed train set with bind_cols function;\n\nrf.predict = rf.fit %>%\n  predict(crtr.testing) %>%\n  bind_cols(crtr.testing) \n\nrf.predict\n\n# A tibble: 13 x 8\n     .pred temperature      do     ph ammonia phosphate nitrate     chl\n     <dbl>       <dbl>   <dbl>  <dbl>   <dbl>     <dbl>   <dbl>   <dbl>\n 1 -0.143        0.407  0.624  -0.756  1.05     -0.485   -0.364 -0.685 \n 2 -0.0964      -0.897  0.603  -1.56   0.401    -0.242   -0.419 -0.0561\n 3 -0.0919      -0.897  0.516  -1.20   0.530     0.0808   1.32   0.206 \n 4  0.366       -1.10   1.69   -1.64   0.562     1.86     3.49  -0.894 \n 5 -0.0544      -0.966  1.30   -0.976 -0.0507   -0.565   -0.364 -0.161 \n 6 -0.0748      -1.58  -0.0480 -1.12   0.498     0.0808   1.26  -1.31  \n 7  0.0753      -1.58   1.17   -0.462  2.05     -0.525    0.721  0.573 \n 8  0.160        0.887  0.538   0.493 -0.244     0.121   -0.473 -0.528 \n 9  0.189        0.887  0.429   0.420 -0.890    -0.202    0.341 -0.528 \n10 -0.0890       0.132 -0.850   0.567 -1.54     -0.767   -0.419 -0.632 \n11 -0.386        0.613 -0.200   0.567  0.272    -0.767   -0.364 -0.423 \n12 -0.0471       1.09  -0.503   0.714  1.66     -0.162   -0.419 -0.528 \n13  0.800        1.16   0.386   1.67   0.0138    2.10    -0.419  2.09  \n\n\nWhen making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format:\n\n\nEvaluate the rf model\nSo far, we have built a model and preprocessed data with a recipe. We predicted new data as a way to bundle a parsnip model and recipe together. The next step is to assess or evaluate the accuracy of the model. We use a metrics function from yardstick package (Kuhn and Vaughan, 2020b)to assess the accuracy of the model by comparing the predicted versus the original outcome variable. Note that we use the predicted dataset we just computed using predict function.\n\nrf.predict %>%\n  metrics(truth = chl, estimate = .pred)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.731\n2 rsq     standard       0.363\n3 mae     standard       0.595"
  },
  {
    "objectID": "posts/tidymodels_regression/index.html#linear-regression-approach",
    "href": "posts/tidymodels_regression/index.html#linear-regression-approach",
    "title": "Machine learning with tidymodels: Linear and Bayesian Regression Models",
    "section": "Linear regression approach",
    "text": "Linear regression approach\n\nMake linear model\nThe good of tidymodels is that when we change the model, we do not need to start over again from the beginning but rather change the engine. For instance, we replace the ranger engine with lm to specify the linear model using the parsnip package (Kuhn and Vaughan, 2020a) .\n\nlm.model = linear_reg() %>%\n  set_engine(engine = \"lm\") %>%\n  set_mode(mode = \"regression\")\n\n\n\nTrain Linear model\nOnce we have specified the model type, engine and mode, the model can be trained with the fit function;\n\nlm.fit = lm.model %>%\n  fit(chl ~ ., data = crtr.training)\n\n\n\nPredict with linear model\nOnce the model is fitted, This fitted object lm_fit has the lm model output built-in, which you can access with lm_fit$fit, but there are some benefits to using the fitted parsnip model object when it comes to predicting. To predict the values we use predict function and parse the model and standardized testing values we computed from the recipe (R Core Team, 2018). Note that here we use the transformed test set and not the original from the split object. In this case we use the model to predict the value and stitch the testing values using the bind_cols function;\n\nlm.predict = lm.fit %>%\n  predict(crtr.testing) %>%\n  bind_cols(crtr.testing) \n\nlm.predict\n\n# A tibble: 13 x 8\n     .pred temperature      do     ph ammonia phosphate nitrate     chl\n     <dbl>       <dbl>   <dbl>  <dbl>   <dbl>     <dbl>   <dbl>   <dbl>\n 1 -0.347        0.407  0.624  -0.756  1.05     -0.485   -0.364 -0.685 \n 2 -0.239       -0.897  0.603  -1.56   0.401    -0.242   -0.419 -0.0561\n 3 -0.260       -0.897  0.516  -1.20   0.530     0.0808   1.32   0.206 \n 4  0.0430      -1.10   1.69   -1.64   0.562     1.86     3.49  -0.894 \n 5 -0.238       -0.966  1.30   -0.976 -0.0507   -0.565   -0.364 -0.161 \n 6 -0.243       -1.58  -0.0480 -1.12   0.498     0.0808   1.26  -1.31  \n 7 -0.614       -1.58   1.17   -0.462  2.05     -0.525    0.721  0.573 \n 8  0.142        0.887  0.538   0.493 -0.244     0.121   -0.473 -0.528 \n 9  0.0942       0.887  0.429   0.420 -0.890    -0.202    0.341 -0.528 \n10  0.115        0.132 -0.850   0.567 -1.54     -0.767   -0.419 -0.632 \n11 -0.191        0.613 -0.200   0.567  0.272    -0.767   -0.364 -0.423 \n12 -0.215        1.09  -0.503   0.714  1.66     -0.162   -0.419 -0.528 \n13  0.760        1.16   0.386   1.67   0.0138    2.10    -0.419  2.09  \n\n\n\n\nEvaluate linear model\nOnce we have our lm.predict dataset that contains the predicted and test values, we can now use the metrics fiction to evaluate the accuracy of the model.\n\nlm.predict%>%\n  metrics(truth = chl, estimate = .pred)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.740\n2 rsq     standard       0.194\n3 mae     standard       0.629\n\n\n\n\nEstimate stats\nSometimes you may wish to plot predicted values with different predictors. To do that you need to create a new tidied data from the the model with tidy function and parse interval = TRUE argument as shown in the code below. This create a tibble shown below and the same data is plotted in figure @ref(fig:fig3).\n\nlm.fit.stats = lm.fit %>% \n  tidy(interval = TRUE)\n\nlm.fit.stats\n\n# A tibble: 7 x 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept) -3.41e-17     0.200 -1.71e-16   1    \n2 temperature  1.29e- 2     0.253  5.09e- 2   0.960\n3 do          -2.09e- 2     0.261 -8.00e- 2   0.937\n4 ph           6.88e- 2     0.289  2.38e- 1   0.814\n5 ammonia     -1.63e- 1     0.235 -6.96e- 1   0.494\n6 phosphate    2.91e- 1     0.277  1.05e+ 0   0.306\n7 nitrate     -6.99e- 2     0.316 -2.21e- 1   0.827\n\n\n\nlm.fit.stats %>%\n  slice(-1) %>%\n  ggplot(aes(x = term, y = estimate)) +\n  geom_point(size = 4)+\n  geom_errorbar(aes(ymin = estimate-std.error, ymax = estimate+std.error), width = .2)+\n  scale_y_continuous(breaks = seq(-1.5,1.5,0.5))+\n  ggpubr::theme_pubclean()+\n  theme(axis.text = element_text(size = 10))+\n  labs(x = \"\", y = \"Estimated chl\")\n\n\n\n\nFigure 3: Estimated value of chlorophyll concentration at different predictors"
  },
  {
    "objectID": "posts/tidymodels_regression/index.html#bayesian-approach",
    "href": "posts/tidymodels_regression/index.html#bayesian-approach",
    "title": "Machine learning with tidymodels: Linear and Bayesian Regression Models",
    "section": "Bayesian approach",
    "text": "Bayesian approach\n\nMake Bayes Model\nFor Bayesian, we also change the engine and specified are called prior and prior_intercept. It turns out that linear_reg() has a stan engine. Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to parsnip::set_engine().\n\nprior.dist = rstanarm::student_t(df = 1)\n\n\nset.seed(401)\n\n## make the parsnip model\nbayes.model = linear_reg() %>%\n  set_engine(engine = \"stan\", \n             prior_intercept = prior.dist, \n             prior = prior.dist) %>%\n  set_mode(mode = \"regression\")\n\nThis kind of Bayesian analysis (like many models) involves randomly generated numbers in its fitting procedure. We can use set.seed() to ensure that the same (pseudo-)random numbers are generated each time we run this code. The number 123 isn’t special or related to our data; it is just a “seed” used to choose random numbers.\n\n\nTrain Bayes model\nOnce we have defined the Bayesian model, we train it using a transformed testing set;\n\n## train the bayes model\nbayes.fit = bayes.model%>%\n  fit(chl ~ ., data = crtr.testing)\n\nbayes.fit\n\nparsnip model object\n\nstan_glm\n family:       gaussian [identity]\n formula:      chl ~ .\n observations: 13\n predictors:   7\n------\n            Median MAD_SD\n(Intercept) -0.2    0.2  \ntemperature -0.5    0.3  \ndo           0.4    0.4  \nph           0.6    0.4  \nammonia      0.1    0.2  \nphosphate    0.6    0.3  \nnitrate     -0.5    0.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.7    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nPredict Bayes fit\n\nbayes.predict = bayes.fit %>%\n  predict(crtr.testing) %>%\n  bind_cols(crtr.testing)\n\nbayes.predict\n\n# A tibble: 13 x 8\n     .pred temperature      do     ph ammonia phosphate nitrate     chl\n     <dbl>       <dbl>   <dbl>  <dbl>   <dbl>     <dbl>   <dbl>   <dbl>\n 1 -0.607        0.407  0.624  -0.756  1.05     -0.485   -0.364 -0.685 \n 2 -0.302       -0.897  0.603  -1.56   0.401    -0.242   -0.419 -0.0561\n 3 -0.827       -0.897  0.516  -1.20   0.530     0.0808   1.32   0.206 \n 4 -0.617       -1.10   1.69   -1.64   0.562     1.86     3.49  -0.894 \n 5  0.0850      -0.966  1.30   -0.976 -0.0507   -0.565   -0.364 -0.161 \n 6 -0.611       -1.58  -0.0480 -1.12   0.498     0.0808   1.26  -1.31  \n 7  0.379       -1.58   1.17   -0.462  2.05     -0.525    0.721  0.573 \n 8  0.0950       0.887  0.538   0.493 -0.244     0.121   -0.473 -0.528 \n 9 -0.686        0.887  0.429   0.420 -0.890    -0.202    0.341 -0.528 \n10 -0.705        0.132 -0.850   0.567 -1.54     -0.767   -0.419 -0.632 \n11 -0.516        0.613 -0.200   0.567  0.272    -0.767   -0.364 -0.423 \n12 -0.256        1.09  -0.503   0.714  1.66     -0.162   -0.419 -0.528 \n13  1.74         1.16   0.386   1.67   0.0138    2.10    -0.419  2.09  \n\n\n\n\nEvaluate Bayes model\n\nbayes.predict %>%\n  metrics(truth = chl, estimate = .pred)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.432\n2 rsq     standard       0.717\n3 mae     standard       0.334\n\n\n\n\nBayes.fit.stats\nTo update the parameter table, the tidy() method is once again used:\n\nbayes.stats = bayes.fit %>% \n  broom.mixed::tidy(intervals = TRUE)\n\nbayes.stats\n\n# A tibble: 7 x 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)   -0.220     0.230\n2 temperature   -0.541     0.338\n3 do             0.394     0.370\n4 ph             0.597     0.379\n5 ammonia        0.120     0.222\n6 phosphate      0.586     0.310\n7 nitrate       -0.527     0.270\n\n\n\nbayes.stats %>% \n  slice(-1) %>%\n  ggplot(aes(x = term, y = estimate)) +\n  geom_point(size = 4)+\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = .2)+\n  scale_y_continuous(breaks = seq(-1.5,1.5,0.5))+\n  ggpubr::theme_pubclean()+\n  theme(axis.text = element_text(size = 10))+\n  labs(x = \"\", y = \"Estimated chl\")"
  },
  {
    "objectID": "posts/tidymodels_regression/index.html#links",
    "href": "posts/tidymodels_regression/index.html#links",
    "title": "Machine learning with tidymodels: Linear and Bayesian Regression Models",
    "section": "Links",
    "text": "Links\n\nMachine Learning with tidymodels"
  },
  {
    "objectID": "posts/ttest/index.html",
    "href": "posts/ttest/index.html",
    "title": "Inferential statistics in R:ttest",
    "section": "",
    "text": "A formal statistical test called a hypothesis test is used to confirm or disprove a statistical hypothesis. The following R hypothesis tests are demonstrated in this course.\nEach type of test can be run using the R function t.test().The function comes with the following arguments;\nwhere: \\(x\\) and \\(y\\) are the vectors of data elements \\(alternative\\): the stated alternative hypothesis \\(mu\\): the true value of the mean \\(paired\\): whether or not to run a paired test \\(var.equal\\): whether to assume that the vaarinaces between the values in the vector are equal \\(con.level\\): The confidence level to use\nBefore we proceed, we need functions from various packages and accessing these functions when needed may render this task tedious. Therefore, lets load the packages in advance. These packages include tidyverse (Wickham and Wickham 2017), patchwork(Pedersen 2020) and magrittr(Bache and Wickham 2014)"
  },
  {
    "objectID": "posts/ttest/index.html#one-sample-t-test",
    "href": "posts/ttest/index.html#one-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "One sample t-test",
    "text": "One sample t-test\nOne sample t-test is widely used in statistical analysis to determine whether the population’s mean is equal to given mean value. The given mean value can be the sample mean for instance. A t.test function in R is used to test one sample parametric test. Let’s consider a situation where we want to determine whether the total length of Nile perch collected during a survey conducted in December 2022 is not equal to a long term mean length of 61cm. Let’s generate a sample by creating a data frame that contain sample of 350 individual of nile perch. Using a run_if function help us simulate weight of nile perch once given the minimum and maximum values. The code for simulating the total length is highlighted in the code chunk below;\n\nsample = tibble::tibble(\n  id = 1:350,\n  tl = runif(n = 350, min = 48, max = 65)\n  )\n\nsample\n\n# A tibble: 350 x 2\n      id    tl\n   <int> <dbl>\n 1     1  61.3\n 2     2  49.7\n 3     3  50.4\n 4     4  48.5\n 5     5  54.9\n 6     6  57.3\n 7     7  60.6\n 8     8  61.9\n 9     9  50.4\n10    10  49.0\n# ... with 340 more rows\n\n\nLet’s use a histogram to check the distribution of the data and add a vertical line of the population mean to identify whether the data is leading away is around the population\n\nsample %>% \n  ggplot(aes(x = tl)) +\n  geom_histogram(bins = 30, color = \"ivory\", fill = \"cyan4\")+\n  geom_vline(xintercept = 61, linetype = 2, color = \"red\")+\n  scale_x_continuous(name = \"Total length (cm.)\", breaks = seq(40,80,4)) +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_minimal()\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nNow we notice that the position of the population mean of the sample dataset, we can not test to determine whether the sample mean total length is lower than the sample mean\n\nsample %$%\n  t.test(x = tl, mu = 61, alternative = \"less\") \n\n\n    One Sample t-test\n\ndata:  tl\nt = -17.907, df = 349, p-value < 2.2e-16\nalternative hypothesis: true mean is less than 61\n95 percent confidence interval:\n     -Inf 56.50406\nsample estimates:\nmean of x \n 56.04797 \n\n\nThe one sample t-test determine the whether the sample mean total length of nile perch was less than the long-term mean length suggest that the sample total length (56.68cm) was less than the population mean (61cm) and the difference was statistically significant (t(349) = -18.19, p < 0.001). Lets try change the alternative to greater\n\nsample %$%\n  t.test(x = tl, mu = 61, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  tl\nt = -17.907, df = 349, p-value = 1\nalternative hypothesis: true mean is greater than 61\n95 percent confidence interval:\n 55.59189      Inf\nsample estimates:\nmean of x \n 56.04797 \n\n\nNotice that the test is not statistically significant (t(349) = 18.91, p = 1) because the population mean (61cm) is greater than the sample mean (56.68)."
  },
  {
    "objectID": "posts/ttest/index.html#two-sample-t-test",
    "href": "posts/ttest/index.html#two-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "Two Sample t-test",
    "text": "Two Sample t-test\nA two sample t-test is used to determine whether the means of two independent samples are equal. Lets consider that two independent survey to measure the stock of nile perch was conducted in two independent period. The first survey was conducted in June 2001 and the subsequency survey was conducted in July 2021. The two survey were conducted during the cool and dry season but with a 20 years time difference. Therefore, we want to determine whether the mean sample of nile perch collected in 2021 is smaller than the those sampled in 2001\n\nset.seed(1254)\n\nsample2 = tibble::tibble(\n  id = 1:350,\n  tl21 = rnorm(n = 350,mean = 52, sd = 18),\n  tl01 = rnorm(n = 350, mean = 61, sd = 20)\n  )\n\nsample2\n\n# A tibble: 350 x 3\n      id  tl21  tl01\n   <int> <dbl> <dbl>\n 1     1 41.7   54.4\n 2     2 68.4   52.2\n 3     3 47.1   69.1\n 4     4 38.7   43.2\n 5     5  3.41  46.9\n 6     6 42.6   64.3\n 7     7 67.3   70.4\n 8     8 65.7   72.9\n 9     9 26.3   25.0\n10    10 56.2   64.5\n# ... with 340 more rows\n\n\nOnce we have created a dataframe with values for the two sampling survey, it’s a good practice to visualize the value to see the patterns.\n\nsample2 %>% \n  pivot_longer(cols = tl21:tl01) %>% \n  ggplot(aes(x = value, fill = name))+\n  geom_density(position = \"identity\", alpha = .4)+\n  scale_fill_brewer(name =\"Surveys\" ,palette = \"Set2\", label = c(\"2001\", \"2021\"))+\n  scale_x_continuous(name = \"Total length (cm)\", breaks = seq(20,150,20))+\n  scale_y_continuous(name = \"Density\")+\n  theme_minimal()\n\n\n\n\nWe notice from a figure above a slight difference in the density shape with the median value for 2001 far east from the 2021, suggesting the size of 2001 is relatively higher than 2001. Let’s perform two sample t-test to determine whether that difference is significant;\n\nsample2 %$%\n  t.test(x = tl21, y = tl01, alternative = \"less\") \n\n\n    Welch Two Sample t-test\n\ndata:  tl21 and tl01\nt = -6.4619, df = 690.02, p-value = 9.771e-11\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -6.905485\nsample estimates:\nmean of x mean of y \n 51.80015  61.06788 \n\n\nThe output display the Welch Two Sample t-test to determine whether the total length of nile perch sampled in 2021 is less than those of 2001. The result suggest that the sample mean in 2021 was 51.8 cm which is less than 61.07 cm of nile perch sampled in 2001. The result suggest that the 2021 nile perch were small in size than those of 2001, and the difference was significant (t(690) = 6.46, p < 0.01)."
  },
  {
    "objectID": "posts/ttest/index.html#paired-sample-t-test",
    "href": "posts/ttest/index.html#paired-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "Paired sample t-test",
    "text": "Paired sample t-test\nThis test is normally used to determine whether the values in paired dataset have different mean. For instance, the weight in nile perch measured after captured and kept in cage for three months and measured again. Therefore, the nile perch individuals were measured before taken to cage and then measured after three months. This means we have measurement before and after. Let’s create a dataframe and simulate before and after total length of nile perch.\n\nset.seed(1254)\n\nsample3 = tibble::tibble(\n  id = 1:50,\n  before = rnorm(n = 50,mean = 52, sd = 12),\n  after = before + rnorm(n = 50) %>% abs()\n  )\n\nsample3\n\n# A tibble: 50 x 3\n      id before after\n   <int>  <dbl> <dbl>\n 1     1   45.1  46.2\n 2     2   62.9  64.7\n 3     3   48.7  49.3\n 4     4   43.2  43.6\n 5     5   19.6  20.4\n 6     6   45.7  46.1\n 7     7   62.2  63.2\n 8     8   61.1  62.5\n 9     9   34.9  35.2\n10    10   54.8  56.0\n# ... with 40 more rows\n\n\nThen we perform paired sample t-test\n\nsample3 %$%\n  t.test(x = before, y = after, paired = TRUE) \n\n\n    Paired t-test\n\ndata:  before and after\nt = -11.171, df = 49, p-value = 4.484e-15\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9512640 -0.6612062\nsample estimates:\nmean of the differences \n             -0.8062351 \n\n\nSince the p < 0.05, we reject the null hypothesis that the mean total length before and after is significant. Therefore, fattening nile perch in cage for three months increased the total length and that increase is significant (t(49) = -11.17, p < 0.001)"
  },
  {
    "objectID": "posts/vizingaApp/index.html",
    "href": "posts/vizingaApp/index.html",
    "title": "Interactive Web Application for Cage Aquaculture in Lake Victoria",
    "section": "",
    "text": "Cage aquaculture is a promising way to increase fish production in Tanzania’s Lake Victoria (Kashindye et al., 2015; Orina et al., 2021). However, proper planning is crucial to ensure the sustainability and success of such ventures (Mlaponi et al., 2015). That’s where the interactive web app for planning cage aquaculture comes in Figure 1. In this digital age, where data is abundant and the ability to make data-driven decisions is crucial, Shiny provides a powerful web applications that can be used to analyze and visualize data.\nDeveloped by Masumbuko Semba from the Nelson Mandela African Institution of Science and Technology and a team of experts from the Tanzania Fisheries Research Institute, this interactive web app is designed to help fish farmers and investors plan their cage aquaculture operations in Lake Victoria.\n\n\n\n\n\nFigure 1: The home page of cage aquaculture web application in Lake Victoria\n\n\n\n\nThis app1, which is still under active development, but its beta version is up and running online provides users with a range of tools and information to help them make informed decisions about their cage aquaculture projects. Users can input data such as the size and location of their cages, the type of fish they want to farm, and the expected production volume. The app then uses this information to generate a detailed plan that takes into account factors such as water quality, feed requirements, and environmental impacts.\nOne of the key features of the Shiny app is its ability to simulate different scenarios and provide users with feedback on the potential outcomes. For example, users can adjust the stocking density of their cages or the amount of feed they provide and see how these changes will affect their production and profitability. This helps users to optimize their operations and make the most of their resources.\nThe Shiny app also provides users with access to a range of resources and information on cage aquaculture in Lake Victoria Figure 2. This includes data on water quality, fish species, and market trends, as well as best practices for cage construction and operation.\n\n\n\n\n\nFigure 2: The weather page of cage aquaculture web application in Lake Victoria\n\n\n\n\nHere are some benefits that this interactive web app (dashboard) offers for cage culture in Lake Victoria:\n\nInteractive maps showing the location of cage culture sites and their proximity to sensitive habitats or protected areas.\nData visualizations of water quality data, including temperature, dissolved oxygen, and nutrient levels, to help identify impacts of cage culture on the surrounding ecosystem.\nA dashboard showing trends in fish production, species diversity, and other indicators of the health of the aquaculture system.\nTools for modeling different scenarios, such as changes in feed composition or stocking densities, to help assess the impacts of different management strategies.\nEducational resources, such as videos or articles, to help users learn more about the environmental impacts of cage culture and how they can make informed decisions about their seafood choices.\n\nOverall, the web app for cage aquaculture in Lake Victoria is an invaluable tool for anyone looking to invest in this growing industry. By providing users with the information and tools they need to make informed decisions, this app can help to ensure the sustainability and success of cage aquaculture operations in Tanzania’s largest lake. Furthermore, this Shiny app could be a valuable tool for researchers, policymakers, and members of the public who are interested in understanding the environmental impacts of aquaculture and exploring ways to promote sustainable practices."
  },
  {
    "objectID": "posts/errorRaster/index.html",
    "href": "posts/errorRaster/index.html",
    "title": "How to handle irregular cell size error when creating Raster in R",
    "section": "",
    "text": "In this blog post, I will discuss how to create a spatraster object from a data frame in R. This can be a useful tool for spatial analysis and visualization, as it allows you to work with raster data in R. To begin, I need to make sure I have the necessary packages installed. I will need the magrittr, tidyverse, terra, and sf packages, which can be loaded using the following code:\nrequire(magrittr)\nrequire(terra)\nrequire(tidyterra)\nrequire(tidyverse)\nrequire(sf)\nOnce I have these packages loaded, I can begin creating our spatRaster object. First, I need to create a data frame with our spatial data. This data frame should include columns for the x and y coordinates, as well as any additional variables want to include in our raster. Instead of creating, I am going to download gridded file with rerddap package (Chamberlain, 2019) and specified the geographical bound of the study area as the lines code code below shows;\nchl = rerddap::griddap(\n  x = \"erdMH1chla8day\",\n  longitude = c(38, 39.5),\n  latitude = c(-6.5,-5.0),\n  time = c(\"2022-01-01\", \"2022-01-31\"),\n  fmt = \"csv\") %&gt;%\n    dplyr::as_tibble() %&gt;%\n    dplyr::mutate(time = lubridate::as_date(time))\nchl\n\n# A tibble: 6,845 x 4\n   time       latitude longitude chlorophyll\n   &lt;date&gt;        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 2021-12-31    -4.98      38.0         NaN\n 2 2021-12-31    -4.98      38.0         NaN\n 3 2021-12-31    -4.98      38.1         NaN\n 4 2021-12-31    -4.98      38.1         NaN\n 5 2021-12-31    -4.98      38.1         NaN\n 6 2021-12-31    -4.98      38.2         NaN\n 7 2021-12-31    -4.98      38.2         NaN\n 8 2021-12-31    -4.98      38.3         NaN\n 9 2021-12-31    -4.98      38.3         NaN\n10 2021-12-31    -4.98      38.4         NaN\n# ... with 6,835 more rows\nOnce I have a dataframe, I use the rast() function to convert the xyz data frame to a spatRaster object.\nchl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  select(-time) %&gt;% \n  terra::rast()\nIt frustrate when seeing the error that suggests that the X cell sizes (i.e., the spatial resolution along the x-axis) in your XYZ file are not consistent, which is required to create a spatRaster Layer. Google or ask chatGPT offers a number of solutions on how to address this problem including interpolate the data. However, when interpolate, you change the grids and also the values.\nThat is a flaw especially when using remote sensing data, in which ought to state the spatial resolution have used. Recognizing that, I decided to take a long route that ensure that can create a raster layer that is similar to the original dataset. Let’s go along step by step on how I managed to overcome this hurdle\nlatRange = chl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  select(-time) %$% \n  range(latitude)\n\nlonRange = chl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  select(-time) %$% \n  range(longitude)\nlatLength = chl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  distinct(latitude) %&gt;% \n  pull(latitude) %&gt;% \n  length()\n\nlonLength = chl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  distinct(longitude) %&gt;% \n  pull(longitude) %&gt;% length()\nzvalue = chl %&gt;% \n  filter(time == \"2021-12-31\") %&gt;% \n  pull(chlorophyll)\nchl_rast = matrix(nrow = lonLength, ncol = latLength) %&gt;% \n  terra::rast()\n\nchl_rast\n\nclass       : SpatRaster \ndimensions  : 37, 37, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 37, 0, 37  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :   NaN \nmax value   :   NaN\nnew_ext = terra::ext(lonRange[1],lonRange[2],latRange[1], latRange[2])\nterra::ext(chl_rast) = new_ext\nchl_rast\n\nclass       : SpatRaster \ndimensions  : 37, 37, 1  (nrow, ncol, nlyr)\nresolution  : 0.04054054, 0.04054054  (x, y)\nextent      : 37.97917, 39.47917, -6.479169, -4.979169  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :   NaN \nmax value   :   NaN\nchl_rast = terra::setValues(x = chl_rast, values = zvalue)\n\nchl_rast\n\nclass       : SpatRaster \ndimensions  : 37, 37, 1  (nrow, ncol, nlyr)\nresolution  : 0.04054054, 0.04054054  (x, y)\nextent      : 37.97917, 39.47917, -6.479169, -4.979169  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        :      lyr.1 \nmin value   : 0.09031657 \nmax value   : 2.96432110\nVoila! Now I have a raster layer with defined spatial extent that fit the local area and the chlorophyll values. But I am not sure whether the spatial resolution of 0.04054054 match the 4 kilometer from MODIS. You check that with simple mathematical multiplication. The universe rule of thumb is that one degree for areas close to the equator is equivalent to 110 kilometer and hence by multiplying the spatial resolution of 0.0405 with 110, I get an approximately 4.459 km spatial resolution, which is close to the stated one.\nFinally, I plot the spatRaster object using the geom_spatraster() function for continuous grids and geom_spatraster_contour() function for contour lines in ggplot2 (Wickham, 2016).\nggplot()+\n  geom_spatraster(data = chl_rast, maxcell = 2000)+\n  geom_spatraster_contour(data = chl_rast, breaks = .5)+\n  ggspatial::layer_spatial(tz, fill = \"#84837a\", color = \"black\", linewidth = .5)+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120), trans = scales::log10_trans(), \n                       name = expression(Chl-a~(mg^{-3})))+\n  metR::scale_x_longitude(ticks = .3)+\n  metR::scale_y_latitude(ticks = .3) +\n coord_sf(xlim = c(38.7,39.4), ylim = c(-6.45,-5.5))+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\"))"
  },
  {
    "objectID": "posts/ndvi/index.html",
    "href": "posts/ndvi/index.html",
    "title": "Compute Normalized Difference Vegetation Index in R",
    "section": "",
    "text": "knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)"
  },
  {
    "objectID": "posts/ndvi/index.html#what-is-ndvi",
    "href": "posts/ndvi/index.html#what-is-ndvi",
    "title": "Compute Normalized Difference Vegetation Index in R",
    "section": "What is NDVI",
    "text": "What is NDVI\nNormalized Difference Vegetation Index (NDVI) is a remote sensing index that measures the health and vigor of vegetation. It is widely used in environmental monitoring, land-use change detection, and vegetation dynamics studies. NDVI is calculated from remotely sensed reflectance values of red and near-infrared (NIR) bands using the following formula:\n\\[\nNDVI = \\frac{NIR-Red}{NIR+Red}\n\\]\nNDVI values range from -1 to 1, with higher values indicating healthier vegetation. Negative values indicate water bodies or non-vegetated areas, while zero values indicate bare soil or areas with very low vegetation cover. NDVI has several advantages over traditional field-based methods of vegetation monitoring, such as its ability to cover large areas at once, its ability to detect changes in vegetation cover over time, and its sensitivity to changes in vegetation health.\nLandsat imagery is a widely used source of data for NDVI calculation. Landsat is a series of Earth-observing satellites that collect data in multiple spectral bands, including red and NIR. The data can be downloaded for free from the United States Geological Survey (USGS) website.\nLets first load the package we are going to use in this post, if these packages are not installed in your machine, you can simply install them as they are found in CRAN\n\nrequire(terra)\nrequire(tidyterra)\nrequire(tidyverse)\nrequire(sf)\nrequire(magrittr)\n\nThen we load the Landsat bands. The terra package (Hijmans, 2022) has rast function which can load load the bands into the a single file. For illustration purpose, we will use landsat band from spDataLarge package (Nowosad and Lovelace, 2022). The chunk below highlight a code on how to load the file into our session:\n\nmulti_rast = terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nmulti_rast\n\nclass       : SpatRaster \ndimensions  : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values  :      7550,      6404,      5678,      5252 \nmax values  :     19071,     22051,     25780,     31961 \n\n\nThe printed object is spatRaster object. SpatRaster is an object class in R that is used to represent spatial raster data. It is a three-dimensional array that contains data on different layers or bands, with the two spatial dimensions representing the row and column coordinates of the raster cells. In addition, our SpatRaster objects have several attributes that describe its spatial properties, such as extent, resolution, and projection.\nThe spatRaster object has four satellite bands - blue, green, red, and near-infrared (NIR). We can rename this band to their corresponding band names using tidyterra package (Hernangómez, 2023) function rename\n\nmulti_rast = multi_rast %&gt;% \n  rename(\n    blue = landsat_1,\n    green = landsat_2,\n    red = landsat_3,\n    nir = landsat_4\n  )\n\nmulti_rast\n\nclass       : SpatRaster \ndimensions  : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       :  blue, green,   red,   nir \nmin values  :  7550,  6404,  5678,  5252 \nmax values  : 19071, 22051, 25780, 31961 \n\n\nOnce we have renamed the band with appropriate band names, our next step should be to compute the NDVI formula into an R function. Thanks again to tidyterra package that has simplified computation of spatRaster object using verbs similar to tidyverse package (Wickham and Wickham, 2017). For us we are going to compute the ndvi as separate band in the spatraster object using mutate function. NDVI can be computed using the formula mentioned above. Here’s the code:\n\nmulti_rast = multi_rast %&gt;% \n  tidyterra::mutate(ndvi = (nir - red)/(nir + red))\n\nmulti_rast\n\nclass       : SpatRaster \ndimensions  : 1428, 1128, 5  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource(s)   : memory\nnames       :  blue, green,   red,   nir,       ndvi \nmin values  :  7550,  6404,  5678,  5252, -0.2352531 \nmax values  : 19071, 22051, 25780, 31961,  0.6076995 \n\n\nThe printed spatRaster object has added a fifth layer as ndvi. The ndvi object now contains the computed NDVI values for each pixel. The computed NDVI raster can be visualized using various R packages, such as rasterVis or ggplot2 (Wickham, 2016). Here’s an example using ggplot2 with additional function from tidyterra package: This code will produce a color-coded plot of the NDVI raster, with higher values shown in shades of green and lower values shown in shades of brown.\n\nggplot()+\n  tidyterra::geom_spatraster(data = multi_rast[[5]])+\n  scale_fill_gradientn(colours = hcl.colors(n = 5, palette = \"Red-Green\"), \n                       guide = guide_colorbar(reverse = TRUE, title = \"NDVI\")) +\n  theme_minimal()\n\n\n\n\nFigure 1: Computed NDVI with color gradient where green color indicate a greenish land cover"
  },
  {
    "objectID": "posts/ndvi/index.html#summary",
    "href": "posts/ndvi/index.html#summary",
    "title": "Compute Normalized Difference Vegetation Index in R",
    "section": "Summary",
    "text": "Summary\nIn this post, we’ll walk through the steps of computing NDVI from Landsat imagery using R. Specifically, we’ll cover how to load the Landsat bands, compute NDVI raster to a file, and visualize the NDVI raster. I hope the information in this post can help you to use R to calculate NDVI from Landsat imagery and use it for further analysis and visualization."
  },
  {
    "objectID": "posts/ndvi/index.html#introduction",
    "href": "posts/ndvi/index.html#introduction",
    "title": "Compute Normalized Difference Vegetation Index in R",
    "section": "Introduction:",
    "text": "Introduction:\nIn this post, we’re going to learn how to compute Normalized Difference Vegetation Index (NDVI) from Landsat imagery using R. NDVI is a widely used remote sensing index that measures the health and vigor of vegetation. It’s calculated from the difference between near-infrared (NIR) and red reflectance values divided by their sum. NDVI values range from -1 to 1, with higher values indicating healthier vegetation. NDVI is an important tool for studying vegetation dynamics, land-use changes, and environmental monitoring.\nRemote sensing image are useful tool that provide satellite data covering a large area and with high frequency temporal resolution. Such satellite image include Landsat. Landsat imagery is a valuable source of data for NDVI calculation. The Landsat satellites collect data in multiple spectral bands, including red and NIR, which can be used to compute NDVI.\nIn this post, we’ll walk through the steps of computing NDVI from Landsat imagery using R. R is a popular programming language for statistics and and provides a range of packages for processing Landsat imagery and computing NDVI. We’ll cover how to load the Landsat bands, compute NDVI, write the NDVI raster to a file, and visualize the NDVI raster. By the end of this post, you’ll be able to use R to calculate NDVI from Landsat imagery and use it for further analysis and visualization."
  },
  {
    "objectID": "posts/dataScienceGap/index.html",
    "href": "posts/dataScienceGap/index.html",
    "title": "The Skill Gap in Data Science",
    "section": "",
    "text": "As we celebrate Labour Day on 1st May 2023, it’s important to reflect on the changing nature of work and the skills required to succeed in the modern workplace. One area where the skill gap is particularly acute is data science. With the exponential growth of data in recent years, the demand for data scientists has skyrocketed. However, universities have struggled to keep up with the pace of change, leaving many graduates ill-equipped for the demands of the job market.\nUniversities have been slow to respond to the growing demand for data scientists, with many struggling to keep up with the rapid pace of change. However, some institutions have taken steps to bridge the gap. In Tanzania, the skill gap in data science is particularly acute. Only one institution, the University of Dar es Salaam (UDSM), offers a data science course at the master’s level. This has left many graduates without the skills required by employers, hindering economic development in the country.\nFor example, many universities have introduced data science courses at both undergraduate and postgraduate levels. These courses typically cover a range of topics, including statistics, machine learning, and programming languages such as Python and R. However, while these courses are a step in the right direction, they often fall short of the skills required by employers. Many students graduate with a theoretical understanding of data science but lack practical experience in applying these skills to real-world problems.\nBlogs and websites have played a crucial role in bridging the skill gap in data science. These platforms provide a wealth of resources for aspiring data scientists, including tutorials, case studies, and discussions on the latest trends in the field.\nOne example of a website that has helped to overcome the challenge is Kaggle1. Kaggle is a popular online platform for data scientists and machine learning enthusiasts. The platform hosts a variety of competitions and challenges related to data science, providing a platform for professionals and enthusiasts to showcase their skills and collaborate on complex problems. The platform also provides a wealth of datasets for users to explore, analyze, and visualize. The community is known for its highly collaborative and supportive nature, with users sharing code, insights, and feedback to help each other improve their skills.\nThere are several online platforms similar to Kaggle, each offering their own unique features and community. One such platform is DrivenData2, which hosts data science competitions with a focus on social impact. DrivenData’s challenges tackle real-world problems related to healthcare, poverty, and education, providing a platform for data scientists to contribute to society while improving their skills.\nDataCamp3 is also a platform that offers a variety of courses and challenges related to data science and machine learning. The platform provides interactive coding environments and personalized learning paths, making it a valuable resource for anyone looking to learn or upskill in the field.\nIn conclusion, the skill gap in data science is a significant challenge facing both individuals and economies around the world. While universities have taken steps to bridge the gap, more needs to be done to ensure that graduates are equipped with the skills required by employers. Blogs and websites have played a crucial role in providing aspiring data scientists with practical experience and resources to develop their skills. Tanzania’s example shows that partnerships between universities and industry leaders can help to overcome the challenge and drive economic growth."
  },
  {
    "objectID": "posts/dataScienceGap/index.html#cited-sources",
    "href": "posts/dataScienceGap/index.html#cited-sources",
    "title": "The Skill Gap in Data Science",
    "section": "Cited sources",
    "text": "Cited sources"
  },
  {
    "objectID": "posts/dataScienceGap/index.html#footnotes",
    "href": "posts/dataScienceGap/index.html#footnotes",
    "title": "The Skill Gap in Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.kaggle.com/↩︎\nhttps://www.drivendata.org/↩︎\nhttps://www.datacamp.com/↩︎"
  },
  {
    "objectID": "posts/streamline/index.html",
    "href": "posts/streamline/index.html",
    "title": "Streamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region",
    "section": "",
    "text": "Streamlines are imaginary lines that are tangent to the velocity vector at every point in space. They represent the path that a fluid particle would follow if it were released into the flow. The density of streamlines represents the speed of the fluid at each point.\nStreamlines are a powerful visualization tool used to understand fluid flow patterns. They are a series of lines that show the direction and magnitude of fluid motion at every point in space. Streamlines are commonly used in fields such as aerodynamics, hydrodynamics, and meteorology to analyze and visualize fluid flow patterns.\nIn this blog post, we will describe how to plot streamlines for surface current using R language. We will use the coastal waters of the Western Indian Ocean region as a case study, using drifter data from GDP. I will show you how to visualize vector field of ocean surface current using the ggplot2 package (Wickham, 2016).\nI will further highlight the drawbacks of the default geom of ggplot2 and why sometimes ggplot2 functions fail to produce elegant oceanographic plots. Last I will illustrate the use of alternative geoms from metR package (Campitelli, 2019) to overcome the challenges inherited in ggplot2 package.\nIn R, loading packages is an essential step to access additional functionality and tools for data manipulation, visualization, and analysis. The chunk below load the packages needed in this post;\n\nrequire(metR)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(oce)\nrequire(sf)\nrequire(patchwork)\n\nWe aslo need a basemap for our area of interest. I simply load the world map spatial object from the spData package and then uses the sf package’s st_crop() function to extract a the East African land mass of the world map based on specified minimum and maximum values for the longitude and latitude coordinates. The resulting spatial object, wio, represents the cropped WIO region area.\n\nworld = spData::world\n\nwio = world |&gt;\n  st_crop(xmin =35 , ymin = -15, xmax = 50, ymax = -2)"
  },
  {
    "objectID": "posts/streamline/index.html#streamline",
    "href": "posts/streamline/index.html#streamline",
    "title": "Streamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region",
    "section": "",
    "text": "Streamlines are imaginary lines that are tangent to the velocity vector at every point in space. They represent the path that a fluid particle would follow if it were released into the flow. The density of streamlines represents the speed of the fluid at each point.\nStreamlines are a powerful visualization tool used to understand fluid flow patterns. They are a series of lines that show the direction and magnitude of fluid motion at every point in space. Streamlines are commonly used in fields such as aerodynamics, hydrodynamics, and meteorology to analyze and visualize fluid flow patterns.\nIn this blog post, we will describe how to plot streamlines for surface current using R language. We will use the coastal waters of the Western Indian Ocean region as a case study, using drifter data from GDP. I will show you how to visualize vector field of ocean surface current using the ggplot2 package (Wickham, 2016).\nI will further highlight the drawbacks of the default geom of ggplot2 and why sometimes ggplot2 functions fail to produce elegant oceanographic plots. Last I will illustrate the use of alternative geoms from metR package (Campitelli, 2019) to overcome the challenges inherited in ggplot2 package.\nIn R, loading packages is an essential step to access additional functionality and tools for data manipulation, visualization, and analysis. The chunk below load the packages needed in this post;\n\nrequire(metR)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(oce)\nrequire(sf)\nrequire(patchwork)\n\nWe aslo need a basemap for our area of interest. I simply load the world map spatial object from the spData package and then uses the sf package’s st_crop() function to extract a the East African land mass of the world map based on specified minimum and maximum values for the longitude and latitude coordinates. The resulting spatial object, wio, represents the cropped WIO region area.\n\nworld = spData::world\n\nwio = world |&gt;\n  st_crop(xmin =35 , ymin = -15, xmax = 50, ymax = -2)"
  },
  {
    "objectID": "posts/streamline/index.html#dataset",
    "href": "posts/streamline/index.html#dataset",
    "title": "Streamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region",
    "section": "Dataset",
    "text": "Dataset\nTo illustrate the concept, I am going to use drifter data from Global Drifter Program (GDP). Drifters are oceanographic instruments that are used to track the movement of surface currents. They consist of a buoy that floats on the ocean’s surface, and a drogue that is attached to the buoy and trails behind it at a depth of 15 meters. The drogue is designed to move with the water, providing an accurate measurement of the surface current.\nThe Drifter dataset comprises information on surface current velocity associated with geographical locations. This spatial data allows us to gain insights into the distribution of high-speed and low-speed currents in the ocean. Visualizing this data within a geospatial context, such as displaying it on an accurate map, is beneficial. I have filtered the data to focus on the western region of the tropical Indian Ocean. I have organized and structured the drifter information into a data frame, which is a tabular representation consisting of variables (columns) and observations (rows). The dataset encompasses surface current observations worldwide, collected by the Global Drifter Program across major oceans. The Table 1 displays some of the variables included in the dataset.\n\nid: a unique number of drifter\nlon: longitude information\nlat: latitude information\nyear, month, day, hour of the drifter records\nu: the zonal component of surface current\nv: the meridional component of the surface current\n\n\nload(\"drifter_split.RData\")\n\n\ndrifter.split%&gt;%\n  select(-c(drogue, season, sst)) %&gt;% \n  sample_n(12) %&gt;% \n  knitr::kable(format = \"html\",caption = \"A sample of drifter dataset\", digits = 2, align = \"c\")%&gt;%\n  kableExtra::column_spec(column = 1:9, width = \"3cm\")%&gt;%\n  kableExtra::add_header_above(c(\"\", \"Location\" = 2, \"Date Records\" = 4,  \"Current\" = 2))\n\n\n\nTable 1: A sample of drifter dataset of the WIO region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nDate Records\n\n\nCurrent\n\n\n\nid\nlon\nlat\nyear\nmonth\nday\nhour\nu\nv\n\n\n\n\n114807\n51.19\n-16.58\n2013\n6\n7\n0\n0.23\n0.19\n\n\n63945810\n50.99\n-4.53\n2017\n12\n30\n18\n0.53\n0.52\n\n\n64726990\n40.03\n-6.82\n2017\n7\n1\n18\n-0.64\n1.40\n\n\n109535\n47.32\n-5.43\n2014\n11\n20\n18\n-0.13\n-0.26\n\n\n9727912\n42.50\n-9.83\n1998\n10\n3\n6\n-0.08\n0.16\n\n\n25006\n52.43\n-11.27\n2001\n8\n5\n12\n-0.27\n0.11\n\n\n63356010\n45.10\n-6.74\n2017\n4\n16\n18\n0.64\n0.16\n\n\n63816\n51.43\n-4.45\n2009\n10\n23\n12\n0.17\n0.31\n\n\n45979\n44.52\n-12.50\n2005\n5\n14\n0\n-0.37\n-0.19\n\n\n52953\n47.45\n-11.91\n2005\n1\n16\n12\n-0.09\n-0.11\n\n\n63071\n50.12\n-4.17\n2008\n1\n10\n18\n0.15\n-0.13\n\n\n61478290\n53.84\n-12.38\n2016\n1\n4\n18\n-0.32\n-0.03\n\n\n\n\n\n\n\n\nThe drifter observations in the specified area, as illustrated in Figure 1, were randomly scattered. To facilitate analysis and visualization, it is necessary to apply binning, which involves creating a grid with equally sized cells across the area. This binning process enables the organization of observations into a structured grid, allowing for easier interpretation and further exploration of the data.\n\n## convert drifter observation into simple features\n\ndrifter.split.sf = drifter.split %&gt;% \n  st_as_sf(coords = c(\"lon\", \"lat\")) %&gt;%\n  st_set_crs(4326)\n\n\nggplot() +\n  geom_point(data = drifter.split %&gt;% filter(season == \"SE\") %&gt;% \n               sample_frac(.05), aes(x = lon, y = lat))+\n  geom_sf(data = wio, fill = \"lightgrey\", col = \"black\")+\n  coord_sf(ylim = c(-12.5,-3), xlim = c(36, 48))+\n  theme_bw()+\n  theme(axis.text = element_text(size = 12, colour = 1))+\n  labs(x = \"\", y = \"\")\n\n\n\n\nFigure 1: The distribution of drifter observation within the area"
  },
  {
    "objectID": "posts/streamline/index.html#binning",
    "href": "posts/streamline/index.html#binning",
    "title": "Streamlines for Surface Currents in Coastal Waters of Western Indian Ocean Region",
    "section": "Binning",
    "text": "Binning\nTo plot streamlines in R, we first need to create a grid of points that covers the area of interest. We can do this using the st_make_grid function from the sf package. The st_make_grid function takes two vectors as input and returns the area divided into equal size grids show in in Figure 2.\n\n## divide the tropical indian ocean region into grids\ndrifter.grid = drifter.split.sf %&gt;% \n  st_make_grid(n = c(40,30))%&gt;%\n  st_sf()\n\n\n## \nggplot()+geom_sf(data = drifter.grid)+\n  geom_sf(data = wio,fill = \"lightgrey\", col = \"black\")+\n  coord_sf(ylim = c(-12.5,-4.5), xlim = c(36, 48))+\n  theme_bw()+\n  theme(axis.text = element_text(size = 12, colour = 1))\n\n\n\n\nFigure 2: Gridded area to fill drifter observations\n\n\n\n\nSubsequently, after performing the binning process to organize the randomly distributed drifter observations into a grid, we can proceed with further computations. Specifically, we can calculate the number of drifters present in each individual grid cell, providing valuable insights into the density and distribution patterns across the area. Additionally, we can analyze the binned observations to determine the mean zonal (east-west) and meridional (north-south) velocity fields. Since we have split drifter data into northeast (NE) and southeast (SE) monsoon season, let’s begin with NE season;\n\ndrifter.split.sf.ne = drifter.split.sf%&gt;% filter(season ==\"NE\")\n\ndrifter.gridded = drifter.grid %&gt;% \n  mutate(id = 1:n(), \n         contained = lapply(st_contains(st_sf(geometry),drifter.split.sf.ne),identity),\n         obs = sapply(contained, length),\n         u = sapply(contained, function(x) {median(drifter.split.sf.ne[x,]$u, na.rm = TRUE)}),\n         v = sapply(contained, function(x) {median(drifter.split.sf.ne[x,]$v, na.rm = TRUE)}))\n\ndrifter.gridded = drifter.gridded %&gt;% \n  select(obs, u, v) %&gt;% \n  na.omit()\n\ncoordinates = drifter.gridded %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;% \n  as_tibble() %&gt;% rename(x = X, y = Y)\n\nst_geometry(drifter.gridded) = NULL\n\ncurrent.gridded.ne = coordinates %&gt;% \n  bind_cols(drifter.gridded)%&gt;% \n  mutate(season = \"NE\")\n\nThen, the same process is done for the southeast monsoon season\n\ndrifter.split.sf.se = drifter.split.sf%&gt;% filter(season==\"SE\")\n\ndrifter.gridded = drifter.grid %&gt;% \n  mutate(id = 1:n(), contained = lapply(st_contains(st_sf(geometry),drifter.split.sf.se),identity),\n         obs = sapply(contained, length),\n         u = sapply(contained, function(x) {median(drifter.split.sf.se[x,]$u, na.rm = TRUE)}),\n         v = sapply(contained, function(x) {median(drifter.split.sf.se[x,]$v, na.rm = TRUE)})) \n\n\ndrifter.gridded = drifter.gridded %&gt;% \n  select(obs, u, v) %&gt;% \n  na.omit()\n\n## obtain the centroid coordinates from the grid as table\ncoordinates = drifter.gridded %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;% \n  as_tibble() %&gt;% \n  rename(x = X, y = Y)\n\n## remove the geometry from the simple feature of gridded drifter dataset\nst_geometry(drifter.gridded) = NULL\n\n## stitch together the extracted coordinates and drifter information int a single table for SE monsoon season\ncurrent.gridded.se = coordinates %&gt;% \n  bind_cols(drifter.gridded) %&gt;% \n  mutate(season = \"SE\")\n\n## bind the gridded table for SE and NE\n## Note that similar NE follow similar procedure, hence not shown in the post\ndrifter.current.gridded = current.gridded.ne %&gt;% \n  bind_rows(current.gridded.se)\n\nUpon analyzing the number of observations and velocity field within each grid cell, it became evident that certain grids lacked any drifter observations. To ensure smooth visualization, it was crucial to address this issue by filling these empty grids with appropriate U and V values. To accomplish this, an interpolation technique was employed. By applying interpolation, the missing U and V values were estimated for the empty grids, enabling a more seamless visualization of the data. The provided code chunk highlights the implementation of the interpolation process.\n\n## select grids for SE season only\ndrf.se = drifter.current.gridded %&gt;%\n  filter(season == \"SE\")\n\n## interpolate the U component\nu.se = interpBarnes(x = drf.se$x, y = drf.se$y, z = drf.se$u)\n\n## obtain dimension that determine the width (ncol) and length (nrow) for tranforming wide into long format table\ndimension = data.frame(lon = u.se$xg, u.se$zg) %&gt;% dim()\n\n## make a U component data table from interpolated matrix\nu.tb = data.frame(lon = u.se$xg, \n                  u.se$zg) %&gt;% \n  gather(key = \"lata\", value = \"u\", 2:dimension[2]) %&gt;% \n  mutate(lat = rep(u.se$yg, each = dimension[1])) %&gt;% \n  select(lon,lat, u) %&gt;% as.tibble()\n\n## interpolate the V component\nv.se = interpBarnes(x = drf.se$x, \n                    y = drf.se$y, \n                    z = drf.se$v)\n\n## make the V component data table from interpolated matrix\nv.tb = data.frame(lon = v.se$xg, v.se$zg) %&gt;% \n  gather(key = \"lata\", value = \"v\", 2:dimension[2]) %&gt;% \n  mutate(lat = rep(v.se$yg, each = dimension[1])) %&gt;% \n  select(lon,lat, v) %&gt;% \n  as.tibble()\n\nOnce the zonal and meridional velocities have been filled in the grids, it becomes possible to calculate the velocity field at each individual grid point. This can be achieved by computing the magnitude of the velocity vector using the equation:\n\\[\nvel = \\sqrt{v^2 + u^2}\n\\] In this equation, the variable v represents the meridional velocity component, while u represents the zonal velocity component. By taking the square root of the sum of the squares of these components, we obtain the magnitude of the velocity vector, denoted as vel. This magnitude represents the overall speed or intensity of the velocity at each point in the grid, providing valuable insights into the spatial variations and patterns of the velocity field. The provided code chunk highlights the computation of the velocity field.\n\n## stitch now the V component intot the U data table and compute the velocity\nuv.se = u.tb %&gt;% \n  bind_cols(v.tb %&gt;% select(v)) %&gt;% \n  mutate(vel = sqrt(u^2+v^2))\n\nThe code takes two data frames, u.tb and v.tb, and combines them into a new data frame and compute the velocity of the surface current and store them in a new column called ve from the corresponding values from the u and v columns.\n\nVisualising Vector field\nFigure 3 was made with ggplot2 package. The smoothed velocity grid was created with the geom_raster() function and the vector field showing the direction and speed superimposed on the current velocity was done with the segment() function. You notice that these geoms did a wonderful job and the figure is surface current speed and direction are clearly distinguished with the arrow and the length of the arrow. The code used to create Figure 3 are shown in the chunk below.\n\nggplot() +\n  geom_raster(data = uv.se, aes(x = lon, y = lat, fill = vel), interpolate = TRUE)+\n  geom_segment(data = uv.se, aes(x = lon, xend = lon + u/1.2, y = lat, yend = lat+v/1.2), \n               arrow = arrow(angle = 20, length = unit(.2, \"cm\"), type = \"open\"))+\n  geom_sf(data = wio,fill = \"lightgrey\", col = \"black\")+\n  coord_sf(ylim = c(-12.5,-4.5), xlim = c(38, 48))+\n  scale_fill_gradientn(name = \"Current\",colours = oceColorsVelocity(120), \n                       limits = c(0,1.6), breaks = seq(0.1,1.6,.3),\n                       guide = guide_colorbar(barwidth = .8, barheight = 15,\n                                              title = expression(Current~speed~(ms^{-1})), \n                                              title.position = \"right\", title.hjust = .5, \n                                              title.theme = element_text(angle = 90)))+\n  theme_bw()+\n  theme(legend.position = \"right\",\n        legend.key.height = unit(1.4, \"cm\"), \n        legend.background = element_blank(),\n        axis.text = element_text(size = 12, colour = 1))+\n  labs(x = \"\", y = \"\")\n\n\n\n\nFigure 3: Vector field of speed and direction of surface current\n\n\n\n\nTo plot the streamlines, i used geom_streamline() from metR package (Campitelli, 2019). The geom_streamline() function takes the x and y coordinates of each point on the grid and the x and y components of the velocity vector as input and generates the streamlines.\n\nggplot() +\n  metR::geom_contour_fill(data = uv.se, aes(x = lon, y = lat, z = vel), \n                          na.fill = TRUE, bins = 70) + \n  metR::geom_streamline(data = uv.se, \n                        aes(x = lon, y = lat, dx = u, dy = v),\n                        L = .9, res = 4, n = 40, jitter = 4)+\n  geom_sf(data = wio,fill = \"lightgrey\", col = \"black\")+\n  coord_sf(ylim = c(-12.5,-4.5), xlim = c(38, 48))+\n  scale_fill_gradientn(name = \"Current\",colours = oceColorsVelocity(120), \n                       limits = c(0,1.6), breaks = seq(0.1,1.6,.3),\n                       guide = guide_colorbar(barwidth = .8, barheight = 15,\n                                              title = expression(Current~speed~(ms^{-1})), \n                                              title.position = \"right\", title.hjust = .5, \n                                              title.theme = element_text(angle = 90)))+\n  theme_bw()+\n  theme(legend.position = \"right\",\n        legend.key.height = unit(1.4, \"cm\"), \n        legend.background = element_blank(),\n        axis.text = element_text(size = 12, colour = 1))+\n  labs(x = \"\", y = \"\")\n\n\n\n\nFigure 4: Streamline of surface current superimposed on the current velocity raster\n\n\n\n\n\n\nConclusion\nMany Oceanographic data are defined in a longitude–latitude grid and and though geom_raster() and geom_segment() plot these field well, but the function from metR package extend the use of ggplot to deal well with oceanographical data and produce graphics that meet the standard. The massless flow of surface current are much clear when stremline are used to plot the figure (Figure 5). The lines of codes used to make Figure 5 are shown in the chunk below.\n\nggplot()+\nmetR::geom_streamline(data = uv.se, \n                        aes(x = lon, y = lat, dx = u, dy = v, \n                            color = sqrt(..dx..^2 + ..dy..^2), \n                            alpha = ..step..),\n                      L = 2, res = 2, n = 40, \n                      arrow = NULL, lineend = \"round\")+\n  geom_sf(data = wio,fill = \"lightgrey\", col = \"black\")+\n  coord_sf(ylim = c(-12.5,-4.5), xlim = c(38, 48))+\n  scale_color_viridis_c(\n    guide = guide_colorbar(barwidth = .8, barheight = 15,\n                           title = expression(Current~speed~(ms^{-1})), \n                           title.position = \"right\", title.hjust = .5, \n                           title.theme = element_text(angle = 90)))+\n  scale_size(range = c(0.2, 1.5), guide = \"none\") +\n  scale_alpha(guide = \"none\") +\n  theme_bw()+\n  theme(legend.position = \"right\",\n        legend.key.height = unit(1.4, \"cm\"), \n        legend.background = element_blank(),\n        axis.text = element_text(size = 12, colour = 1))+  \n  labs(x = \"\", y = \"\")\n\n\n\n\nFigure 5: streamlines of surface current flow"
  },
  {
    "objectID": "posts/ims/index.html",
    "href": "posts/ims/index.html",
    "title": "Information Management Strategy for Coastal and Marine Data",
    "section": "",
    "text": "Coastal and marine data is crucial for various stakeholders, including government agencies, non-governmental organizations, researchers, and the private sector. However, managing this data can be a daunting task, given the complexity of the coastal and marine environment. An information management strategy is a framework that outlines how data will be collected, stored, analyzed, and disseminated to meet the needs of stakeholders. In this article, we will discuss what an information management strategy is and how to write one for coastal and marine data."
  },
  {
    "objectID": "posts/ims/index.html#introduction",
    "href": "posts/ims/index.html#introduction",
    "title": "Information Management Strategy for Coastal and Marine Data",
    "section": "",
    "text": "Coastal and marine data is crucial for various stakeholders, including government agencies, non-governmental organizations, researchers, and the private sector. However, managing this data can be a daunting task, given the complexity of the coastal and marine environment. An information management strategy is a framework that outlines how data will be collected, stored, analyzed, and disseminated to meet the needs of stakeholders. In this article, we will discuss what an information management strategy is and how to write one for coastal and marine data."
  },
  {
    "objectID": "posts/ims/index.html#what-is-an-information-management-strategy",
    "href": "posts/ims/index.html#what-is-an-information-management-strategy",
    "title": "Information Management Strategy for Coastal and Marine Data",
    "section": "What is an Information Management Strategy?",
    "text": "What is an Information Management Strategy?\nAn information management strategy is a plan that outlines how an organization will manage its data. It includes policies, procedures, and practices that ensure data quality, security, accessibility, and usability. An effective information management strategy should align with the organization’s goals and objectives, as well as its overall business strategy.\nAn information management strategy typically includes the following components:\n\nData Governance: This refers to the policies and procedures that ensure data quality, security, and privacy. It includes roles and responsibilities for data management, data standards, and data security measures.\nData Architecture: This refers to the design of the data infrastructure, including databases, data warehouses, and data lakes. It includes the organization’s data models, data flow diagrams, and data dictionaries.\nData Management: This refers to the processes for collecting, storing, and managing data. It includes data entry procedures, data cleaning processes, and data storage protocols.\nData Analysis: This refers to the processes for analyzing data to derive insights. It includes statistical analysis, machine learning algorithms, and data visualization.\nData Dissemination: This refers to the processes for sharing data with stakeholders. It includes data publishing protocols, data access policies, and data sharing agreements.\n\nHow to Write an Information Management Strategy for Coastal and Marine Data?\nWriting an information management strategy for coastal and marine data can be challenging due to the complexity of the environment. However, following these steps can help you develop an effective strategy:\n\nStep 1: Define the Scope\nThe first step in writing an information management strategy is to define the scope of the strategy. This includes identifying the stakeholders who will use the data and the types of data that will be managed. For coastal and marine data, stakeholders may include government agencies responsible for managing coastal resources, researchers studying marine ecosystems, and private sector companies involved in offshore activities. The types of data may include oceanographic data, ecological data, socio-economic data, and geospatial data.\n\n\nStep 2: Assess Data Needs\nThe next step is to assess the data needs of stakeholders. This involves identifying the types of data that stakeholders require to achieve their objectives. For example, government agencies may require oceanographic data to monitor water quality and weather patterns, while researchers may require ecological data to study marine biodiversity.\n\n\nStep 3: Develop Data Governance Policies\nOnce you have identified the stakeholders and their data needs, the next step is to develop data governance policies. This includes defining roles and responsibilities for data management, establishing data quality standards, and ensuring compliance with relevant regulations.\n\n\nStep 4: Design Data Architecture\nThe next step is to design the data architecture. This involves selecting appropriate database technologies and designing a schema that reflects the relationships between different types of data. For example, oceanographic data may be linked to ecological data to understand how changes in water quality affect marine biodiversity.\n\n\nStep 5: Develop Data Management Processes\nThe next step is to develop data management processes. This includes defining procedures for collecting, storing, and managing data. For example, procedures may be established for collecting oceanographic data using sensors deployed on buoys or research vessels.\n\n\nStep 6: Define Data Analysis Procedures\nThe next step is to define procedures for analyzing data. This includes selecting appropriate statistical methods or machine learning algorithms to derive insights from the data. For example, machine learning algorithms may be used to predict changes in water quality based on historical oceanographic data.\n\n\nStep 7: Develop Data Dissemination Protocols\nThe final step is to develop protocols for disseminating data to stakeholders. This includes defining access policies and sharing agreements that ensure that sensitive or confidential information is protected. For example, access controls may be established to limit access to certain types of data based on user roles or permissions."
  },
  {
    "objectID": "posts/ims/index.html#conclusion",
    "href": "posts/ims/index.html#conclusion",
    "title": "Information Management Strategy for Coastal and Marine Data",
    "section": "Conclusion",
    "text": "Conclusion\nAn information management strategy is essential for managing coastal and marine data effectively. It provides a framework for collecting, storing, analyzing, and disseminating data to meet the needs of stakeholders. Developing an effective information management strategy requires careful planning and consideration of stakeholder needs. By following these steps, organizations can develop a strategy that aligns with their goals and objectives while ensuring that coastal and marine resources are managed sustainably for future generations."
  },
  {
    "objectID": "posts/ims/index.html#cited-sources",
    "href": "posts/ims/index.html#cited-sources",
    "title": "Information Management Strategy for Coastal and Marine Data",
    "section": "Cited sources",
    "text": "Cited sources"
  }
]