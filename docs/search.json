[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "plotting in Python with Seaborn: box plot\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: bar plot\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Line plot\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Joint plot\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\nplotting in Python with Seaborn: Distribution plot\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\nMulti-lingual: R and Python for Data Science\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\nMain and Inset maps with R\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n\n\n\n\n\n\nInferential statistics in R:ttest\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\nCombining plots in R\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n\n\n\n\n\n\nManipulating Simple features: point to polyline\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nForecasting Rising Temperature with prophet package in R\n\n\n\n\n\n\n\n\n\nAug 13, 2022\n\n\n\n\n\n\n\n\nTeacher’s Employment Allocations by LGA\n\n\n\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\nSpatial Data is Maturing in R\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\nGetting and Processing Satellite Data Made Easier in R\n\n\n\n\n\n\n\n\n\nSep 18, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "ng'ara",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: box plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 24, 2023\n\n\nMasumbuko Semba\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: bar plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 23, 2023\n\n\nMasumbuko Semba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Joint plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 22, 2023\n\n\nMasumbuko Semba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Line plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 22, 2023\n\n\nMasumbuko Semba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nplotting in Python with Seaborn: Distribution plot\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFebruary 21, 2023\n\n\nMasumbuko Semba\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-lingual: R and Python for Data Science\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 20, 2023\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMain and Inset maps with R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 19, 2023\n\n\nMasumbuko Semba\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferential statistics in R:ttest\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 15, 2023\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCombining plots in R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFebruary 4, 2023\n\n\nMasumbuko Semba, Nyamisi Peter\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating Simple features: point to polyline\n\n\n\n\n\n\n\nManipulation\n\n\nVisualization\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 1, 2022\n\n\nMasumbuko Semba\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Rising Temperature with prophet package in R\n\n\n\n\n\n\n\ncode\n\n\nAnalysis\n\n\nSpatial\n\n\n\n\n\n\n\n\n\n\n\nAugust 13, 2022\n\n\nMasumbuko Semba\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTeacher’s Employment Allocations by LGA\n\n\n\n\n\n\n\nVisualization\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nAugust 11, 2022\n\n\nMasjumbuko Semba\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Data is Maturing in R\n\n\n\n\n\n\n\nAnalysis\n\n\nSpatial\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nAugust 8, 2022\n\n\nMasjumbuko Semba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nGetting and Processing Satellite Data Made Easier in R\n\n\n\n\n\n\n\nAnalysis\n\n\nVisualization\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18, 2018\n\n\nMasumbuko Semba\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ng'ara",
    "section": "",
    "text": "Hi, I’m Masumbuko Semba, a data scientist and spatial analyst and oceanographer. With experience in programming, I have developed tools, algorithms and techniques, and analytical workflows that makes easy to organize, manage, analyse, model data and share the findings and insights with modern web-tehnology media. I’m multi-lingual — use several programming languages – MATLAB, Python, Observable JavaScript, and R to automate data processing and analysis and generate reports in either PDF or html documents. The scripts and tools allows reproducibility of data analysis worklow and reporting to support in decision making. These code and some of the work that I do on daily basis are freely available to the public to use at the my blog, website and github accounts."
  },
  {
    "objectID": "index.html#peer-review-articles",
    "href": "index.html#peer-review-articles",
    "title": "ng'ara",
    "section": "Peer Review Articles",
    "text": "Peer Review Articles\n\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. S. (2018). The influence of physical-chemical variables on the spatial and seasonal variation of Chlorophyll-a in coastal waters of Unguja, Zanzibar, Tanzania. Western Indian Ocean Journal of Marine Science, 17(2), 25-34.\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. (2021). Seasonal variability of vertical patterns in chlorophyll-a fluorescence in the coastal waters off Kimbiji, Tanzania. Western Indian Ocean Journal of Marine Science, 20(1), 21-33.\nSemba, M., Kimirei, I., Kyewalyanga, M., Peter, N., Brendonck, L., & Somers, B. (2016). The decline in phytoplankton biomass and prawn catches in the Rufiji-Mafia Channel, Tanzania. Western Indian Ocean Journal of Marine Science, 15(1), 15-29.\nSemba, M., Lumpkin, R., Kimirei, I., Shaghude, Y., & Nyandwi, N. (2019). Seasonal and spatial variation of surface current in the Pemba Channel, Tanzania. PloS one, 14(1), e0210303\nCrawford, B., Herrera, M. D., Hernandez, N., Leclair, C. R., Jiddawi, N., Masumbuko, S., & Haws, M. (2010). Small scale fisheries management: lessons from cockle harvesters in Nicaragua and Tanzania. Coastal Management, 38(3), 195-215.\nKyewalyanga, M. S., Peter, N., Semba, M., & Mahongo, S. B. (2020). Coastal upwelling and seasonal variation in phytoplankton biomass in the Pemba Channel. Western Indian Ocean Journal of Marine Science, (1/2020), 19-32.\nSilas, M. O., Kishe, M. A., Mshana, J. G., Semba, M. L., Mgeleka, S. S., Kuboja, B. N., … & Matiku, P. (2021). Growth, mortality, exploitation rate and recruitment pattern of Octopus cyanea (Mollusca: Cephalopoda) in the WIO region: A case study from the Mafia Archipelago, Tanzania. Western Indian Ocean Journal of Marine Science, 20(1), 71-79.\nSilas, M. O., Semba, M. L., Mgeleka, S., Linderholm, H. W., & Gullström, M. (2022). Fishers’ ecological knowledge for management in data-poor fisheries: a case of East African small-scale fisheries in a changing climate."
  },
  {
    "objectID": "index.html#book-chapters",
    "href": "index.html#book-chapters",
    "title": "ng'ara",
    "section": "Book chapters",
    "text": "Book chapters\n\nKimirei, I. A., Igulu, M. M., Semba, M., & Lugendo, B. R. (2016). Small estuarine and non-estuarine mangrove ecosystems of Tanzania: overlooked coastal habitats?. Estuaries: A Lifeline of Ecosystem Services in the Western Indian Ocean, 209-226.\nKimirei, I. A., Semba, M., Mwakosya, C., Mgaya, Y. D., & Mahongo, S. B. (2017). Environmental Changes in the Tanzanian Part of Lake Victoria. Lake Victoria fisheries resources: Research and management in Tanzania, 37-59.\nMahatane, A., Mgaya, Y. D., Hoza, R. B., Onyango, P. O., & Semba, M. (2017). Co-management of Lake Victoria Fisheries. Lake Victoria Fisheries Resources: Research and Management in Tanzania, 219-239.\nSobo, F., Mgaya, Y. D., Kayanda, R. J., & Semba, M. (2017). Fisheries Statistics for Lake Victoria, Tanzania. Lake Victoria Fisheries Resources: Research and Management in Tanzania, 241-253.\nSemba, M., Ndebele-Murisa, M., Mubaya, C. P., Kimirei, I. A., Chavula, G., Mwedzi, T., … & Zenda, S. (2021). Historical and Future Climate Scenarios of the Zambezi River Basin. Ecological Changes in the Zambezi River Basin, 115.\nNdebele-Murisa, M., Kimirei, I. A., Mubaya, C. P., Chavula, G., Mwedzi, T., Mutimukuru-Maravanyika, T., & Semba, M. (2021). A Review of the Comparative Research Method. Ecological Changes in the Zambezi River Basin, 15.\nNdebele-Murisa, M. (Ed.). (2021). Ecological Changes in the Zambezi River Basin. African Books Collective.\nMgaya, Y. D., & Mahongo, S. B. (2017). Lake victoria fisheries resources. Springer."
  },
  {
    "objectID": "index.html#digital-books",
    "href": "index.html#digital-books",
    "title": "ng'ara",
    "section": "Digital books",
    "text": "Digital books\n\nSemba M (2021). Practical Spatial Data Coastal and Marine Environment in R\nSemba M, Peter N. & I. Kimirei (2020). Developing packages in R\nSemba M. (2018). A gentle Introduction of Coding with R\nSemba M., Silas M., Matiku P., & Chande M. (2019) Spatial distribution of octopus fishery in Kilwa District\nSemba M. (2018). Putting fishing landing sites and their instranstructure on spatial aspect\nSemba M. (2023). Fisheries Data Management in Modern R and Excel. Manual with Applications in the Western Indian Ocean Region]. Food and Agriculture Organization and the Western Indian Ocean Marine Science Association"
  },
  {
    "objectID": "index.html#interactive-apps",
    "href": "index.html#interactive-apps",
    "title": "ng'ara",
    "section": "Interactive Apps",
    "text": "Interactive Apps\n\nKwala Commercial and Investment City\nThe Exclusive Economic Zone Tool\nThe Pemba Channel Hydrographic Hub\nThe coastal and Marine Data Visualization tool\nThe National Environmenal Master Plan\nThe Dashboard for Marine Spatial planning"
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "ng'ara",
    "section": "Packages",
    "text": "Packages\nA wior packaged built by Masumbuko Semba and Nyamisi Peter focusing on Easy Tidy and Process Oceanographic Data. The packages is basically developed to help marine and freshwater scientist access a large and varied format of in-situ and satellite data in easy way. In fact the package has made data access in much easy way. The authors are trying to remove the barrier of data access and leave a space for scientists to focus in much deeper thinking of their field rather than spending several days to understand codes for a specific data download. The package contains several tools that allows scientist to get a wide array of datasets. And the funny things is that you get a tidy format result of the download, which is easy to handle in R and also to share it out of R environment. The tidy format is in form that many scientist familiar with Excel spreadsheet will find it handy. You can access this package through this link: https://github.com/lugoga/wior"
  },
  {
    "objectID": "posts/combining_plots/index.html",
    "href": "posts/combining_plots/index.html",
    "title": "Combining plots in R",
    "section": "",
    "text": "The ggplot2 package doesn’t provide a function to arrange multiple plots in a single figure (Wickham 2016). Still, some packages allow combining multiple plots into a single figure with custom layouts, width, and height, such as cowplot (Wilke 2018), gridExtra, and patchwork (Pedersen 2020). In this post we are going to use several packages, let’us load them in our session\nSample datasets"
  },
  {
    "objectID": "posts/combining_plots/index.html#gridextra",
    "href": "posts/combining_plots/index.html#gridextra",
    "title": "Combining plots in R",
    "section": "gridExtra",
    "text": "gridExtra\nThe gridExtra package provides the grid.arrange function to combine several plots on a single figure.\n\ngridExtra::grid.arrange(hist, box, ridges)\n\n\n\n\nWe can also specify the number of rows with nrow, the number of columns with ncol, and the sizes with widths and heights, and also we can add labels at the top, bottom, left, and right of the figures.\n\ngridExtra::grid.arrange(hist, box, ridges, nrow = 2, top = \"Top Panel\", bottom = \"Bottom Panel\")"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html",
    "href": "posts/forecastingTimeseries/index.html",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "",
    "text": "Time-series analysis aims to analyse and learn the temporal behavior of datasets over a period. Examples include the investigation of long-term records of temperature , sea-level fluctuations, the effect of the El Niño/Southern Oscillation on tropical rainfall, and surface current influences on distribution of temperature and rainfall. Th e temporal pattern of a sequence of events in a time series data can be either random, clustered, cyclic, or chaotic.\nTime-series analysis provides various tools with which to detect these temporal patterns. Moreover, it helps in learning the behavior of the dataset by plotting the time series object on the graph. In R language, time series objects are handled easily using ts() function that takes the data vector and converts it into time series object as specified in function parameters. Therefore, understanding the underlying processes that produced the observed data allows us to predict future values of the variable.\nIn this post we learn how to forecast in R. We will use the prophet package (prophet?), which contain all the necessary routines for time-series analysis. Prophet is a package developed by Facebook for forecasting time series objects or data. Prophet package is based on decompose model i.e., trend, seasonality and holidays that helps in making more accurate predictive models. It is much better than the ARIMA model as it helps in tuning and adjusting the input parameters.\n\nrequire(sf)\nrequire(tidyverse)\nrequire(patchwork)\nrequire(magrittr)\nrequire(tmap)\nrequire(prophet)\n\n## basemap shapefile from tmap package\ndata(\"World\")\n\n\n# color codes\nmycolor3 = c(\"#9000B0\", \"#C900B0\", \"#C760AF\", \"#1190F9\", \"#60C8F8\", \"#90C9F8\", \"#F8F8F8\", \"#F8F800\",  \"#F8D730\", \"#f8b030\", \"#f8602f\", \"#f80000\")"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#dataset",
    "href": "posts/forecastingTimeseries/index.html#dataset",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Dataset",
    "text": "Dataset\nWe will use the NASA GISTEMP V4 dataset that combine NOAA GHCN meteorological stations and ERSST ocean temperature to form a comprehensive long record of temperature variability of the earth surface. The dataset contains monthly temperature values from 1880 to present, which is widely used to monitor the weather and climate at regional and global scale. Rather than using absolute temperature values, the dataset uses anomaly obtained by using base period (1951-1980).\nNASA’s analysis incorporates surface temperature measurements from more than 26,000 weather stations and thousands of ship- and buoy-based observations of sea surface temperatures. These raw measurements are analyzed using an algorithm that considers the varied spacing of temperature stations around the globe and urban heating effects that could skew the conclusions if not taken into account. The result of these calculations is an estimate of the global average temperature difference from a baseline period of 1951 to 1980.\nThis dataset is open and free to download as netCDF format file at GISTEMP. I have processed the file and we can load as the csv file here.\n\nglobal = read_csv(\"../data/temperature_lss_global_1990_2020_2021.csv\")\n\nThe Earth’s global average surface temperature in 2021 tied 2018 (See Figure 1) is the sixth-warmest year on record, according to independent analyses from NASA and the National Oceanic and Atmospheric Administration (NOAA). According to scientists at NASA’s Goddard Institute for Space Studies (GISS), global temperatures in 2021 were 0.85 degrees Celsius above the average for NASA’s baseline period,\n\ntemperature = global %>% filter(year == 2021)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 1: Global land and sea surface temperature anomaly for a year 2021 compared to to the 1950-1980 average\n\n\n\n\nRegardless of the COVID-19 pandemic that reduction in mobility and human activities, along with reduced industrial production, has led to lower levels of nitrogen dioxide (NO2) and and the subsequent decrease of fossil fuel burning and CO2 emissions, NASA found that the year 2020 (Figure 2) was the hottest year on record. Continuing the planet’s long-term warming trend, the year’s globally averaged temperature was 1.02 degrees Celsius warmer than the baseline 1951-1980 mean, according to scientists at NASA’s Goddard Institute for Space Studies (GISS) in New York. 2020 edged out 2016 by a very small amount, within the margin of error of the analysis, making the years effectively tied for the warmest year on record. The last seven years have been the warmest seven years on record.\n\ntemperature = global %>% filter(year == 2020)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 2: 2020! the hottest year on record"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "href": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Data Preparation & Exploration",
    "text": "Data Preparation & Exploration\nProphet works best with periodicity data with at least one year of historical data. It’s possible to use Prophet to forecast using sub-daily or monthly data, but for the purposes of this post, we’ll use the monthly periodicity of global temperature—land and ocean . Let’s us read the file into our session\n\nmonthly = read_csv(\"../data/temperature_lss.csv\")\n\nmonthly %>% FSA::headtail(n = 5)\n\n           time temperature\n1    1880-01-15  -0.2322751\n2    1880-02-15  -0.4008110\n3    1880-03-15  -0.1818604\n4    1880-04-15  -0.2003687\n5    1880-05-15  -0.1266611\n1701 2021-09-15   1.1336590\n1702 2021-10-15   1.3624321\n1703 2021-11-15   1.0853070\n1704 2021-12-15   0.9597652\n1705 2022-01-15   1.1144421\n\n\nLooking on the printed dataset, we note that we have records of land and sea temperature since January 1880 through January 2022. That is the long historical data that suits our analysis. The first thing we need to do is to create a time-series object in R. This is done by using a ts function and specify the start year and the frequency of observation. Since we have monthly records, the frequency for each year will be 12 as the chunk highlight\n\nts.temp = monthly %>% \n  pull(temperature) %>% \n  ts(start = c(1880,1), frequency = 12)\n\nBut, although we have that long historical dataset, we are more interested in the most recent records. Therefore, we filter all records since 1980 for our analysis. We can achieve this by simply passing the limiting year and month in the window function. Once we have filtered the dataset, we then convert the ts object to prophet format using a ts_to_prophet function from TSstudio package\n\nts.df = ts.temp %>% \n  window(start = c(1980,1)) %>% \n  TSstudio::ts_to_prophet()\n\nTracking global temperature trends provides a critical indicator of the impact of human activities— specifically, greenhouse gas emissions – on our planet. Visualizing the dataset as seen in Figure 3. It’s an undeniable fact the global mean temperature is constantly rising. Earth’s average temperature has risen above 1.2 degrees Celsius) since the late 19th century and the IPCC has pointed out the increase should be limited to 1.5 °C above pre-industrial levels, to have any hope of mitigating the harmful effects of climate change.\n\nts.df %>% \n  ggplot(aes(x = ds, y = y))+\n  geom_line()+\n  geom_smooth(fill = \"red\", color = \"red\", alpha = .2)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nFigure 3: Trend of global temperature"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#forecasting",
    "href": "posts/forecastingTimeseries/index.html#forecasting",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Forecasting",
    "text": "Forecasting\nThe first step in creating a forecast using Prophet is importing the prophet library into our R session. Once the prophet library is loaded into our session, we’re ready to fit a model to our historical data. We can achieve that by simply calling theprophet() function using your prepared dataframe as an input:\n\nm = ts.df %>% prophet()\n\nOnce we have used Prophet to fit the model dataset, we can now start making predictions for future dates. Prophet has a built-in helper function make_future_dataframe to create a dataframe of future dates. The make_future_dataframe function, which allows to specify the frequency and number of periods we would like to forecast into the future. By default, the frequency is set to days. Since we are using daily periodicity data in this example, we will leave freq at it’s default and set the periods argument to 365, indicating that we would like to forecast 365 days into the future.\n\nfuture = m %>% \n  make_future_dataframe(120,freq = \"month\")\n\nWe can now use the predict() function to make predictions for each row in the future dataframe.\n\nforecast = m %>% predict(future)\n\nAt this point, Prophet will have created a new dataframe assigned to the forecast variable that contains the forecasted values for future dates under a column called yhat, as well as uncertainty intervals and components for the forecast. We can visualize the forecast using Prophet’s built-in plot helper function:\n\nplot(x = m, fcst = forecast, uncertainty = T,plot_cap = T,)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  # scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank())+\n  add_changepoints_to_plot(m = m, threshold = 0.1)\n\n\n\n\nFigure 4: Forecasted sea surface temperature\n\n\n\n\nIf we want to visualize the individual forecast components, we can use Prophet’s built-in plot_components function:\n\nplot_forecast_component(m = m, fcst = forecast, name = \"trend\")\n\n\n\n\nFigure 5: Visualize the trend"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#cited-references",
    "href": "posts/forecastingTimeseries/index.html#cited-references",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Cited references",
    "text": "Cited references"
  },
  {
    "objectID": "posts/inset_main_map/index.html",
    "href": "posts/inset_main_map/index.html",
    "title": "Main and Inset maps with R",
    "section": "",
    "text": "In this post, We learn how we can make publication quality inset maps in R using ggplot2 package (Wickham 2016). When publishing scientific research in journals or presenting research work at a conference, showing maps of data collection sites or experimental locations is one of the key visualization elements. Maps of study sites or sampling locations can help the audience and readers to fathom the data in a better way. Mapping sounds fancy, but it needs substantial training and skill set to make high-quality maps that are reader-friendly and visually aesthetic.\nSometimes, the study sites are more dispersed and are easy to visualize in large geographic areas. However, in some cases, study sites are clustered, which makes it hard to show them on a broader scale. In that case, inset maps help us show the locations with reference to familiar geographical regions. An inset map is a smaller map featured on the same page as the main map. Traditionally, inset maps are shown at a larger scale (smaller area) than the main map. Often, an inset map is used as a locator map that shows the area of the main map in a broader, more familiar geographical frame of reference."
  },
  {
    "objectID": "posts/inset_main_map/index.html#focus-map",
    "href": "posts/inset_main_map/index.html#focus-map",
    "title": "Main and Inset maps with R",
    "section": "Focus Map",
    "text": "Focus Map\nNow, I’ll plot a focused map of Madagascar. We need to define the geographical extent of the area. For that purpose, we first need to identify the extent of the study sites and we can do that using extent function from **sf package;\n\nmadagascar %>% st_bbox()\n\n     xmin      ymin      xmax      ymax \n 42.71862 -25.60895  50.48378 -11.94543 \n\n\nThe printed results indicates tha maxima and minima of longitude and latitude, which define the geographical extent of the area. Using the min and max values of coordinates from the previous map, we can draw a polygon over the study sites and see if this extent can best visualize the data.\n\nmap.site = ggplot() +\n  ggspatial::layer_spatial(data = madagascar, fill = \"cyan4\", color = \"black\",size = .4)+\n  geom_sf(data = sampling.points, color = \"red\", size = 2)+\n  ggsci::scale_color_lancet()+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  coord_sf(xlim = c(43, 51), ylim = c(-18,-11))\n\nmap.site\n\n\n\n\nSampling points in the coastal waters of Madagascar Island\n\n\n\n\nAs you can see, the study sites are located on the northern part of Madagascar Island. However, to make a better sense of the study locations with reference to WIO region, we need to plot them on a focused scale."
  },
  {
    "objectID": "posts/inset_main_map/index.html#add-map-elements",
    "href": "posts/inset_main_map/index.html#add-map-elements",
    "title": "Main and Inset maps with R",
    "section": "Add Map elements",
    "text": "Add Map elements\nProfessional maps also include some elements like North Arrow and scale etc. We’ll add these components to our map as well. Besides that, I’ll also fill the non-land area with lightblue color for reference and distinction respectively Figure 1.\n\nmap.site = map.site +\n      guides(size = \"none\") +\n  ggspatial::annotation_north_arrow(location = \"tr\", \n                                    height = unit(1.2, \"cm\"),  \n                                    width = unit(.75, \"cm\"))+\n  ggspatial::annotation_scale(location = \"br\")\n\nmap.site\n\n\n\n\nFigure 1: Sampling points in the coastal waters of Madagascar Island"
  },
  {
    "objectID": "posts/inset_main_map/index.html#inset-map",
    "href": "posts/inset_main_map/index.html#inset-map",
    "title": "Main and Inset maps with R",
    "section": "Inset Map",
    "text": "Inset Map\nNow, I’ll create a full-scale map of WIO region with a red polygon showing the extent of study sites and the focused map. The code below produce Figure 2;\n\ninset.map = ggplot() +\n  ggspatial::layer_spatial(data = africa, fill = \"grey90\", color = \"grey90\") +\n  ggspatial::layer_spatial(data = wio, fill = \"grey60\", color = \"ivory\",size = .4)+\n  # geom_sf_text(data = wio, aes(label = country))+\n  ggrepel::geom_text_repel(data = wio.point.country, \n                           aes(x = lon, y = lat, label = country), size = 3)+\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  geom_rect(aes(xmin = 43, xmax = 51, ymin = -18, ymax = -11), \n            color = \"red\", fill = NA, size = 1.2)+\n  coord_sf(xlim = c(20, 60), ylim = c(-40,15))+\n  theme_test() + \n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.title=element_blank(),\n        plot.margin = margin(0, 0, 0, 0, \"cm\"),\n        panel.background = element_rect(fill = \"lightblue\"))\n\ninset.map\n\n\n\n\nFigure 2: An inset map of the WIO region\n\n\n\n\nThis version looks better compared to the previous one. However, we need to add some information to give it a reference."
  },
  {
    "objectID": "posts/inset_main_map/index.html#final-map",
    "href": "posts/inset_main_map/index.html#final-map",
    "title": "Main and Inset maps with R",
    "section": "Final Map",
    "text": "Final Map\nNow, to combine both maps where the map of WIO region is inset on the upper left corner we use the function from cowplot package. a draw_plot function allow to places a plot somewhere onto the drawing canvas that is established using ggdraw function also from cowplot package (Wilke 2018). By default, coordinates run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. The function also allows us to specify the size of the inset map using the width and height functions.\n\ncowplot::ggdraw(plot = map.site) +\n  cowplot::draw_plot(inset.map, x = .1, y = .13, width = .4, height = .5)\n\n\n\n\nFigure 3: Map of the northwest side of Madagascar. An inset map indicate the location of the study area in the Western Indian Ocean region\n\n\n\n\n\nSummary\nTherefore , a final map shown in Figure 3 shows the locations of study sites with reference to the country and provinces and is more professional."
  },
  {
    "objectID": "posts/inset_main_map/index.html#last-updated",
    "href": "posts/inset_main_map/index.html#last-updated",
    "title": "Main and Inset maps with R",
    "section": "Last updated",
    "text": "Last updated\n\n\n\n[1] \"16 February 2023\""
  },
  {
    "objectID": "posts/newvisuals/index.html",
    "href": "posts/newvisuals/index.html",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "",
    "text": "On April, 2022, the government of the United Republic of Tanzania approved permission of TAMISEMI to recruit and employ 9,800 primary and secondary school teachers, and 7,612 health experts. A total of 165,948 applied for the positions where for Health Cadres is 42,558, and the Teaching Cadre is 123,390.\nAllocation of health and Teaching position in Centers of providing health services and schools have considered the requirements of employees in the respective regions. The allocation of new employees was based on the needs of employees in Councils with new Hospitals, Centers New health, and new completed clinics that faced shortage of medical staffs.\nIn addition, teachers were allocated to councils based on the division of space for each subject, and the qualifications. In this post, we are going to discuss"
  },
  {
    "objectID": "posts/newvisuals/index.html#tilemap",
    "href": "posts/newvisuals/index.html#tilemap",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Tilemap",
    "text": "Tilemap\n\ndistrict.tb %>% \n  hchart(type = \"tilemap\", hcaes(x = lon, y = lat, name = district, group = zone)) %>% \n  hc_chart(type = \"tilemap\") %>% \n  hc_plotOptions(\n    series = list(\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        color = \"white\",\n        style = list(textOutline = FALSE)\n      )\n    )\n  ) %>% \n  hc_tooltip(\n    headerFormat = \"\",\n    pointFormat = \"<b>{point.name}</b> is in <b>{point.region_nam}</b>\"\n    ) %>% \n  hc_xAxis(visible = FALSE) %>% \n  hc_yAxis(visible = FALSE) %>% \n  hc_size(height = 800, width = 600)"
  },
  {
    "objectID": "posts/newvisuals/index.html#packedbubble",
    "href": "posts/newvisuals/index.html#packedbubble",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "packedbubble",
    "text": "packedbubble\nA bubble chart requires three dimensions of data; the x-value and y-value to position the bubble along the value axes and a third value for its volume. Packed Bubble charts have a simpler data structure, a flat, one-dimensional array with volumes is sufficient. The bubble’s x/y position is automatically calculated using an algorithm that packs the bubbles in a cluster. The series data point configuration has support for setting colors and label values. Drag’n drop feature was also added to give the user a chance to quickly move one bubble between series and then check how their relations will change.\n\n\n\n\nwalimu.lga = walimu.clean %>% \n  separate(halmashauri, into = c(\"district\", \"b\", \"c\"), sep = \" \") %>% \n  unite(col = code, b:c, sep = \" \") %>% \n  mutate(lga = case_when(code == \"District Council\"~\"DC\",\n                         code == \"Municipal Council\"~\"MC\",\n                         code == \"City Council\"~\"CC\",\n                         code == \"Town Council\"~\"TC\",\n                         code == \"Mikindani Municipal\"~\"MC\",\n                         code == \"Ujiji Municipal\"~\"MC\"))\n\n  \nwalimu.lga.freq = walimu.lga %>% \n  group_by(district, lga) %>% \n  count()\n\ndistrict.walimu = district.tb %>% \n  left_join(walimu.lga.freq) %>% \n  select(region_nam, zone, n, district)%>% \n  separate(district, into = c(\"code\", \"aa\"), sep = 3, remove = FALSE) %>% \n  mutate(code = str_to_upper(code)) %>% \n  select(-aa)\n\n\nhc = district.walimu %>% \n   hchart(type = \"packedbubble\", hcaes(name = district, value = n, group = zone))\n\n\n\nq95 <- as.numeric(quantile(district.walimu$n, .95, na.rm = TRUE))\n\nhc %>% \n  hc_tooltip(\n    useHTML = TRUE,\n    pointFormat = \"<b>{point.name}:</b> {point.n}\"\n  ) %>% \n  hc_plotOptions(\n    packedbubble = list(\n      maxSize = \"150%\",\n      zMin = 0,\n      layoutAlgorithm = list(\n        gravitationalConstant =  0.05,\n        splitSeries =  TRUE, # TRUE to group points\n        seriesInteraction = TRUE,\n        dragBetweenSeries = TRUE,\n        parentNodeLimit = TRUE\n      ),\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        filter = list(\n          property = \"y\",\n          operator = \">\",\n          value = q95\n        ),\n        style = list(\n          color = \"black\",\n          textOutline = \"none\",\n          fontWeight = \"normal\"\n        )\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/newvisuals/index.html#sankey",
    "href": "posts/newvisuals/index.html#sankey",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Sankey",
    "text": "Sankey\nA sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called nodes and the connections are called links.Sankey diagrams can also visualize the energy accounts, material flow accounts on a regional or national level, and cost breakdowns.[1] The diagrams are often used in the visualization of material flow analysis.\nSankey diagrams emphasize the major transfers or flows within a system. They help locate the most important contributions to a flow. They often show conserved quantities within defined system boundaries.\n\n   quest.tb =  walimu.clean %>% \n      group_by(kiwango_cha_elimu, jinsi) %>% \n      summarise(value = n(), .groups = \"drop\") %>% \n      rename(source = 2, target = 1) %>% \n      filter(value > 100)%>% \n      as.data.frame()\n        \n    \n    # From these flows we need to create a node data frame: it lists every entities involved in the flow\n    nodes <- data.frame(name=c(as.character(quest.tb$source), \n                               as.character(quest.tb$target)) %>% \n                          unique())\n    \n    nodes = quest.tb %>% \n      select(-value) %>% \n      pivot_longer(cols = source:target) %>% \n      distinct(value) %>% \n      rename(name = 1) %>% \n      as.data.frame()\n    \n    # With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.\n    quest.tb$IDsource=match(quest.tb$source, nodes$name)-1 \n    quest.tb$IDtarget=match(quest.tb$target, nodes$name)-1\n    \n    \n    # Make the Network \n    networkD3::sankeyNetwork(Links = quest.tb, \n                             Nodes = nodes,\n                             Source = \"IDsource\", \n                             Target = \"IDtarget\",\n                             Value = \"value\", \n                             NodeID = \"name\", \n                             fontFamily = \"Myriad Pro\",\n                             LinkGroup = \"source\",\n                             sinksRight=FALSE,\n                             # height = 600, width = 800,\n                             # colourScale=ColourScal,\n                             nodeWidth=30, \n                             iterations = 5,\n                             fontSize=14, \n                             nodePadding=30, \n                             width = 1000, \n                             height = 400)"
  },
  {
    "objectID": "posts/newvisuals/index.html#cited-references",
    "href": "posts/newvisuals/index.html#cited-references",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Cited references",
    "text": "Cited references"
  },
  {
    "objectID": "posts/r_python/index.html",
    "href": "posts/r_python/index.html",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "",
    "text": "If you work with data science, R and Python must be the two programming languages that you use the most. Both R and Python are quite robust languages and either one of them is actually sufficient to carry out the data analysis task. However, instead of considering them as tools that supplement each other, more often you will find people dealing with data claim one language to be better than the other. Truth be told, R and Python are excellent tools in ther own right but are often conceived as rivals. One major reason for such view lies on the experts. Because data analysts have divided the data science field into camps based on the choice of the programming language they are familiar with.\nThere major two camps—R camp and Python camp—and history is the testimony that camps can not live in harmony. Members of both camps believe that their choice of language . Honestly, I do not hold to their opinion, but rather wish I have skills for both languages. So, whether you have in R or Python camp, one thing you will notice is that the problem we have in data science is simply that divergence does not lie with the tools but with the people using those tools.\nI believe there are few people in the Data Science community who use both R and Python in their analytical workflow. But majority are committed to only one programming language, but wish they had access to some functions from other language. Therefore, there is no reason that hold us to stick using this programming language or the other. Our ultimate goal should be to do better analytics and derive better insights and choice of which programming language to use should not hinder us from reaching our goals.\nThe questions that always resolute in my mind is whether can we utilize the statistical power of R along with the programming capabilities of Python?. Its undeniable truth that there are definitely some high and low points for both languages and if we can utilize the strength of both, we can end up dong a much better job. Thanks to Kevin Ushey and his colleges (2020) for developing a reticulate package. reticulate package provides a comprehensive set of tools that allows to work with R and Python in the same environment. The reticulate package provide the following facilities;\n\nCalling Python from R in a variety of ways including rmarkdown, sourcing, Python scripts, importing Python modules and using Python interactively within and R session.\nTranslation between R and Python objects—for example r_to_py function allows to construct R to Pandas data frame and py_to_r() function convert python object like data frame, matrix and etc to R\nFlexible binding to different versions of Python including virtual environments and conda environment."
  },
  {
    "objectID": "posts/r_python/index.html#tibble-to-pandas-dataframe",
    "href": "posts/r_python/index.html#tibble-to-pandas-dataframe",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Tibble to Pandas Dataframe",
    "text": "Tibble to Pandas Dataframe\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We first need to import the dataset from the package where is stored into the R session. let us load the packages that we are glint to use in this post.\n\nrequire(tidyverse)\nrequire(reticulate)\n\nOnce we have loaded the package, we then import the dataset.\n\npeng = palmerpenguins::penguins\npeng\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nThe printed result suggest that this dataset is a tibble format—a modern data frame from the tidyverse ecosystem (Wickham and Wickham 2017). Let’s visualize the dataset with pair plot in GGally package (Schloerke et al. 2020)\n\npeng %>% \n  filter(!is.na(sex)) %>% \n  GGally::ggpairs(columns = 3:6, aes(color = sex))\n\n\n\n\nFigure 1: Matrix of numerical variable in the penguins dataset\n\n\n\n\nHowever, our interest in this post is plotting this dataset using python. Therefore, we need to first import three key libraries that we will use throughtout this post. The chunk below highlight these packages and how to import them inside the python chunk.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nOnce the tibble file is in the environment, we need to convert from tibble data frame into pandas dataframe. Make a copy of pandas dataframe from tible with the r. function\n\n\n\n\n\n\nInfo\n\n\n\nnote that conversion from tibble to pandas data frame must be done in the Python chunk and not R chunk\n\n\n\npeng = r.peng\npeng\n\n       species     island  bill_length_mm  ...  body_mass_g     sex  year\n0       Adelie  Torgersen            39.1  ...         3750    male  2007\n1       Adelie  Torgersen            39.5  ...         3800  female  2007\n2       Adelie  Torgersen            40.3  ...         3250  female  2007\n3       Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4       Adelie  Torgersen            36.7  ...         3450  female  2007\n..         ...        ...             ...  ...          ...     ...   ...\n339  Chinstrap      Dream            55.8  ...         4000    male  2009\n340  Chinstrap      Dream            43.5  ...         3400  female  2009\n341  Chinstrap      Dream            49.6  ...         3775    male  2009\n342  Chinstrap      Dream            50.8  ...         4100    male  2009\n343  Chinstrap      Dream            50.2  ...         3775  female  2009\n\n[344 rows x 8 columns]"
  },
  {
    "objectID": "posts/r_python/index.html#plotting",
    "href": "posts/r_python/index.html#plotting",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Plotting",
    "text": "Plotting\n\nPairplot\n\nfig = plt.figure()\nsns.pairplot(data = peng, hue = \"species\")\n\n\n\n\nFigure 2: The pairplot of penguins dataset\n\n\n\nplt.show()\n\n\n\n\nFigure 3: The pairplot of penguins dataset\n\n\n\n\n\n\nScatter plot\n\n\nfig = plt.figure()\n\nsns.scatterplot(\n  data = peng, \n  x = \"bill_length_mm\", \n  y = \"bill_depth_mm\", \n  hue = \"island\"\n  )\n  \n\nplt.xlabel(\"Length (mm)\")\nplt.ylabel(\"Depth (mm)\")\nplt.legend(loc = \"lower right\")\nplt.show()\n\n\n\n\nFigure 4: Scatterplot of length and depth of penguins\n\n\n\n\n\n\nHistogram\n\nfig = plt.figure()\nsns.histplot(data = peng, x = \"bill_depth_mm\", color = \"steelblue\")\nplt.xlabel(\"Bill depth (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nFigure 5: Histogram of bill depth\n\n\n\n\n\nfig = plt.figure()\nsns.histplot(data = peng[peng.island == \"Dream\"], x = \"bill_depth_mm\", color = \"steelblue\", label = \"Dream\")\nsns.histplot(data = peng[peng.island == \"Biscoe\"], x = \"bill_depth_mm\", color = \"darkorchid\", label = \"Biscoe\")\nsns.histplot(data = peng[peng.island == \"Torgersen\"], x = \"bill_depth_mm\", color = \"lightblue\", label = \"Torgersen\")\nplt.xlabel(\"Bill depth (mm)\")\nplt.ylabel(\"Frequency\")\nplt.legend(loc = \"upper right\")\nplt.show()\n\n\n\n\nFigure 6: Histogram of bill depth\n\n\n\n\n##3 Density\n\nfig = plt.figure()\nsns.kdeplot(data = peng, x = \"bill_length_mm\", shade = \"steelblue\")\nplt.axvline(43.40, color=\"k\", linestyle=\"--\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 7: Density distribution of of bill depth\n\n\n\n\nThe difference of bill length among the three species is interesting. Let’s look at the density plots of these species:\n\nfig = plt.figure()\nsns.kdeplot(data = peng[peng.species == \"Adelie\"], x = \"bill_length_mm\", label = \"Adelie\", shade = \"steelblue\")\nsns.kdeplot(data = peng[peng.species == \"Chinstrap\"], x = \"bill_length_mm\", label = \"Chinstrap\", shade = \"orange\")\nsns.kdeplot(data = peng[peng.species == \"Gentoo\"], x = \"bill_length_mm\", label = \"Gentoo\", shade = \"green\")\nplt.legend(loc = \"upper right\")\nplt.xlabel(\"Bill length (mm)\")\n\nplt.show()\n\n\n\n\nFigure 8: Density plot of bill length by species\n\n\n\n\n\n\nBoxplot\n\n\nfig = plt.figure()\nsns.boxplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 9: Boxplot of bill depth by island and species\n\n\n\n\n\n\nfig = plt.figure()\nsns.violinplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 10: Violin plots of bill depth by island and species\n\n\n\n\n\n\nfig = plt.figure()\nsns.violinplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nsns.boxplot(data = peng, x = \"island\", y = \"bill_depth_mm\", hue = \"species\")\nplt.xlabel(\"\")\nplt.ylabel(\"Bill depth (mm)\")\nplt.legend(loc = \"lower right\")\n# plt.gca().legend_.remove() # uncomment to remove legend\nplt.show()\n\n\n\n\nFigure 11: Violin and Boxplot of bill depth by island and species"
  },
  {
    "objectID": "posts/r_python/index.html#pandas-dataframe-to-tibble",
    "href": "posts/r_python/index.html#pandas-dataframe-to-tibble",
    "title": "Multi-lingual: R and Python for Data Science",
    "section": "Pandas Dataframe to Tibble",
    "text": "Pandas Dataframe to Tibble\nThe power of multilingual is clearly demonstrated with Rstudio, which allows you to swap dataset between R and python. In the previous section we created a peng dataset in python from R. In this session we are going to use this python dataset and convert it back to R. A py function from reticulate package is used as the chunk below illustrates:\n\npeng.r = reticulate::py$peng\npeng.r %>% as_tibble()\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <dbl>   <dbl> <fct> <dbl>\n 1 Adelie  Torgersen           39.1          18.7     1.81e2  3.75e3 male   2007\n 2 Adelie  Torgersen           39.5          17.4     1.86e2  3.8 e3 fema~  2007\n 3 Adelie  Torgersen           40.3          18       1.95e2  3.25e3 fema~  2007\n 4 Adelie  Torgersen           NA            NA      -2.15e9 -2.15e9 <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3     1.93e2  3.45e3 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6     1.9 e2  3.65e3 male   2007\n 7 Adelie  Torgersen           38.9          17.8     1.81e2  3.62e3 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6     1.95e2  4.68e3 male   2007\n 9 Adelie  Torgersen           34.1          18.1     1.93e2  3.48e3 <NA>   2007\n10 Adelie  Torgersen           42            20.2     1.9 e2  4.25e3 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\n\n\n\n\n\n\nInfo\n\n\n\nnote that conversion from pandas to tibble data frame must be done in the R chunk and not Python chunk"
  },
  {
    "objectID": "posts/satelliteData/index.html",
    "href": "posts/satelliteData/index.html",
    "title": "Getting and Processing Satellite Data Made Easier in R",
    "section": "",
    "text": "The amount of data being generated by satellites has soared in recent years. The proliferation of remote sensing data can be explained by recent advancements in satellite technologies. However, according to Febvre1, this advancement set another challenge of handling and processing pentabyte of data satellite generates. Thanks to ERDDAP, for being in front-line to overcome this challenge. We can obtain satellite data within the geographical extent of the area we are interested. The ERDDAP server provides simple and consistent way to subset and download oceanographic datasets from satellites and buoys to your area of interest. ERDDAP is providing free public access to huge amounts of environmental datasets. Current the ERDDAP server has a list of 1385 datasets.This server allows scientist to request data of a specific area of interest in two forms—grids or tabular data.\nxtractomatic is R package developed by Roy Mendelssohn2 that works with ERDDAP servers. The xtractomatic functions were originally developed for the marine science, to match up ship track and tagged animals with satellites data. Some of the satellite data includes sea-surface temperature, sea-surface chlorophyll, sea-surface height, sea-surface salinity, and wind vector. However, the package has been expanded and it can now handle gridded environmental satellite data from ERDAPP server.\nIn this post I will show two main routine operations extractomatic package can do. The first one is to match up drifter observation data with modis sea surface temperature satellite data using xtracto function. The second operation involves xtracto_3D to extract gridded data of sea surface temperature and chlorophyll a, both acquired by MODIS sensors."
  },
  {
    "objectID": "posts/seabornjointplot/index.html",
    "href": "posts/seabornjointplot/index.html",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going focus on jointplot. jointplot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. Seaborn has nifty function called jointplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seabornjointplot/index.html#loading-libraries",
    "href": "posts/seabornjointplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seabornjointplot/index.html#dataset",
    "href": "posts/seabornjointplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: Joint plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year.\nThe joint plot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. To plot a joint plot, you need to call the jointplot() function. The following script plots a joint plot for bill_length_mm and bill_depth_mm columns of the df dataset.\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005A814D00>\n\nplt.show()\n\n\n\n\nAssigning a hue variable will add conditional colors to the scatter plot and draw separate density curves (using kdeplot()) on the marginal axes. In this case we specify hue = \"island\"\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue = \"island\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D55D160>\n\nplt.show()\n\n\n\n\nSeveral different approaches to plotting are available through the kind parameter. Setting kind=“kde” will draw both bivariate and univariate KDEs:\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", kind=\"kde\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D707310>\n\nplt.show()\n\n\n\n\nSet kind=\"reg\" to add a linear regression fit (using regplot()) and univariate KDE curves:\n\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"reg\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005D8163D0>\n\nplt.show()\n\n\n\n\n\n\nfig = plt.figure()\nsns.jointplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"hex\")\n\n<seaborn.axisgrid.JointGrid object at 0x000000005DC64610>\n\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html",
    "href": "posts/seabornVisualization/index.html",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "",
    "text": "Wikipedia (2023) describe data visualization as an interdisciplinary field that deals with the graphic representation of data and information. It is a particularly efficient way of communicating when the data are processed to generate information that is shared.\nIt is also the study of visual representations of abstract data to reinforce human cognition using common graphics, such as charts, plots, infographics, maps, and even animations. The abstract data include both numerical and non-numerical data, such as text and geographic information.\nFurthermore, it is related to infographics and scientific visualization to identify important patterns in the data that can be used for organizational decision making. Visualizing data graphically can reveal trends that otherwise may remain hidden from the naked eye.\nIn the following is the series of post that focuse plotting with seaborn library in Python, we will learn the most commonly used plots using Seaborn library in Python (Waskom 2021; Bisong and Bisong 2019). We will also touches on different types of plots using Maplotlib (Bisong and Bisong 2019), and Pandas (Betancourt et al. 2019) libraries. In this post we will focus on the distplot."
  },
  {
    "objectID": "posts/seabornVisualization/index.html#loading-libraries",
    "href": "posts/seabornVisualization/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#dataset",
    "href": "posts/seabornVisualization/index.html#dataset",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We first need to import the dataset from the package where is stored into the R session. let us load the packages that we are glint to use in this post.\n\npengr = palmerpenguins::penguins\npengr\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nOnce the tibble file is in the environment, we need to convert from tibble data frame into pandas dataframe. Make a copy of pandas dataframe from tibble with the r. function. please note that the conversion of tibble data frame to pandas data frame must be inside the Python chunk as chunk below;\n\npengp = r.pengr\n\nLet’s use head function to explore the first five rows on the converted penguin pandas data frame\n\npengp.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nThe pengp dataset comprise various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains eight variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year. You do not need to download this dataset as it comes with the palmerpenguin library in R. We will use this dataset to plot some of the seaborn plots. Lets begin plotting\nAlternatively, you can load the package as\n\n\ndf = sns.load_dataset(\"penguins\")\ndf.head()\n\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#univariable-distribution",
    "href": "posts/seabornVisualization/index.html#univariable-distribution",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Univariable distribution",
    "text": "Univariable distribution\nThe distplot, also commonly refers as the distribution plot, is widely used to plot a histogram of data for a specific variable in a dataset. To make this plot seaborn has a dedicated function called displot\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nThe new displot functions support the kernel density estimate line, by passing kde=True\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kde = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nTo change the distribution from counts to density, we simply parse an argument stat=\"density\"\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kde = True, stat = \"density\")\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#kdeplot",
    "href": "posts/seabornVisualization/index.html#kdeplot",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "kdeplot",
    "text": "kdeplot\nWhen you want to draw the density plot alone without overlay it to the histogram as presented using the displot function, seaboarn has a kdeplot function\n\n\nfig = plt.figure()\nsns.kdeplot(pengp.bill_length_mm)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\ndisplot still can draw the kde plot, however, you need to parse an argument kind=\"kde\" in displot:\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kind = \"kde\", rug = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nIf you parse rug = True function, wll add the rug in the plots\n\n\nfig = plt.figure()\nsns.displot(pengp.bill_length_mm, kind = \"kde\", rug = True)\n\n\n\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\naa = pengp[[\"bill_length_mm\", \"bill_depth_mm\"]]\n\nfig = plt.figure()\nsns.kdeplot(data = aa)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nPlot conditional distributions with hue mapping of a second variable. Unlike the previous plot, for this kind you need to specify the x-variable and the hue in the dataset;\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nStack the conditional distributions by simply parsing argument multiple = \"stack\"\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\", multiple = \"stack\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nmultiple = \"fill\" simply normalize the stacked distribution at each value in the grid\n\n\nfig = plt.figure()\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\", multiple = \"fill\")\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nEstimate the cumulative distribution function(s), normalizing each subset:\n\n\nfig = plt.figure()\n\nsns.kdeplot(data = pengp, x = \"bill_length_mm\", hue = \"species\",  cumulative=True, common_norm=False, common_grid=True)\nplt.xlabel(\"Bill length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()"
  },
  {
    "objectID": "posts/seabornVisualization/index.html#bivariate-distribution",
    "href": "posts/seabornVisualization/index.html#bivariate-distribution",
    "title": "plotting in Python with Seaborn: Distribution plot",
    "section": "Bivariate distribution",
    "text": "Bivariate distribution\nFor bivariates, we are going to use geyser dataset. Old Faithful is a cone geyser in Yellowstone National Park in Wyoming, United States. It is a highly predictable geothermal feature and has erupted every 44 minutes to two hours since 2000. We do not need to download this dataset as it comes with the seaborn package.\n\ngeyser = sns.load_dataset(\"geyser\")\ngeyser.head()\n\n   duration  waiting   kind\n0     3.600       79   long\n1     1.800       54  short\n2     3.333       74   long\n3     2.283       62  short\n4     4.533       85   long\n\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\")\nplt.show()\n\n\n\n\nMap a third variable with a hue semantic to show conditional distributions:\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\")\nplt.show()\n\n\n\n\nFill the contour by parsing fill = True\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\", fill = True)\nplt.show()\n\n\n\n\nShow fewer contour levels, covering less of the distribution by parsing a levels and thresh functions in the kdeplot:\n\n\nfig = plt.figure()\nsns.kdeplot(data=geyser, x=\"waiting\", y=\"duration\", hue = \"kind\", levels = 5, thresh = .2)\nplt.show()"
  },
  {
    "objectID": "posts/seaborn_barplot/index.html",
    "href": "posts/seaborn_barplot/index.html",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going learn barplot. The bar plot is used to capture the relationship between a categorical and numerical column. For each unique value in a categorical column, a bar is plotted, which by default, displays the mean value for the data in a numeric column specified by the bar plot. Seaborn has nifty function called barplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seaborn_barplot/index.html#loading-libraries",
    "href": "posts/seaborn_barplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_barplot/index.html#dataset",
    "href": "posts/seaborn_barplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: bar plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year.\nNext, we will call the barplot() function from the Seaborn library to plot a bar plot that displays the average length of penguin species.\n\nfig = plt.figure()\nsns.barplot(data=df, x=\"species\", y=\"bill_length_mm\")\nplt.show()\n\n\n\n\nFigure 1: Average length by island\n\n\n\n\nThe Figure 1 shows that the average length of penguins from the three island. We can parse an argument hue = \"sex\" to stack the plot as Figure 2 shows.\n\n\nfig = plt.figure()\nsns.barplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.show()\n\n\n\n\nFigure 2: Average length by species and island"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html",
    "href": "posts/seaborn_boxplot/index.html",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "",
    "text": "In Visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going learn how to plot boxplot in seaborn. The boxplot is used to plot the quantile information of numeric variables in a dataset. To plot a box plot, the boxplot() function is used. To plot a horizontal boxplot, the variable name of the dataset is passed to the x-axis."
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#loading-libraries",
    "href": "posts/seaborn_boxplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#dataset",
    "href": "posts/seaborn_boxplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a penguin dataset from palmerpenguins package (Horst, Hill, and Gorman 2020). We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset(\"penguins\")\n\n\ndf.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...         3750    male  2007\n1  Adelie  Torgersen            39.5  ...         3800  female  2007\n2  Adelie  Torgersen            40.3  ...         3250  female  2007\n3  Adelie  Torgersen             NaN  ...  -2147483648     NaN  2007\n4  Adelie  Torgersen            36.7  ...         3450  female  2007\n\n[5 rows x 8 columns]\n\n\nA printed df dataset shows that is made up of various measurements of three different penguin species — Adelie, Gentoo, and Chinstrap. The dataset contains seven variables – species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year."
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#boxplot",
    "href": "posts/seaborn_boxplot/index.html#boxplot",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Boxplot",
    "text": "Boxplot\nNext, we will call the boxplot() function from the Seaborn library to plot a bar plot that displays the average length of penguin species.\n\nfig = plt.figure()\nsns.boxplot(data=df, x=\"species\", y=\"bill_length_mm\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 1: Boxplot length by species\n\n\n\n\nThe Figure 1 shows that the average length of penguins from the three island. We can parse an argument hue = \"sex\" to stack the plot as Figure 2 shows.\n\n\nfig = plt.figure()\nsns.boxplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 2: Boxplot of length by species and island\n\n\n\n\nDraw a vertical boxplot with nested grouping by two variables:\n\n\nfig = plt.figure()\nsns.boxplot(data=df, y=\"species\", x=\"bill_length_mm\", hue = \"sex\")\nplt.xlabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 3: Boxplot of length by species and island"
  },
  {
    "objectID": "posts/seaborn_boxplot/index.html#violin",
    "href": "posts/seaborn_boxplot/index.html#violin",
    "title": "plotting in Python with Seaborn: box plot",
    "section": "Violin",
    "text": "Violin\nViolin plots are similar to Box plots. However, unlike Box plots that plot quantile information, the Violin plots plot the overall distribution of values in the numeric columns. The following script plots two Violin plots for the passengers traveling alone and for the passengers traveling along with another passenger. The violinplot() function is used to plot a swarm plot with Seaborn.\n\n\nfig = plt.figure()\nsns.violinplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\")\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 4: Violin plot of length by species and island\n\n\n\n\nWe can split the violin plot with split = True argument. One key advantage of splited violins is that take up less space (Figure 5):\n\n\nfig = plt.figure()\nsns.violinplot(data=df, x=\"species\", y=\"bill_length_mm\", hue = \"sex\", \nsplit = True)\nplt.ylabel(\"Bill length (mm)\")\nplt.show()\n\n\n\n\nFigure 5: Violin plot of length by species and island"
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html",
    "href": "posts/seaborn_lineplot/index.html",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "",
    "text": "In visualization with Seaborn of this series, we were introduced on the power visualization and dove into distributions plot. In this post we are going focus on jointplot. jointplot is used to plot the histogram distribution of two columns, one on the x-axis and the other on the y-axis. A scatter plot is by default drawn for the points in the two columns. Seaborn has nifty function called jointplot(), which is dedicated for this type of plot."
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html#loading-libraries",
    "href": "posts/seaborn_lineplot/index.html#loading-libraries",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "Loading libraries",
    "text": "Loading libraries\nThough most people are familiar with plotting using matplot, as it inherited most of the functions from MatLab. Python has an extremely nady library for data visualiztion called seaborn. The Seaborn library is based on the Matplotlib library. Therefore, you will also need to import the Matplotlib library.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/seaborn_lineplot/index.html#dataset",
    "href": "posts/seaborn_lineplot/index.html#dataset",
    "title": "plotting in Python with Seaborn: Line plot",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use a flights dataset, which has 10 years of monthly airline passenger data. We do not need to download this dataset as it comes with the seaborn package. We only need to load it from the package into our session using sns.load_dataset function and specify the penguins as the name of the dataset and assign it as df;\n\n\n\n\n\n\n\ndf = sns.load_dataset('flights')\n\n\ndf\n\n     year month  passengers\n0    1949   Jan         112\n1    1949   Feb         118\n2    1949   Mar         132\n3    1949   Apr         129\n4    1949   May         121\n..    ...   ...         ...\n139  1960   Aug         606\n140  1960   Sep         508\n141  1960   Oct         461\n142  1960   Nov         390\n143  1960   Dec         432\n\n[144 rows x 3 columns]\n\n\nA printed df dataset shows that a flight dataset is made up of three variables — year, month, and number of passenger between January 1949 and December 1960, which are arranged in long format. To draw a line plot using long-form data, we simply assign the x and y variables\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 1: Annual number of flight passenger\n\n\n\n\nHowever, looking at Figure 2, we notice that the confidence level is aslo plotted thought we did not specify them. The reason is that each year has twelve records of monthly number of passengers, which when plotted are also estimated to show the variability of passenger on that year. So if we want to draw only the line, we may filter a specific year, for this case I only want to plot passengers for July during the same period.\nFirst, we need to filter the dataset to July and assign a new dataset as df_july. The chunk below shows a line of code that filter passenger between 1949 to 1960 for July only.\n\ndf_july = df[df.month == 'Jul']\n\nThen plot.\n\nfig = plt.figure()\nsns.lineplot(data=df_july, x=\"year\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 2: Number of flight passenger for July during the period\n\n\n\n\nBecause I still learning how to deal with time in python, I simply switch to R as quarto allows me to swap code for the two languages within the same environment. I switch to R code and then convert the panda data frame to tibble using the reticulate package (Ushey, Allaire, and Tang 2020). The package allows us to convert pandas dataframe to tibble using a py function within R chunk;\n\ntb = reticulate::py$df\n\nThen within R chunk, we can add a new column date contain date for each month\n\ntb = tb |>\n  dplyr::mutate(date = \nseq(lubridate::my(011949), lubridate::my(121960), by = \"month\"))\n\ntb |> head()\n\n  year month passengers       date\n1 1949   Jan        112 1949-01-01\n2 1949   Feb        118 1949-02-01\n3 1949   Mar        132 1949-03-01\n4 1949   Apr        129 1949-04-01\n5 1949   May        121 1949-05-01\n6 1949   Jun        135 1949-06-01\n\n\nThe printed tibble has one added variable date. Since we have created this dataset, we can convert it back to python pandas dataframe by simply using r. funtion within the python chunk;\n\npdf = r.tb\npdf\n\n       year month  passengers        date\n0    1949.0   Jan       112.0  1949-01-01\n1    1949.0   Feb       118.0  1949-02-01\n2    1949.0   Mar       132.0  1949-03-01\n3    1949.0   Apr       129.0  1949-04-01\n4    1949.0   May       121.0  1949-05-01\n..      ...   ...         ...         ...\n139  1960.0   Aug       606.0  1960-08-01\n140  1960.0   Sep       508.0  1960-09-01\n141  1960.0   Oct       461.0  1960-10-01\n142  1960.0   Nov       390.0  1960-11-01\n143  1960.0   Dec       432.0  1960-12-01\n\n[144 rows x 4 columns]\n\n\nThen we can now plot a line and use the new column date we just created instead of year.\n\nfig = plt.figure()\nsns.lineplot(data=pdf, x=\"date\", y=\"passengers\")\nplt.ylabel('Passengers')\nplt.show()\n\n\n\n\nFigure 3: Monthly Number of flight passengers\n\n\n\n\nFigure 3 and Figure 2 are almost similar but while Figure 3 used year as x-axis, Figure 3 used date in the x-axis. You can see that Figure 3 clearly shows the variability of passenger within the year, which was not possible with Figure 2.\nWe can assign a grouping semantic (hue, size, or style) to plot separate lines\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\", hue = \"month\")\nplt.ylabel(\"Passengers\")\nplt.show()\n\n\n\n\nFigure 4: Number of flight passenger by month during the period\n\n\n\n\nSimilarly, we can assign multiple semanti variables of the same variable that makes the plot more appealing ore easily to distinguish between the assigned parameters.\n\nfig = plt.figure()\nsns.lineplot(data=df, x=\"year\", y=\"passengers\", hue = \"month\", style = \"month\")\nplt.ylabel(\"Passengers\")\nplt.show()\n\n\n\n\nFigure 5: Number of flight passenger by month during the period"
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html",
    "href": "posts/simplefeature_pointline/index.html",
    "title": "Manipulating Simple features: point to polyline",
    "section": "",
    "text": "In this post we are going to learn one of the key skills that one dealing with spatial data need to know. That’s is how to read a file that contain geographical information (longitude and latitude) and associated attribute information for each location. In its native tabular form, the geographical information are of no use at all until are converted and presented as spatial information. While industrial leading software like ArcMAP and QGIS provide the tools necessary for that conversion, but these software requires us to click the function every time we want to use them to do the process.\nUnlike ArcMAP and QGIS, R software has tools that allows automate processing and conversion of geographical information into spatial data in an intuitive approach, which offers a possible way to reproduce the same. Before we proceed with our task, we need to load some of the packages whose function are needed for this post. We simply load these packages using require function of R (r-base?). The chunk below load these packages;"
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#dataset",
    "href": "posts/simplefeature_pointline/index.html#dataset",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Dataset",
    "text": "Dataset\nWe are going to use Argo float dataset. The step involved to prepare this dataset are explained in Getting and Processing Satellite Data Made Easier in R Let’s load this dataset. We import this dataset using read_csv function from readr package (Wickham, Hester, and Francois 2017)\n\nargo.surface.tb = read_csv(\"../data/argo_profile_cast_location.csv\") %>% \n  janitor::clean_names()\n\nargo.surface.tb\n\n# A tibble: 217 x 8\n    scan date       time     longitude latitude depth temperature salinity\n   <dbl> <date>     <time>       <dbl>    <dbl> <dbl>       <dbl>    <dbl>\n 1   1.5 2008-10-28 01:43:57      64.0    -21.6     0        23.5     35.4\n 2   1.5 2008-11-07 02:17:07      64.2    -21.3     0        24.6     35.0\n 3   1.5 2008-11-17 02:30:18      64.1    -21.0     0        25.0     35.0\n 4   1.5 2008-11-27 02:37:35      63.6    -21.1     0        25.7     35.0\n 5   1.5 2008-12-07 05:03:59      63.4    -21.3     0        26.1     35.0\n 6   1.5 2008-12-17 03:03:40      63.3    -21.5     0        27.0     35.1\n 7   1.5 2008-12-27 05:09:53      63.5    -21.6     0        26.7     35.1\n 8   1.5 2009-01-06 02:40:37      63.9    -21.6     0        28.4     35.1\n 9   1.5 2009-01-16 04:57:04      64.4    -21.5     0        28.0     35.1\n10   1.5 2009-01-26 02:30:53      64.8    -21.2     0        28.1     35.0\n# ... with 207 more rows\n\n\nThe dataset has 217 rows and eight variables with date and time stamp the argo surfaced and pushed profile data into the satellite after roving the water column after ten days. The geographical location (longitude and latitude) of the float when was at the surface with the associated temperature and salinity at the surface represent with zero depth."
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#simple-feature",
    "href": "posts/simplefeature_pointline/index.html#simple-feature",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Simple feature",
    "text": "Simple feature\nThe main task of this post is to convert this dataset that contain 217 geographical information into simple feature. Wikipedia contributors (2022) describe simple feature as set of standards that specify a common storage and access model of geographic feature made of mostly two-dimensional geometries used by geographic information systems. In general, simple feature is a model of a non-topological way to store geospatial data in a database. The three common types of simple feature are point, lines and polygon, each represent a particular type of geographical feature on earth. Further, simple features refers a formal standard that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\nPebesma (2018) developed a simple feature packages (sf) that is dedicated to deal with simple feature objects in R. The packages has hundred of functions that make dealing with spatial information in R environment much easier than before. Its not the focus of this post to describe the package, but if you wish to dive on this topic you can simply consult book such as geocompuation in R (Lovelace, Nowosad, and Muenchow 2019).\nFor this post we begin by looking on how we can convert tabular data with geographical information into simple feature. That is achieved using st_as_sf function of sf package (Pebesma 2018) and parse the argument coords = c(\"longitude\", \"latitude\") that bind the geographical coordinate and argument the datum crs = 4326 to define the datuma and the geographical coordinate system, which is WGS 1984 (epsg = 4326). The chunk below has code of the above description;\n\nargo.surface.sf = argo.surface.tb %>% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nargo.surface.sf\n\nSimple feature collection with 217 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 39.94 ymin: -22.623 xmax: 65.075 ymax: -1.233\nGeodetic CRS:  WGS 84\n# A tibble: 217 x 7\n    scan date       time     depth temperature salin~1         geometry\n * <dbl> <date>     <time>   <dbl>       <dbl>   <dbl>      <POINT [°]>\n 1   1.5 2008-10-28 01:43:57     0        23.5    35.4  (64.03 -21.644)\n 2   1.5 2008-11-07 02:17:07     0        24.6    35.0  (64.198 -21.28)\n 3   1.5 2008-11-17 02:30:18     0        25.0    35.0  (64.068 -21.01)\n 4   1.5 2008-11-27 02:37:35     0        25.7    35.0 (63.607 -21.102)\n 5   1.5 2008-12-07 05:03:59     0        26.1    35.0 (63.429 -21.253)\n 6   1.5 2008-12-17 03:03:40     0        27.0    35.1 (63.322 -21.458)\n 7   1.5 2008-12-27 05:09:53     0        26.7    35.1 (63.472 -21.558)\n 8   1.5 2009-01-06 02:40:37     0        28.4    35.1 (63.901 -21.599)\n 9   1.5 2009-01-16 04:57:04     0        28.0    35.1 (64.447 -21.493)\n10   1.5 2009-01-26 02:30:53     0        28.1    35.0 (64.835 -21.207)\n# ... with 207 more rows, and abbreviated variable name 1: salinity\n\n\nA printed object is the metadata of the file with key information that tells us that is a simple feature with 217 simple features (recores) and six variables (column) and the type of the object are points. The metadata also display the bounding extent covered with these points and the datum of WGS84. We can check whether the conversion is successful by simply map the simple feature object we created into an interactive map shown in Figure 1.\n\nargo.surface.sf %>% \n  tm_shape() +\n  tm_bubbles(\n    size = \"temperature\", \n    col = \"salinity\", \n    scale = c(0.3,.8),\n    border.col = \"black\", \n    border.alpha = .5, \n    style=\"fixed\", \n    breaks=c(-Inf, seq(34.8, 35.6, by=.2), Inf),\n    palette=\"-RdYlBu\", contrast=1, \n    title.size=\"Temperature\", \n    title.col=\"Salinity (%)\", \n    id=\"Date\",\n    popup.vars=c(\"Temperature: \"=\"temperature\", \"Salinity: \"=\"salinity\"),\n    popup.format=list(temperature=list(digits=2))\n    ) \n\n\n\n\n\nFigure 1: Map with bubble size representing temperature variation and the color is the salinity gradients."
  },
  {
    "objectID": "posts/simplefeature_pointline/index.html#trajectory",
    "href": "posts/simplefeature_pointline/index.html#trajectory",
    "title": "Manipulating Simple features: point to polyline",
    "section": "Trajectory",
    "text": "Trajectory\nLooking on Figure 1 we notice that the points are following a certain. This path is commonly known as trajectory – a path that an Argo float follows through in the Indian Ocean over its lifespan. Therefore, we ought to convert the point simple feature into trajectory. Fortunate, a combination of function from dplyr package (Wickham et al. 2019) and sf (Pebesma 2018) has made it possible. Though we can create a trajectory for the entire lifespan it recorded profiles in the area, but for exploration purpose, I first created a year variable and extract year variable from date variable and then use the year variable to group the information by year and then create trajectories that are grouped by year.\n\nargo_traj = argo.surface.sf %>% \n  dplyr::mutate(year = lubridate::year(date) %>% as.factor()) %>% \n  dplyr::group_by(year) %>% \n  dplyr::summarise() %>% \n  sf::st_cast(to = \"LINESTRING\")\n\ntm_shape(shp = argo_traj)+\n  tm_lines(col = \"year\", lwd = 3, stretch.palette = TRUE)\n\n\n\n\n\nFigure 2: Incorrect trajectories of the Argo float separated by year\n\n\n\nWe notice that Figure 2 trajectories are un ordered and very confusing. In fact the trajectories in Figure 2 do not reflect the pathway of the Argo float. The problem like this arise especially when you forget to parse an argument do_uninon = FALSE in the summarise function. But if we simply parse that argument, as the chunk below highlight, we correct the observed error and create a trajectory that reflect the pathwary of the Argo float shown in Figure 3\n\nargo_traj = argo.surface.sf %>% \n  mutate(year = lubridate::year(date) %>% as.factor()) %>% \n  arrange(date) %>% \n  group_by(year) %>% \n  summarise(do_union = FALSE) %>% \n  st_cast(to = \"LINESTRING\")\n\ntm_shape(shp = argo_traj)+\n  tm_lines(col = \"year\", lwd = 3, stretch.palette = TRUE)\n\n\n\n\n\nFigure 3: Correct trajectories of Argo floats separated by year"
  },
  {
    "objectID": "posts/spatialState/index.html",
    "href": "posts/spatialState/index.html",
    "title": "Spatial Data is Maturing in R",
    "section": "",
    "text": "R is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications\nR is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications. As data becomes more geographical, there is a growing necessity to make spatial data more accessible and easy to process. While there are plenty of tools out there that can make your life much easier when processing spatial data (e.g. QGIS and ArcMap) using R to conduct spatial analysis can be just as easy. This is especially true if you’re new to some of these packages and don’t feel like reading through all of the documentation to learn the package or, even more tedious, writing hundreds of lines of your own code to do something relatively simple. In this article I discuss a few packages that make common spatial statistics methods easy to perform in R (Bivand 2006).\nWe will conduct a high-level assessment of the R packages that are dedicated for spatial analysis. By showing network connection across package dependencies — which packages utilize code from another package to execute a task – we will undertake a high-level assessment of the condition of spatial in R. For comparison, we’ll compare our Analysis of Spatial Data task view to the tidyverse, one of R’s most well-known collections of packages, as well as the venerable Environmetrics task view, which includes numerous environmental analysis tools. To accomplish so, we’ll need to write some R code and install the following packages:\nWe will use the handy CRAN_package_db function from the tools package which conveniently grabs information from the DESCRIPTION file of every package on CRAN and turns it into a dataframe.\nHere we are interested with the package and imports columns, so we will select them and drop the rest from the dataset. Then, we parse clean and tidy the columns in the dataset to make it a little easier to work with:"
  },
  {
    "objectID": "posts/spatialState/index.html#package-connectivity",
    "href": "posts/spatialState/index.html#package-connectivity",
    "title": "Spatial Data is Maturing in R",
    "section": "Package Connectivity",
    "text": "Package Connectivity\nLet’s start with a look at the tidyverse. We can take the unusual step of actually employing a function from the tidyverse package (aptly titled tidyverse_packages), which identifies those packages that are formally part of the tidyverse. To see package connection, we filter for those packages and their imports, convert to tbl_graph, then plot using ggraph:\n\ntidyverse_tbl <- tidied_cran_imports %>% \n  filter(package %in% tidyverse_packages()) %>%\n  filter(imports %in% tidyverse_packages()) %>%\n  as_tbl_graph()\n\n\nggraph(tidyverse_tbl, layout = \"nicely\")  + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .12) +\n  theme_void()\n\n\n\n\nMany intersecting lines traverse in all directions, as one might anticipate, because many packages in tidyverse import other packages. As the tidyverse develops, this is to be expected.\n\nenv_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Environmetrics\") %>% \n  unlist(use.names = FALSE)\n\n\nenv_tbl <- tidied_cran_imports %>%\n  filter(package %in% env_packages) %>%\n  filter(imports %in% env_packages) %>%\n  as_tbl_graph()\n\nenv_tbl %>% \n  ggraph(layout = 'nicely') + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .3) +\n  theme_void()\n\n\n\n\nNext, let’s look at the Spatial Analysis task view, where we might not expect to see the same level of connectedness. The infrastructure underlying CRAN task views, the ctv package, (sort of) provides a function to obtain a vector of package names for a given task view, which we can use to make a network plot:\n\nspatial_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Spatial\") %>% \n  unlist(use.names = FALSE)\n\nWe then pull the packages that are in spatial analysis task view that are found in all packages that are tidied and convert them to ggraph table and plot the network\n\nsp_tbl <- tidied_cran_imports %>%\n  filter(package %in% spatial_packages) %>%\n  filter(imports %in% spatial_packages) %>%\n  as_tbl_graph()\n\nsp_tbl %>% \n  ggraph(layout = 'fr') + \n  geom_edge_link(colour = \"grey\") + \n  geom_node_point(colour=\"lightblue\", size=2) + \n  geom_node_text(aes(label=name), repel=FALSE, check_overlap = TRUE, nudge_y = .2) +  \n  theme_graph()\n\n\n\n\n\nsp_tbl %>% \n  ggraph(layout = 'linear',circular = TRUE) + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = TRUE, check_overlap = TRUE) +\n  theme_void()\n\n\n\n\nThere is clearly some connectivity among spatial-related packages, which serves as a reminder that task views on CRAN aren’t the only location where users find packages to use. Some programs, like sf, establish a hub of related packages because they share a package maintainer, while others, like sp, investigate spatial systems using a wide range of spatial packages. The graph below shows the number of downloads of the cranlogs package from the RStudio CRAN mirror over the last year.\n\nkgcount <- cran_downloads(packages = spatial_packages, \n                           from = Sys.Date()-1*365, \n                           to = Sys.Date())\n\n\nkgcount %>%\n  group_by(package) %>%\n  summarise(downloads = sum(count)) %>%\n  filter(downloads >= 450000) %>% \n  arrange(desc(downloads)) %>% \n  hchart(type = \"bar\", hcaes(x = package, y = downloads)) %>% \n  hc_xAxis(title = list(text = \"Mothly downloads\")) %>% \n  hc_yAxis(title = FALSE)"
  },
  {
    "objectID": "posts/ttest/index.html",
    "href": "posts/ttest/index.html",
    "title": "Inferential statistics in R:ttest",
    "section": "",
    "text": "A formal statistical test called a hypothesis test is used to confirm or disprove a statistical hypothesis. The following R hypothesis tests are demonstrated in this course.\nEach type of test can be run using the R function t.test().The function comes with the following arguments;\nwhere: \\(x\\) and \\(y\\) are the vectors of data elements \\(alternative\\): the stated alternative hypothesis \\(mu\\): the true value of the mean \\(paired\\): whether or not to run a paired test \\(var.equal\\): whether to assume that the vaarinaces between the values in the vector are equal \\(con.level\\): The confidence level to use\nBefore we proceed, we need functions from various packages and accessing these functions when needed may render this task tedious. Therefore, lets load the packages in advance. These packages include tidyverse (Wickham and Wickham 2017), patchwork(Pedersen 2020) and magrittr(Bache and Wickham 2014)"
  },
  {
    "objectID": "posts/ttest/index.html#one-sample-t-test",
    "href": "posts/ttest/index.html#one-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "One sample t-test",
    "text": "One sample t-test\nOne sample t-test is widely used in statistical analysis to determine whether the population’s mean is equal to given mean value. The given mean value can be the sample mean for instance. A t.test function in R is used to test one sample parametric test. Let’s consider a situation where we want to determine whether the total length of Nile perch collected during a survey conducted in December 2022 is not equal to a long term mean length of 61cm. Let’s generate a sample by creating a data frame that contain sample of 350 individual of nile perch. Using a run_if function help us simulate weight of nile perch once given the minimum and maximum values. The code for simulating the total length is highlighted in the code chunk below;\n\nsample = tibble::tibble(\n  id = 1:350,\n  tl = runif(n = 350, min = 48, max = 65)\n  )\n\nsample\n\n# A tibble: 350 x 2\n      id    tl\n   <int> <dbl>\n 1     1  61.3\n 2     2  49.7\n 3     3  50.4\n 4     4  48.5\n 5     5  54.9\n 6     6  57.3\n 7     7  60.6\n 8     8  61.9\n 9     9  50.4\n10    10  49.0\n# ... with 340 more rows\n\n\nLet’s use a histogram to check the distribution of the data and add a vertical line of the population mean to identify whether the data is leading away is around the population\n\nsample %>% \n  ggplot(aes(x = tl)) +\n  geom_histogram(bins = 30, color = \"ivory\", fill = \"cyan4\")+\n  geom_vline(xintercept = 61, linetype = 2, color = \"red\")+\n  scale_x_continuous(name = \"Total length (cm.)\", breaks = seq(40,80,4)) +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_minimal()\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nNow we notice that the position of the population mean of the sample dataset, we can not test to determine whether the sample mean total length is lower than the sample mean\n\nsample %$%\n  t.test(x = tl, mu = 61, alternative = \"less\") \n\n\n    One Sample t-test\n\ndata:  tl\nt = -17.907, df = 349, p-value < 2.2e-16\nalternative hypothesis: true mean is less than 61\n95 percent confidence interval:\n     -Inf 56.50406\nsample estimates:\nmean of x \n 56.04797 \n\n\nThe one sample t-test determine the whether the sample mean total length of nile perch was less than the long-term mean length suggest that the sample total length (56.68cm) was less than the population mean (61cm) and the difference was statistically significant (t(349) = -18.19, p < 0.001). Lets try change the alternative to greater\n\nsample %$%\n  t.test(x = tl, mu = 61, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  tl\nt = -17.907, df = 349, p-value = 1\nalternative hypothesis: true mean is greater than 61\n95 percent confidence interval:\n 55.59189      Inf\nsample estimates:\nmean of x \n 56.04797 \n\n\nNotice that the test is not statistically significant (t(349) = 18.91, p = 1) because the population mean (61cm) is greater than the sample mean (56.68)."
  },
  {
    "objectID": "posts/ttest/index.html#two-sample-t-test",
    "href": "posts/ttest/index.html#two-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "Two Sample t-test",
    "text": "Two Sample t-test\nA two sample t-test is used to determine whether the means of two independent samples are equal. Lets consider that two independent survey to measure the stock of nile perch was conducted in two independent period. The first survey was conducted in June 2001 and the subsequency survey was conducted in July 2021. The two survey were conducted during the cool and dry season but with a 20 years time difference. Therefore, we want to determine whether the mean sample of nile perch collected in 2021 is smaller than the those sampled in 2001\n\nset.seed(1254)\n\nsample2 = tibble::tibble(\n  id = 1:350,\n  tl21 = rnorm(n = 350,mean = 52, sd = 18),\n  tl01 = rnorm(n = 350, mean = 61, sd = 20)\n  )\n\nsample2\n\n# A tibble: 350 x 3\n      id  tl21  tl01\n   <int> <dbl> <dbl>\n 1     1 41.7   54.4\n 2     2 68.4   52.2\n 3     3 47.1   69.1\n 4     4 38.7   43.2\n 5     5  3.41  46.9\n 6     6 42.6   64.3\n 7     7 67.3   70.4\n 8     8 65.7   72.9\n 9     9 26.3   25.0\n10    10 56.2   64.5\n# ... with 340 more rows\n\n\nOnce we have created a dataframe with values for the two sampling survey, it’s a good practice to visualize the value to see the patterns.\n\nsample2 %>% \n  pivot_longer(cols = tl21:tl01) %>% \n  ggplot(aes(x = value, fill = name))+\n  geom_density(position = \"identity\", alpha = .4)+\n  scale_fill_brewer(name =\"Surveys\" ,palette = \"Set2\", label = c(\"2001\", \"2021\"))+\n  scale_x_continuous(name = \"Total length (cm)\", breaks = seq(20,150,20))+\n  scale_y_continuous(name = \"Density\")+\n  theme_minimal()\n\n\n\n\nWe notice from a figure above a slight difference in the density shape with the median value for 2001 far east from the 2021, suggesting the size of 2001 is relatively higher than 2001. Let’s perform two sample t-test to determine whether that difference is significant;\n\nsample2 %$%\n  t.test(x = tl21, y = tl01, alternative = \"less\") \n\n\n    Welch Two Sample t-test\n\ndata:  tl21 and tl01\nt = -6.4619, df = 690.02, p-value = 9.771e-11\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -6.905485\nsample estimates:\nmean of x mean of y \n 51.80015  61.06788 \n\n\nThe output display the Welch Two Sample t-test to determine whether the total length of nile perch sampled in 2021 is less than those of 2001. The result suggest that the sample mean in 2021 was 51.8 cm which is less than 61.07 cm of nile perch sampled in 2001. The result suggest that the 2021 nile perch were small in size than those of 2001, and the difference was significant (t(690) = 6.46, p < 0.01)."
  },
  {
    "objectID": "posts/ttest/index.html#paired-sample-t-test",
    "href": "posts/ttest/index.html#paired-sample-t-test",
    "title": "Inferential statistics in R:ttest",
    "section": "Paired sample t-test",
    "text": "Paired sample t-test\nThis test is normally used to determine whether the values in paired dataset have different mean. For instance, the weight in nile perch measured after captured and kept in cage for three months and measured again. Therefore, the nile perch individuals were measured before taken to cage and then measured after three months. This means we have measurement before and after. Let’s create a dataframe and simulate before and after total length of nile perch.\n\nset.seed(1254)\n\nsample3 = tibble::tibble(\n  id = 1:50,\n  before = rnorm(n = 50,mean = 52, sd = 12),\n  after = before + rnorm(n = 50) %>% abs()\n  )\n\nsample3\n\n# A tibble: 50 x 3\n      id before after\n   <int>  <dbl> <dbl>\n 1     1   45.1  46.2\n 2     2   62.9  64.7\n 3     3   48.7  49.3\n 4     4   43.2  43.6\n 5     5   19.6  20.4\n 6     6   45.7  46.1\n 7     7   62.2  63.2\n 8     8   61.1  62.5\n 9     9   34.9  35.2\n10    10   54.8  56.0\n# ... with 40 more rows\n\n\nThen we perform paired sample t-test\n\nsample3 %$%\n  t.test(x = before, y = after, paired = TRUE) \n\n\n    Paired t-test\n\ndata:  before and after\nt = -11.171, df = 49, p-value = 4.484e-15\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9512640 -0.6612062\nsample estimates:\nmean of the differences \n             -0.8062351 \n\n\nSince the p < 0.05, we reject the null hypothesis that the mean total length before and after is significant. Therefore, fattening nile perch in cage for three months increased the total length and that increase is significant (t(49) = -11.17, p < 0.001)"
  }
]