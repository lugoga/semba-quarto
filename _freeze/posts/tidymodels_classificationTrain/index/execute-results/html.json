{
  "hash": "33d99e2bc21cffd2c6bea5e127b5a715",
  "result": {
    "markdown": "---\ntitle: \"Machine learning with tidymodels: Binary Classification Model\"\nauthor: \n  - name: Masumbuko Semba\n    url: https://semba.netlify.app\n    orcid: 0000-0002-5002-9747\n    affiliation: Nelson Mandela African Institution of Science and Technology\n    affiliation-url: https://semba.netlify.app/ \ndate: \"2023-04-13\"\ncategories: [Manipulation,Visualization, R, Modelling]\ntags: \n  - tidymodels\n  - classification\n  - modelling\n  - Masumbuko Semba\n# image: \"thumbnail.jpg\"\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nbibliography: ../blog.bib\ncsl:  ../apa.csl\nexecute: \n  warning: false\nfig-width: 7\nfig-height: 5\ncode-line-numbers: true\n---\n\n\n# A gentle introduction\nIn this post, we’ll learn how to create Machine learning models using R. Machine learning is the foundation for predictive modeling and artificial intelligence. We’ll learn the core principles of machine learning and how to use common tools and frameworks to train, evaluate, and use machine learning models.\n\nModules that will be covered in this learning path include:\n\n+ Explore and analyze data with R\n+ Train and evaluate regression models\n+ Train and evaluate classification models\n+ Train and evaluate clustering models (under development) \n+ Train and evaluate deep learning models (under development)\n\nWe need some packages to accomplish the step in this post. We can have them installed as: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c('tidyverse', 'tidymodels', 'ranger', 'vip', 'palmerpenguins', 'skimr', 'paletteer', 'nnet', 'here'))\n```\n:::\n\n\nThen we load the packages in the session\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(magrittr)\nrequire(patchwork)\n```\n:::\n\n\n\n## Binary classification\nLet’s start by looking at an example of *binary* classification, where the model must predict a label that belongs to one of two classes. In this exercise, we’ll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data.\n\n### Explore the data\nThe first step in any machine learning project is to explore the data that you will use to train a model. And before we can explore the data, we must first import the dataset in our R environment. So, let’s begin by importing a CSV file of patent data into a tibble direct from the internet with `read_csv` function of **readr** package [@readr], part of the tidyverse ecosystem [@tidyverse];\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/diabetes.csv\")\n```\n:::\n\n\nWe then print the dataset to explore the variables and records contained in the dataset;\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15,000 x 10\n   PatientID Pregn~1 Plasm~2 Diast~3 Trice~4 Serum~5   BMI Diabe~6   Age Diabe~7\n       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <dbl>   <dbl>\n 1   1354778       0     171      80      34      23  43.5  1.21      21       0\n 2   1147438       8      92      93      47      36  21.2  0.158     23       0\n 3   1640031       7     115      47      52      35  41.5  0.0790    23       0\n 4   1883350       9     103      78      25     304  29.6  1.28      43       1\n 5   1424119       1      85      59      27      35  42.6  0.550     22       0\n 6   1619297       0      82      92       9     253  19.7  0.103     26       0\n 7   1660149       0     133      47      19     227  21.9  0.174     21       0\n 8   1458769       0      67      87      43      36  18.3  0.236     26       0\n 9   1201647       8      80      95      33      24  26.6  0.444     53       1\n10   1403912       1      72      31      40      42  36.9  0.104     26       0\n# ... with 14,990 more rows, and abbreviated variable names 1: Pregnancies,\n#   2: PlasmaGlucose, 3: DiastolicBloodPressure, 4: TricepsThickness,\n#   5: SerumInsulin, 6: DiabetesPedigree, 7: Diabetic\n```\n:::\n:::\n\n\n\nThough the printed output tell us that there are fifteen thousand records and ten variable, but sometimes you may wish to explore the internal structure of the dataset. The `glimpse` function from **dplyr** package can do that by listing the available variables and their types;\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 15,000\nColumns: 10\n$ PatientID              <dbl> 1354778, 1147438, 1640031, 1883350, 1424119, 16~\n$ Pregnancies            <dbl> 0, 8, 7, 9, 1, 0, 0, 0, 8, 1, 1, 3, 5, 7, 0, 3,~\n$ PlasmaGlucose          <dbl> 171, 92, 115, 103, 85, 82, 133, 67, 80, 72, 88,~\n$ DiastolicBloodPressure <dbl> 80, 93, 47, 78, 59, 92, 47, 87, 95, 31, 86, 96,~\n$ TricepsThickness       <dbl> 34, 47, 52, 25, 27, 9, 19, 43, 33, 40, 11, 31, ~\n$ SerumInsulin           <dbl> 23, 36, 35, 304, 35, 253, 227, 36, 24, 42, 58, ~\n$ BMI                    <dbl> 43.50973, 21.24058, 41.51152, 29.58219, 42.6045~\n$ DiabetesPedigree       <dbl> 1.21319135, 0.15836498, 0.07901857, 1.28286985,~\n$ Age                    <dbl> 21, 23, 23, 43, 22, 26, 21, 26, 53, 26, 22, 23,~\n$ Diabetic               <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,~\n```\n:::\n:::\n\nThis data consists of diagnostic information about some patients who have been tested for diabetes. Note that the final column in the dataset (Diabetic) contains the value 0 for patients who tested negative for diabetes, and 1 for patients who tested positive. This is the label that we will train our model to predict; most of the other columns (Pregnancies, PlasmaGlucose, DiastolicBloodPressure, BMI and so on) are the features we will use to predict the Diabetic label.\n\nLet’s reformat the data to make it easier for a model to use effectively. For now, let’s drop the PatientID column, encode the Diabetic column as a categorical variable, and clean the variable names:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes.tidy = diabetes %>% \n  janitor::clean_names() %>% \n  select(-patient_id) %>% \n  mutate(diabetic = factor(diabetic, levels = c(1,0)))\n\ndiabetes.tidy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15,000 x 9\n   pregnancies plasma_gluc~1 diast~2 trice~3 serum~4   bmi diabe~5   age diabe~6\n         <dbl>         <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl> <dbl> <fct>  \n 1           0           171      80      34      23  43.5  1.21      21 0      \n 2           8            92      93      47      36  21.2  0.158     23 0      \n 3           7           115      47      52      35  41.5  0.0790    23 0      \n 4           9           103      78      25     304  29.6  1.28      43 1      \n 5           1            85      59      27      35  42.6  0.550     22 0      \n 6           0            82      92       9     253  19.7  0.103     26 0      \n 7           0           133      47      19     227  21.9  0.174     21 0      \n 8           0            67      87      43      36  18.3  0.236     26 0      \n 9           8            80      95      33      24  26.6  0.444     53 1      \n10           1            72      31      40      42  36.9  0.104     26 0      \n# ... with 14,990 more rows, and abbreviated variable names 1: plasma_glucose,\n#   2: diastolic_blood_pressure, 3: triceps_thickness, 4: serum_insulin,\n#   5: diabetes_pedigree, 6: diabetic\n```\n:::\n:::\n\n\nThe primary goal of this exploration is to try to understand the relationships between of the variables in the dataset. In general to assess any apparent correlation between the features and the label we want to predict. The best approach is to use mental picture through data visualization. Now let’s compare the feature distributions for each label value. We’ll begin by formatting the data to a long format to make it somewhat easier to make multiple facets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes.tidy.long = diabetes.tidy %>% \n  pivot_longer(\n    cols = !diabetic, \n    names_to = \"features\", \n    values_to = \"values\"\n               )\n```\n:::\n\n\n\nOnce we have pivoted the data to long format, we can make some plots for visual exploration\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_light())\n\ndiabetes.tidy.long %>% \n  ggplot(aes(x = diabetic, y = values, fill = features)) +\n  geom_boxplot() + \n  facet_wrap(~ features, scales = \"free\", ncol = 4) +\n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAmazing! For some of the features, there’s a noticeable difference in the distribution for each label value. In particular, Pregnancies and Age show markedly different distributions for diabetic patients than for non-diabetic patients. These features may help predict whether or not a patient is diabetic.\n\n### Split the data\nOur dataset includes known values for the label, so we can use this to train a classifier so that it finds a statistical relationship between the features and the label value; but how will we know if our model is any good? How do we know it will predict correctly when we use it with new data that it wasn’t trained with?\n\nIt is best practice to hold out part of the data for testing in order to get a better estimate of how models will perform on new data by comparing the predicted labels with the already known labels in the test set. Well, we can take advantage of the fact we have a large dataset with known label values, use only some of it to train the model, and hold back some to test the trained model - enabling us to compare the predicted labels with the already known labels in the test set.\n\nIn R, the **tidymodels** framework provides a collection of packages for modeling and machine learning using tidyverse principles [@tidymodels]. For instance, **rsample**, a package in tidymodels, provides infrastructure for efficient data splitting and resampling [@rsample]:\n\n+ `initial_split()`: specifies how data will be split into a training and testing set\n+ `training()` and `testing()` functions extract the data in each split\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split data into 70% for training and 30% for testing\nset.seed(2056)\ndiabetes_split <- diabetes.tidy %>% \n  initial_split(prop = 0.70)\n\n\n# Extract the data in each split\ndiabetes_train <- training(diabetes_split)\ndiabetes_test <- testing(diabetes_split)\n\n\n# Print the number of cases in each split\ncat(\"Training cases: \", nrow(diabetes_train), \"\\n\",\n    \"Test cases: \", nrow(diabetes_test), sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining cases: 10500\nTest cases: 4500\n```\n:::\n:::\n\n### Train and Evaluate a Binary Classification Model\nOnce we have separated the dataset into train and test set, now we’re ready to train our model by fitting the training features to the training labels (diabetic). There are various algorithms we can use to train the model. In this example, we’ll use *Logistic Regression*, which is a well-established algorithm for classification. Logistic regression is a binary classification algorithm, meaning it predicts two categories.\n\nThere are quite a number of ways to fit a logistic regression model in tidymodels. For now, let’s fit a logistic regression model via the default `stats::glm()` engine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a model specifcation\nlogreg_spec <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\n\n\n# Print the model specification\nlogreg_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\nAfter a model has been specified, the model can be estimated or trained using the fit() function, typically using a symbolic description of the model (a formula) and some data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train a logistic regression model\nlogreg_fit <- logreg_spec %>% \n  fit(diabetic ~ ., data = diabetes_train)\n\n\n# Print the model object\nlogreg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetic ~ ., family = stats::binomial, \n    data = data)\n\nCoefficients:\n             (Intercept)               pregnancies            plasma_glucose  \n                8.624243                 -0.266296                 -0.009615  \ndiastolic_blood_pressure         triceps_thickness             serum_insulin  \n               -0.012297                 -0.022807                 -0.003932  \n                     bmi         diabetes_pedigree                       age  \n               -0.049028                 -0.923262                 -0.056876  \n\nDegrees of Freedom: 10499 Total (i.e. Null);  10491 Residual\nNull Deviance:\t    13290 \nResidual Deviance: 9221 \tAIC: 9239\n```\n:::\n:::\n\n\nThe model print out shows the coefficients learned during training.\n\nNow we’ve trained the model using the training data, we can use the test data we held back to evaluate how well it predicts using `parsnip::predict()`. Let’s start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make predictions then bind them to the test set\nresults <- diabetes_test %>% \n  select(diabetic) %>% \n  bind_cols(\n    logreg_fit %>% predict(new_data = diabetes_test)\n    )\n\n# Compare predictions\nresults %>% \n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 2\n   diabetic .pred_class\n   <fct>    <fct>      \n 1 0        0          \n 2 0        0          \n 3 0        0          \n 4 0        0          \n 5 1        1          \n 6 0        0          \n 7 0        0          \n 8 1        0          \n 9 0        0          \n10 0        0          \n```\n:::\n:::\n\n\nComparing each prediction with its corresponding “ground truth” actual value isn’t a very efficient way to determine how well the model is predicting. Fortunately, Tidymodels has a few more tricks up its sleeve: **yardstick** - a package used to measure the effectiveness of models using performance metrics [@yardstick].\n\nThe most obvious thing you might want to do is to check the accuracy of the predictions - in simple terms, what proportion of the labels did the model predict correctly? `yardstick::accuracy()` does just that!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate accuracy: proportion of data predicted correctly\naccuracy(\n  data = results, \n  truth = diabetic, \n  estimate = .pred_class\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.789\n```\n:::\n:::\n\nThe accuracy is returned as a decimal value - a value of 1.0 would mean that the model got 100% of the predictions right; while an accuracy of 0.0 is, well, pretty useless! Accuracy seems like a sensible metric to evaluate (and to a certain extent it is), but you need to be careful about drawing too many conclusions from the accuracy of a classifier. Remember that it’s simply a measure of how many cases were predicted correctly. Suppose only 3% of the population is diabetic. You could create a classifier that always just predicts 0, and it would be 97% accurate - but not terribly helpful in identifying patients with diabetes!\n\nFortunately, there are some other metrics that reveal a little more about how our classification model is performing.One performance metric associated with classification problems is the confusion matrix. A confusion matrix describes how well a classification model performs by tabulating how many examples in each class were correctly classified by a model. In our case, it will show you how many cases were classified as negative (0) and how many as positive (1); the confusion matrix also shows you how many were classified into the wrong categories. The `conf_mat()` function from yardstick calculates this cross-tabulation of observed and predicted classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion matrix for prediction results\nconf_mat(data = results, truth = diabetic, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction    1    0\n         1  897  293\n         0  657 2653\n```\n:::\n:::\n\n**tidymodels** provides yet another succinct way of evaluating all these metrics. Using yardstick::metric_set(), you can combine multiple metrics together into a new function that calculates all of them at once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine metrics and evaluate them all at once\neval_metrics <- \n  metric_set(ppv, recall, accuracy, f_meas)\n\n\neval_metrics(\n  data = results, \n  truth = diabetic, \n  estimate = .pred_class\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 ppv      binary         0.754\n2 recall   binary         0.577\n3 accuracy binary         0.789\n4 f_meas   binary         0.654\n```\n:::\n:::\n\n\nUntil now, we’ve considered the predictions from the model as being either 1 or 0 class labels. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on probability; so what actually gets predicted by a binary classifier is the probability that the label is true (P(y)\n) and the probability that the label is false (1−P(y)\n). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (P(y)>0.5\n) or a 0 (P(y)<=0.5\n). Let’s see the probability pairs for each case:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict class probabilities and bind them to results\nresults <- results %>% \n  bind_cols(logreg_fit %>% \n              predict(new_data = diabetes_test, type = \"prob\"))\n\n  \n\n\n# Print out the results\nresults %>% \n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 4\n   diabetic .pred_class .pred_1 .pred_0\n   <fct>    <fct>         <dbl>   <dbl>\n 1 0        0            0.417    0.583\n 2 0        0            0.0985   0.902\n 3 0        0            0.0469   0.953\n 4 0        0            0.0561   0.944\n 5 1        1            0.581    0.419\n 6 0        0            0.331    0.669\n 7 0        0            0.288    0.712\n 8 1        0            0.270    0.730\n 9 0        0            0.275    0.725\n10 0        0            0.131    0.869\n```\n:::\n:::\n\n\nThe decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the true positive rate (which is another name for recall) and the false positive rate (1 - specificity) for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a received operator characteristic (ROC) chart, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a roc_chart\nresults %>% \n  roc_curve(truth = diabetic, .pred_1) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThe ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\n\nThe area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. The closer to 1 this value is, the better the model. Once again, Tidymodels includes a function to calculate this metric: yardstick::roc_auc()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the AUC\nresults %>% \n  roc_auc(diabetic, .pred_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n```\n:::\n:::\n\n\n\n## Cited Materials\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}