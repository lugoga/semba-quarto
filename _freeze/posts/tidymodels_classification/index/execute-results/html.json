{
  "hash": "37a5946e4e62b37bd1a2782d4f8722d1",
  "result": {
    "markdown": "---\ntitle: \"Machine learning with tidymodels: Classification Models\"\nauthor: \n  - name: Masumbuko Semba\n    url: https://semba.netlify.app\n    orcid: 0000-0002-5002-9747\n    affiliation: Nelson Mandela African Institution of Science and Technology\n    affiliation-url: https://semba.netlify.app/ \ndate: \"2023-04-03\"\ncategories: [Manipulation,Visualization, R, Modelling]\ntags: \n  - tidymodels\n  - classification\n  - modelling\n  - Masumbuko Semba\n# image: \"thumbnail.jpg\"\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nbibliography: ../blog.bib\ncsl:  ../apa.csl\nexecute: \n  warning: false\nfig-width: 7\nfig-height: 5\ncode-line-numbers: true\n---\n\n\n# A gentle introduction to classification\n*Classification* is a form of machine learning in which you train a model to predict which category an item belongs to. Categorical data has distinct ‘classes’, rather than numeric values. For example, a health clinic might use diagnostic data such as a patient’s height, weight, blood pressure, blood-glucose level to predict whether or not the patient is diabetic.\n\nClassification is an example of a supervised machine learning technique, which means it relies on data that includes known feature values (for example, diagnostic measurements for patients) as well as known label values (for example, a classification of non-diabetic or diabetic). A classification algorithm is used to fit a subset of the data to a function that can calculate the probability for each class label from the feature values. The remaining data is used to evaluate the model by comparing the predictions it generates from the features to the known class labels.\n\nThe best way to learn about classification is to try it for yourself, so that’s what you’ll do in this exercise.\n\nWe’ll require some packages to knock-off this module. You can have them installed as: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c('tidyverse', 'tidymodels', 'ranger', 'tidyverse', 'forecats', 'skimr', 'paletteer', 'nnet', 'here'))\n```\n:::\n\n\n\nOnce you have installed the package, you can load the required packages \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(forcats)\n```\n:::\n\n\n\n## Dataset\n\nOnce the packages are loaded then we are going to import the dataset into the session. In this post we will explore a multi-class classification problem using the Covertype Data Set, which I obtained from the UCI Machine Learning Repository. This data set provides a total of 581,012 instances. The goal is to differentiate seven forest community types using several environmental variables including elevation, topographic aspect, topographic slope, horizontal distance to streams, vertical distance to streams, horizontal distance to roadways, hillshade values at 9AM, hillshade values at noon, hillshade values at 3PM, horizontal distance to fire points, and a wilderness area designation, a binary and nominal variable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncover.type = read_csv(\"../data/ml/covtype.csv\")\ncover.type %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 581,012\nColumns: 55\n$ Elevation                          <dbl> 2596, 2590, 2804, 2785, 2595, 2579,~\n$ Aspect                             <dbl> 51, 56, 139, 155, 45, 132, 45, 49, ~\n$ Slope                              <dbl> 3, 2, 9, 18, 2, 6, 7, 4, 9, 10, 4, ~\n$ Horizontal_Distance_To_Hydrology   <dbl> 258, 212, 268, 242, 153, 300, 270, ~\n$ Vertical_Distance_To_Hydrology     <dbl> 0, -6, 65, 118, -1, -15, 5, 7, 56, ~\n$ Horizontal_Distance_To_Roadways    <dbl> 510, 390, 3180, 3090, 391, 67, 633,~\n$ Hillshade_9am                      <dbl> 221, 220, 234, 238, 220, 230, 222, ~\n$ Hillshade_Noon                     <dbl> 232, 235, 238, 238, 234, 237, 225, ~\n$ Hillshade_3pm                      <dbl> 148, 151, 135, 122, 150, 140, 138, ~\n$ Horizontal_Distance_To_Fire_Points <dbl> 6279, 6225, 6121, 6211, 6172, 6031,~\n$ Wilderness_Area1                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ Wilderness_Area2                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Wilderness_Area3                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Wilderness_Area4                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type1                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type2                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type3                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type4                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type5                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type6                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type7                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type8                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type9                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type10                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type11                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type12                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type13                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type14                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type15                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type16                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type17                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type18                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,~\n$ Soil_Type19                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type20                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type21                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type22                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type23                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type24                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type25                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type26                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type27                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type28                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type29                        <dbl> 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,~\n$ Soil_Type30                        <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ Soil_Type31                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type32                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type33                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type34                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type35                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type36                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type37                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type38                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type39                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Soil_Type40                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ Cover_Type                         <dbl> 5, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 2,~\n```\n:::\n:::\n\nThe seven community types are:\n\n+ 1 = Spruce/Fir\n+ 2 = Lodgepole Pine\n+ 3 = Ponderosa Pine\n+ 4 = Cottonwood/Willow\n+ 5 = Aspen\n+ 6 = Douglas Fir\n+ 7 = Krummholz\n\nWe need to recode the cover type with the corresponding names as follows;\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncover.type %>% \n  distinct(Cover_Type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 1\n  Cover_Type\n       <dbl>\n1          5\n2          2\n3          1\n4          7\n5          3\n6          6\n7          4\n```\n:::\n\n```{.r .cell-code}\ncover.type = cover.type %>% \n  mutate(cover = case_when(Cover_Type == 1 ~ \"Spruce\",\n                               Cover_Type == 2 ~ \"Lodgepole\",\n                               Cover_Type == 3 ~ \"Ponderosa\",\n                               Cover_Type == 4 ~ \"Cottonwood\",\n                               Cover_Type == 5 ~ \"Aspen\",\n                               Cover_Type == 6 ~ \"Douglas\",\n                               Cover_Type == 7 ~ \"Krummholz\")\n         )\n```\n:::\n\n\n\nI then use dplyr `count` function to to compute the number of records from each community type\n\n::: {.cell}\n\n```{.r .cell-code}\ncover.type %>% \n  group_by(cover) %>% \n  summarise(n = n()) %>% \n  mutate(area_ha = (n*900)/4063, \n         pct = n/sum(n) * 100, \n         across(is.numeric, round, 2)) %>% \n  arrange(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 4\n  cover           n area_ha   pct\n  <chr>       <dbl>   <dbl> <dbl>\n1 Lodgepole  283301  62754. 48.8 \n2 Spruce     211840  46925. 36.5 \n3 Ponderosa   35754   7920.  6.15\n4 Krummholz   20510   4543.  3.53\n5 Douglas     17367   3847.  2.99\n6 Aspen        9493   2103.  1.63\n7 Cottonwood   2747    608.  0.47\n```\n:::\n:::\n\n\n\nThe printed output suggests significant data imbalance. In order to speed up the tuning and training process, I then select out 500 samples from each class using a stratified random sample. For potentially improved results, I should use all available samples. However, this would take a lot longer to execute. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ncover.type.sample = cover.type %>% \n  group_by(cover) %>% \n  sample_n(size = 500) %>% \n  ungroup()\n\ncover.type.sample %>% \n  group_by(cover) %>% \n  summarise(n = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 2\n  cover          n\n  <chr>      <int>\n1 Aspen        500\n2 Cottonwood   500\n3 Douglas      500\n4 Krummholz    500\n5 Lodgepole    500\n6 Ponderosa    500\n7 Spruce       500\n```\n:::\n:::\n\n\nNext, I use the parsnips package [@parsnip] to define a random forest implementation using the `ranger` engine in *classification* mode. Note the use of `tune()` to indicate that I plan to tune the *mtry* parameter. Since the data have not already been split into *training* and *testing* sets, I use the `initial_split()` function from **rsample** to define training and testing partitions followed by the `training()` and `testing()` functions to create new datasets for each split [@rsample].\n\n## Define Model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model = rand_forest(mtry=tune(), trees=500) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n## Set split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\ncover_split = cover.type.sample %>% \n  initial_split(prop=.75, strata=cover)\n\ncover_train = cover_split %>% training()\ncover_test = cover_split %>% testing()\n```\n:::\n\n\n\nI would like to normalize all continuous predictor variables and create a dummy variable from the single nominal predictor variable (“wilderness”). I define these transformations within a recipe using functions available in recipes package [@recipes]. This also requires defining the formula and the input data. Here, I am referencing only the training set, as the test set should not be introduced to the model at this point, as this could result in a later bias assessment of model performance. The `all_numeric()`, `all_nominal()`, and `all_outcomes()` functions are used to select columns on which to apply the desired transformations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncover_recipe = cover_train %>% \n  recipe(cover~.) %>%\n  step_normalize(all_numeric()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n```\n:::\n\n\nThe model and pre-processing recipe are then combined into a workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncover_wf = workflow() %>%\n  add_model(rf_model) %>% \n  add_recipe(cover_recipe)\n```\n:::\n\n\n\nI then use **yardstick** [@yerdstick] and the `metric_set()` function to define the desired assessment metrics, in this case only overall accuracy. To prepare for hyperparameter tuning using five-fold cross validation, I define folds using the `vfold_cv()` function from **rsample**. Similar to the training and testing split above, the folds are stratified by the community type to maintain class balance within each fold. Lastly, I then define values of *mtry* to test using the dials package. It would be better to test more values and maybe optimize additional parameters. However, I am trying to decrease the time required to execute the example.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define metrics\nmy_mets = metric_set(accuracy)\n\n#Define folds\nset.seed(42)\ncover_folds = vfold_cv(cover_train, v=5, strata=cover)\n\n#Define tuning grid\nrf_grid = grid_regular(mtry(range = c(1, 12)),\n                        levels = 6)\n```\n:::\n\n\nNow that the model, pre-processing steps, workflow, metrics, data partitions, and mtry values to try have been defined, I tune the model using `tune_grid()` from the tune package. Note that this may take several minutes. Specifically, I make sure to use the defined workflow so that the pre-processing steps defined using the recipe are used. Once completed, I collect the resulting metrics for each mtry value for each fold using collect_metrics() from tune. The summarize parameter is set to FALSE because I want to obtain all results for each fold, as opposed to aggregated results. I then calculate the minimum, maximum, and median overall accuracies for each fold using dplyr and plot the results using ggplot2.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_tuning = cover_wf %>% \n  tune_grid(resamples=cover_folds, grid = rf_grid, metrics=my_mets)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_result = rf_tuning %>% \n  collect_metrics(summarize=FALSE) %>%\n  filter(.metric == 'accuracy') %>%  \n  group_by(mtry) %>%  \n  summarize(min_acc = min(.estimate),             \n            median_acc = mean(.estimate),             \n            max_acc = max(.estimate))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tune_result, aes(y=median_acc, x=mtry))+\n  geom_point()+\n  geom_errorbar(aes(ymin=min_acc, ymax=max_acc), width = .4)+\n  theme_bw()+\n  labs(x=\"mtry Parameter\", y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe best mtry parameter is defined using the `select_best()` function from tune. The workflow is then finalized and the model is trained using `last_fit()` from tune. The `collect_predictions()` function from tune is used to obtain the class prediction for each sample in the withheld test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rf_model = rf_tuning %>% \n  select_best(metric=\"accuracy\")\n\nfinal_cover_wf = cover_wf %>% \n  finalize_workflow(best_rf_model)\n\nfinal_cover_fit = final_cover_wf %>% \n  last_fit(split=cover_split, metrics=my_mets) %>% \n  collect_predictions()\n```\n:::\n\n\nLastly, I use the `conf_mat()` function from the yardstick package to obtain a multi-class error matrix from the reference and predicted classes for each sample in the withheld testing set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_cover_fit %>% \n  conf_mat(truth=cover, estimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Truth\nPrediction   Aspen Cottonwood Douglas Krummholz Lodgepole Ponderosa Spruce\n  Aspen        125          0       0         0         0         0      0\n  Cottonwood     0        125       0         0         0         0      0\n  Douglas        0          0     125         0         0         0      0\n  Krummholz      0          0       0       125         0         0      0\n  Lodgepole      0          0       0         0       124         0      0\n  Ponderosa      0          0       0         0         1       125      0\n  Spruce         0          0       0         0         0         0    125\n```\n:::\n:::\n\n\nPassing the matrix to `summary()` will provide a set of assessment metrics calculated from the error matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_cover_fit %>% \n  conf_mat(truth=cover, estimate=.pred_class) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.999\n 2 kap                  multiclass     0.999\n 3 sens                 macro          0.999\n 4 spec                 macro          1.00 \n 5 ppv                  macro          0.999\n 6 npv                  macro          1.00 \n 7 mcc                  multiclass     0.999\n 8 j_index              macro          0.999\n 9 bal_accuracy         macro          0.999\n10 detection_prevalence macro          0.143\n11 precision            macro          0.999\n12 recall               macro          0.999\n13 f_meas               macro          0.999\n```\n:::\n:::\n\n\n## Concluding Remarks\nSimilar to the tidyverse [@tidyverse], tidymodels [@tidymodels] is a very powerful framework for creating machine learning workflows and experimental environments using a common philosophy and syntax. Although this introduction was brief and there are many more components that could be discussed, this can serve as a starting point for continued learning and experimentation. Check out the [tidymodels website](https://www.tidymodels.org/start/tuning/) for additional examples and tutorials. \n\n\n## Cited Materials\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}